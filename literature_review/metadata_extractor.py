#!/usr/bin/env python3
"""
å…ƒæ•°æ®æå–æ¨¡å— - ç±»å°è£…ç‰ˆæœ¬
ä» Markdown æ–‡æœ¬ä¸­æå–æ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ã€ç« èŠ‚ç»“æ„ç­‰å…ƒæ•°æ®
"""
import re
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from literature_review.logger import get_logger

logger = get_logger("metadata_extractor")


class MetadataExtractor:
    """å…ƒæ•°æ®æå–å™¨ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æå–å™¨"""
        pass
    
    def extract_from_content_list(self, content_list_path: str) -> Dict:
        """
        ä» MinerU çš„ content_list_v2.json æå–æ ‡é¢˜å’Œä½œè€…
        
        MinerU è¾“å‡ºçš„ JSON ç»“æ„æ¸…æ™°ï¼š
        - ç¬¬ä¸€ä¸ª type="title" å°±æ˜¯è®ºæ–‡æ ‡é¢˜
        - æ ‡é¢˜å’Œ Abstract ä¹‹é—´çš„ type="paragraph" åŒ…å«ä½œè€…ä¿¡æ¯
        
        Args:
            content_list_path: content_list_v2.json æ–‡ä»¶è·¯å¾„
        
        Returns:
            {"title": "...", "authors": ["...", ...]} æˆ–ç©ºå­—å…¸
        """
        path = Path(content_list_path)
        if not path.exists():
            return {}
        
        try:
            with open(path, 'r', encoding='utf-8') as f:
                pages = json.load(f)
        except (json.JSONDecodeError, Exception):
            return {}
        
        # å±•å¹³æ‰€æœ‰é¡µé¢çš„å…ƒç´ 
        elements = []
        for page in pages:
            if isinstance(page, list):
                elements.extend(page)
        
        if not elements:
            return {}
        
        # 1) æå–æ ‡é¢˜ï¼šç¬¬ä¸€ä¸ª type="title" çš„å…ƒç´ 
        title = None
        title_idx = -1
        for i, elem in enumerate(elements):
            if elem.get('type') == 'title':
                title_parts = elem.get('content', {}).get('title_content', [])
                title_text = ''.join(
                    p.get('content', '') for p in title_parts 
                    if p.get('type') == 'text'
                ).strip()
                if title_text and len(title_text) > 10:
                    title = title_text
                    title_idx = i
                    break
        
        if title is None:
            return {}
        
        # 2) æå–ä½œè€…ï¼šæ ‡é¢˜ä¹‹åã€ä¸‹ä¸€ä¸ª titleï¼ˆé€šå¸¸æ˜¯ Abstractï¼‰ä¹‹å‰çš„ paragraph
        author_text_parts = []
        for elem in elements[title_idx + 1:]:
            if elem.get('type') == 'title':
                # é‡åˆ° Abstract æˆ–ç« èŠ‚æ ‡é¢˜ï¼Œåœæ­¢
                break
            if elem.get('type') == 'paragraph':
                parts = elem.get('content', {}).get('paragraph_content', [])
                text = ''.join(
                    p.get('content', '') for p in parts 
                    if p.get('type') == 'text'
                ).strip()
                if text:
                    author_text_parts.append(text)
        
        # è§£æä½œè€…åï¼ˆç¬¬ä¸€ä¸ª paragraph é€šå¸¸æ˜¯ä½œè€…è¡Œï¼‰
        authors = []
        if author_text_parts:
            author_line = author_text_parts[0]
            authors = self._parse_author_line(author_line)
        
        result = {}
        if title:
            result['title'] = title
        if authors:
            result['authors'] = authors
        
        return result
    
    def _parse_author_line(self, text: str) -> List[str]:
        """
        ä»ä½œè€…è¡Œæ–‡æœ¬ä¸­æå–ä½œè€…å
        å¤„ç†å„ç§æ ¼å¼ï¼šä¸Šæ ‡æ•°å­—ã€ç¬¦å·ã€Fellow/IEEE ç­‰
        """
        # å»æ‰å¸¸è§å™ªéŸ³
        cleaned = text
        # å»æ‰ä¸Šæ ‡æ•°å­—å’Œç¬¦å· (1, 2, *, â€ , â€¡, âˆ—)
        cleaned = re.sub(r'[0-9]+[âˆ—\*â€ â€¡Â·,]?\s*', '', cleaned)
        cleaned = re.sub(r'[âˆ—\*â€ â€¡Â·]+', '', cleaned)
        # å»æ‰ Fellow/Member/IEEE ç­‰
        cleaned = re.sub(r',?\s*(Fellow|Senior Member|Member|Student Member),?\s*(IEEE)?', '', cleaned)
        # å»æ‰é‚®ç®±
        cleaned = re.sub(r'\S+@\S+', '', cleaned)
        
        # æŒ‰ 'and' å’Œ ',' åˆ†å‰²
        cleaned = cleaned.replace(' and ', ',')
        parts = [p.strip() for p in cleaned.split(',')]
        
        # è¿‡æ»¤æœ‰æ•ˆä½œè€…åï¼ˆè‡³å°‘ 2 ä¸ªå­—ç¬¦ï¼Œçœ‹èµ·æ¥åƒäººåï¼‰
        authors = []
        for part in parts:
            part = part.strip()
            if not part or len(part) < 2:
                continue
            # è·³è¿‡æœºæ„åï¼ˆåŒ…å« University/Laboratory/Institute ç­‰ï¼‰
            if re.search(r'(University|Laboratory|Institute|Department|School|College)', part, re.IGNORECASE):
                continue
            # è·³è¿‡æ³¨é‡Šè¡Œï¼ˆEqual Contribution, Correspondence ç­‰ï¼‰
            if re.search(r'(Equal|Contribution|Correspondence|Advising)', part, re.IGNORECASE):
                continue
            authors.append(part)
        
        return authors[:20]
    
    def extract_title(self, markdown_text: str) -> Optional[str]:
        """
        ä» Markdown ä¸­æå–è®ºæ–‡æ ‡é¢˜
        é€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªä¸€çº§æ ‡é¢˜æˆ–æœ€å¼€å§‹çš„å¤§æ ‡é¢˜æ–‡æœ¬
        """
        lines = markdown_text.split('\n')
        
        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªéç©ºè¡Œï¼ˆé€šå¸¸æ˜¯æ ‡é¢˜ï¼‰
        for line in lines[:20]:  # åªæ£€æŸ¥å‰20è¡Œ
            line = line.strip()
            if not line:
                continue
            
            # æ¸…ç†æ ‡é¢˜
            title = line.lstrip('#').strip()
            
            # å¦‚æœæ ‡é¢˜å¤ªçŸ­æˆ–åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼Œè·³è¿‡
            if len(title) > 10 and not title.startswith('http'):
                return title
        
        return None
    
    def extract_authors(self, markdown_text: str) -> List[str]:
        """
        ä» Markdown ä¸­æå–ä½œè€…åˆ—è¡¨
        """
        lines = markdown_text.split('\n')
        authors = []
        
        # ç­–ç•¥1ï¼šåœ¨æ ‡é¢˜åçš„å‰å‡ è¡Œå¯»æ‰¾ä½œè€…
        # MinerU æ ¼å¼ï¼šæ ‡é¢˜åç›´æ¥æ˜¯ä½œè€…è¡Œ
        title_found = False
        for i, line in enumerate(lines[:30]):
            line = line.strip()
            
            # è·³è¿‡æ ‡é¢˜è¡Œ
            if line.startswith('#'):
                title_found = True
                continue
            
            # æ ‡é¢˜åçš„ç¬¬ä¸€ä¸ªéç©ºè¡Œå¯èƒ½æ˜¯ä½œè€…
            if title_found and line and not line.startswith('#'):
                # æ£€æŸ¥æ˜¯å¦åŒ…å«äººåæ¨¡å¼
                # æ¨¡å¼1: "Name1, Name2, and Name3" æˆ– "Name1 and Name2"
                if ' and ' in line or ',' in line:
                    # åˆ†ç¦»ä½œè€…ï¼ˆæŒ‰é€—å·æˆ– and åˆ†å‰²ï¼‰
                    author_text = line
                    # ç§»é™¤èŒç§°ç­‰åç¼€ï¼ˆä¾‹å¦‚ ", Fellow, IEEE"ï¼‰
                    author_text = re.sub(r',\s*(Fellow|Member|Senior Member|Prof\.|Dr\.)[^,]*$', '', author_text)
                    
                    # æŒ‰ 'and' å’Œ ',' åˆ†å‰²
                    author_text = author_text.replace(' and ', ',')
                    potential_authors = [a.strip() for a in author_text.split(',')]
                    
                    # è¿‡æ»¤æœ‰æ•ˆçš„ä½œè€…åï¼ˆè‡³å°‘åŒ…å«2ä¸ªå•è¯ï¼‰
                    for author in potential_authors:
                        if len(author.split()) >= 2:
                            # ç§»é™¤èŒç§°
                            author = re.sub(r'\s*(Fellow|Member|Senior Member|Prof\.|Dr\.)\s+.*$', '', author)
                            author = re.sub(r',.*$', '', author).strip()
                            if author and len(author.split()) >= 2:
                                authors.append(author)
                    
                    if authors:
                        return authors[:20]
        
        # ç­–ç•¥2ï¼šæŸ¥æ‰¾åŒ…å«ä½œè€…çš„è¡Œï¼ˆåŸæœ‰é€»è¾‘ä½œä¸ºå¤‡é€‰ï¼‰
        for i, line in enumerate(lines[:50]):
            line = line.strip()
            
            # æŸ¥æ‰¾åŒ…å«ä½œè€…çš„è¡Œï¼ˆé€šå¸¸åŒ…å«å¤šä¸ªåå­—ï¼Œå¯èƒ½ç”¨é€—å·åˆ†éš”ï¼‰
            # ç‰¹å¾ï¼šå¤§å†™å­—æ¯å¼€å¤´çš„åå­—ï¼Œå¯èƒ½åŒ…å«é€—å·æˆ– and
            if re.search(r'[A-Z][a-z]+\s+[A-Z][a-z]+', line):
                # å¯èƒ½æ˜¯ä½œè€…è¡Œ
                # åˆ†ç¦»ä½œè€…åå­—
                potential_authors = re.findall(
                    r'[A-Z][a-z]+(?:\s+[A-Z]\.?\s*)?[A-Z][a-z]+',
                    line
                )
                if potential_authors and len(potential_authors) <= 15:  # åˆç†ä½œè€…æ•°é‡
                    authors.extend(potential_authors)
                    if len(authors) >= 3:  # æ‰¾åˆ°è¶³å¤Ÿä½œè€…ååœæ­¢
                        break
        
        return authors[:20]  # æœ€å¤šè¿”å›20ä¸ªä½œè€…
    
    def extract_abstract(self, markdown_text: str) -> Optional[str]:
        """
        ä» Markdown ä¸­æå–æ‘˜è¦
        """
        # ç­–ç•¥1ï¼šæŸ¥æ‰¾ Markdown æ ‡é¢˜æ ¼å¼çš„ Abstract (# Abstract, ## Abstract, etc.)
        abstract_pattern = r'(?i)#+\s*abstract\s*\n+(.*?)(?=\n#+|\Z)'
        match = re.search(abstract_pattern, markdown_text, re.DOTALL)
        
        if match:
            abstract = match.group(1).strip()
            # æ¸…ç†å¤šä½™çš„ç©ºç™½
            abstract = re.sub(r'\n+', ' ', abstract)
            abstract = re.sub(r'\s+', ' ', abstract)
            return abstract
        
        # ç­–ç•¥2ï¼šæŸ¥æ‰¾éæ ‡é¢˜æ ¼å¼çš„ Abstractï¼ˆä¾‹å¦‚ "Abstractâ€”" æˆ– "Abstract:"ï¼‰
        abstract_pattern2 = r'(?i)abstract\s*[â€”:]\s*(.*?)(?=\n\n[A-Z]|\n#+|\Z)'
        match = re.search(abstract_pattern2, markdown_text, re.DOTALL)
        
        if match:
            abstract = match.group(1).strip()
            # æ¸…ç†å¤šä½™çš„ç©ºç™½
            abstract = re.sub(r'\n+', ' ', abstract)
            abstract = re.sub(r'\s+', ' ', abstract)
            # ç§»é™¤ "Index Terms" ç­‰åç»­å†…å®¹
            abstract = re.split(r'(?i)index\s+terms', abstract)[0].strip()
            return abstract
        
        return None
    
    def extract_sections(self, markdown_text: str) -> List[Dict[str, any]]:
        """
        ä» Markdown ä¸­æå–ç« èŠ‚ç»“æ„
        
        Returns:
            [
                {"title": "Introduction", "level": 1, "content": "..."},
                {"title": "Methods", "level": 1, "content": "..."},
                ...
            ]
        """
        sections = []
        
        # åŒ¹é…ç« èŠ‚æ ‡é¢˜ï¼ˆ# å¼€å¤´ï¼‰
        lines = markdown_text.split('\n')
        current_section = None
        section_content = []
        
        for line in lines:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜è¡Œ
            heading_match = re.match(r'^(#+)\s+(.+)$', line)
            
            if heading_match:
                # ä¿å­˜ä¸Šä¸€ä¸ªç« èŠ‚
                if current_section:
                    current_section['content'] = '\n'.join(section_content).strip()
                    sections.append(current_section)
                
                # å¼€å§‹æ–°ç« èŠ‚
                level = len(heading_match.group(1))
                title = heading_match.group(2).strip()
                
                current_section = {
                    "title": title,
                    "level": level,
                    "content": ""
                }
                section_content = []
            else:
                # æ·»åŠ åˆ°å½“å‰ç« èŠ‚å†…å®¹
                if current_section:
                    section_content.append(line)
        
        # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
        if current_section:
            current_section['content'] = '\n'.join(section_content).strip()
            sections.append(current_section)
        
        return sections
    
    def validate_metadata(self, metadata: Dict) -> Tuple[bool, List[str]]:
        """
        éªŒè¯å…ƒæ•°æ®çš„å®Œæ•´æ€§
        
        Returns:
            (is_valid, missing_fields)
        """
        required_fields = ['title', 'authors', 'sections']
        missing_fields = []
        
        for field in required_fields:
            if field not in metadata or not metadata[field]:
                missing_fields.append(field)
        
        # éªŒè¯ç« èŠ‚ç»“æ„
        if 'sections' in metadata and metadata['sections']:
            # æ£€æŸ¥æ˜¯å¦æœ‰å¸¸è§ç« èŠ‚
            section_titles = [s['title'].lower() for s in metadata['sections']]
            common_sections = ['introduction', 'method', 'result', 'conclusion']
            
            has_common_section = any(
                any(common in title for common in common_sections)
                for title in section_titles
            )
            
            if not has_common_section:
                missing_fields.append('standard_sections')
        
        is_valid = len(missing_fields) == 0
        return is_valid, missing_fields
    
    def extract_metadata(self, markdown_text: str) -> Dict:
        """
        ä» Markdown æå–æ‰€æœ‰å…ƒæ•°æ®
        
        Returns:
            {
                "title": "è®ºæ–‡æ ‡é¢˜",
                "authors": ["ä½œè€…1", "ä½œè€…2"],
                "abstract": "æ‘˜è¦å†…å®¹",
                "sections": [...],
                "validation": {
                    "is_valid": True,
                    "missing_fields": []
                }
            }
        """
        logger.info("ğŸ“‹ æå–å…ƒæ•°æ®...")
        
        metadata = {
            "title": self.extract_title(markdown_text),
            "authors": self.extract_authors(markdown_text),
            "abstract": self.extract_abstract(markdown_text),
            "sections": self.extract_sections(markdown_text)
        }
        
        # éªŒè¯
        is_valid, missing_fields = self.validate_metadata(metadata)
        metadata['validation'] = {
            "is_valid": is_valid,
            "missing_fields": missing_fields
        }
        
        # æ‰“å°æå–ç»“æœ
        logger.info(f"âœ… æ ‡é¢˜: {metadata['title'][:50] if metadata['title'] else 'æœªæ‰¾åˆ°'}...")
        logger.info(f"âœ… ä½œè€…æ•°: {len(metadata['authors'])}")
        logger.info(f"âœ… æ‘˜è¦: {'å·²æå–' if metadata['abstract'] else 'æœªæ‰¾åˆ°'}")
        logger.info(f"âœ… ç« èŠ‚æ•°: {len(metadata['sections'])}")
        
        if not is_valid:
            logger.warning(f"ç¼ºå°‘å­—æ®µ: {', '.join(missing_fields)}")
        else:
            logger.info("âœ… å…ƒæ•°æ®éªŒè¯é€šè¿‡")
        
        return metadata


# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸæœ‰çš„å‡½æ•°æ¥å£
def extract_metadata(markdown_text: str) -> Dict:
    """å‘åå…¼å®¹çš„å‡½æ•°æ¥å£"""
    extractor = MetadataExtractor()
    return extractor.extract_metadata(markdown_text)


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    import sys
    import json
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python metadata_extractor.py <markdown_file>")
        sys.exit(1)
    
    markdown_file = sys.argv[1]
    
    try:
        with open(markdown_file, 'r', encoding='utf-8') as f:
            markdown_text = f.read()
        
        extractor = MetadataExtractor()
        metadata = extractor.extract_metadata(markdown_text)
        
        print("\n" + "=" * 60)
        print("æå–çš„å…ƒæ•°æ®")
        print("=" * 60)
        print(json.dumps(
            {k: v for k, v in metadata.items() if k != 'sections'},
            ensure_ascii=False,
            indent=2
        ))
        
        print(f"\nç« èŠ‚åˆ—è¡¨:")
        for i, section in enumerate(metadata['sections'][:10], 1):
            print(f"  {i}. {'  ' * (section['level']-1)}{section['title']}")
        
        if len(metadata['sections']) > 10:
            print(f"  ... è¿˜æœ‰ {len(metadata['sections']) - 10} ä¸ªç« èŠ‚")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
