#!/usr/bin/env python3
"""
æ–‡çŒ®ç»¼è¿°æµæ°´çº¿ - ä¸»ç¨‹åº
è‡ªåŠ¨è§£æè®ºæ–‡ã€ç”Ÿæˆæœç´¢æŸ¥è¯¢ã€æ£€ç´¢ç›¸å…³æ–‡çŒ®
"""
import os
import sys
import json
import argparse
from pathlib import Path
from datetime import datetime
import logging

# å¯¼å…¥æ¨¡å—
from pdf_parser_mineru import parse_pdf_to_markdown
from metadata_extractor import extract_metadata, validate_metadata
from query_generator import generate_queries_from_metadata
from arxiv_searcher import search_and_deduplicate
from literature_review.logger import get_logger


def  main():
    parser = argparse.ArgumentParser(
        description='æ–‡çŒ®ç»¼è¿°æµæ°´çº¿ - è‡ªåŠ¨è§£æè®ºæ–‡å¹¶æ£€ç´¢ç›¸å…³æ–‡çŒ®'
    )
    parser.add_argument(
        'pdf_path',
        help='PDF è®ºæ–‡è·¯å¾„'
    )
    parser.add_argument(
        '-o', '--output-dir',
        help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼šä¸PDFåŒç›®å½•ï¼‰',
        default=None
    )
    parser.add_argument(
        '-n', '--num-queries',
        type=int,
        help='ç”Ÿæˆçš„æœç´¢æŸ¥è¯¢æ•°é‡ï¼ˆé»˜è®¤ï¼š7ï¼‰',
        default=7
    )
    parser.add_argument(
        '-r', '--results-per-query',
        type=int,
        help='æ¯ä¸ªæŸ¥è¯¢è¿”å›çš„æœ€å¤§ç»“æœæ•°ï¼ˆé»˜è®¤ï¼š10ï¼‰',
        default=10
    )
    parser.add_argument(
        '-y', '--start-year',
        type=int,
        help='è®ºæ–‡èµ·å§‹å¹´ä»½ï¼ˆé»˜è®¤ï¼š2020ï¼‰',
        default=2020
    )
    parser.add_argument(
        '--skip-search',
        action='store_true',
        help='è·³è¿‡ arXiv æœç´¢ï¼Œä»…è§£æè®ºæ–‡'
    )
    parser.add_argument(
        '--format',
        choices=['json', 'csv', 'both'],
        default='both',
        help='è¾“å‡ºæ ¼å¼ï¼ˆé»˜è®¤ï¼šbothï¼‰'
    )
    
    args = parser.parse_args()

    # è·å–æ—¥å¿—å™¨
    logger = get_logger("main")

    # éªŒè¯ PDF æ–‡ä»¶
    if not os.path.exists(args.pdf_path):
        logger.error(f"âŒ é”™è¯¯: PDF æ–‡ä»¶ä¸å­˜åœ¨: {args.pdf_path}")
        sys.exit(1)
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir is None:
        args.output_dir = os.path.dirname(args.pdf_path) or '.'
    
    output_path = Path(args.output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    logger.info("=" * 70)
    logger.info("ğŸ“š æ–‡çŒ®ç»¼è¿°æµæ°´çº¿")
    logger.info("=" * 70)
    logger.info(f"PDF æ–‡ä»¶: {args.pdf_path}")
    logger.info(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    logger.info(f"æŸ¥è¯¢æ•°é‡: {args.num_queries}")
    logger.info(f"èµ·å§‹å¹´ä»½: {args.start_year}")
    logger.info("=" * 70)
    
    # æ­¥éª¤ 1: è§£æ PDF
    logger.info("\n" + "=" * 70)
    logger.info("ç¬¬ 1 æ­¥: è§£æ PDF")
    logger.info("=" * 70)

    try:
        parse_result = parse_pdf_to_markdown(args.pdf_path, args.output_dir)
        markdown_text = parse_result['markdown']
        logger.info(f"âœ… PDF è§£ææˆåŠŸï¼")
    except Exception as e:
        logger.exception(f"âŒ PDF è§£æå¤±è´¥: {e}")
        sys.exit(1)
    
    # æ­¥éª¤ 2: æå–å…ƒæ•°æ®
    logger.info("\n" + "=" * 70)
    logger.info("ç¬¬ 2 æ­¥: æå–å…ƒæ•°æ®")
    logger.info("=" * 70)

    try:
        metadata = extract_metadata(markdown_text)

        # ä¿å­˜å…ƒæ•°æ®
        metadata_path = output_path / "metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")

        # æ£€æŸ¥éªŒè¯ç»“æœ
        if not metadata['validation']['is_valid']:
            logger.warning(f"âš ï¸  è­¦å‘Š: å…ƒæ•°æ®ä¸å®Œæ•´ï¼Œç¼ºå°‘: {', '.join(metadata['validation']['missing_fields'])}")
            logger.warning("ç»§ç»­æ‰§è¡Œï¼Œä½†æœç´¢ç»“æœå¯èƒ½ä¸å‡†ç¡®")

    except Exception as e:
        logger.exception(f"âŒ å…ƒæ•°æ®æå–å¤±è´¥: {e}")
        sys.exit(1)
    
    # æ­¥éª¤ 3: ç”Ÿæˆæœç´¢æŸ¥è¯¢
    logger.info("\n" + "=" * 70)
    logger.info("ç¬¬ 3 æ­¥: ç”Ÿæˆæœç´¢æŸ¥è¯¢")
    logger.info("=" * 70)

    try:
        queries = generate_queries_from_metadata(metadata, num_queries=args.num_queries)

        # ä¿å­˜æŸ¥è¯¢
        queries_path = output_path / "search_queries.json"
        with open(queries_path, 'w', encoding='utf-8') as f:
            json.dump(queries, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… æŸ¥è¯¢å·²ä¿å­˜: {queries_path}")

    except Exception as e:
        logger.exception(f"âŒ æŸ¥è¯¢ç”Ÿæˆå¤±è´¥: {e}")
        sys.exit(1)
    
    # æ­¥éª¤ 4: æœç´¢ arXiv
    if not args.skip_search:
        logger.info("\n" + "=" * 70)
        logger.info("ç¬¬ 4 æ­¥: æœç´¢ arXiv")
        logger.info("=" * 70)

        try:
            papers = search_and_deduplicate(
                queries,
                max_results_per_query=args.results_per_query,
                start_year=args.start_year
            )

            # æ­¥éª¤ 5: å¯¼å‡ºç»“æœ
            logger.info("\n" + "=" * 70)
            logger.info("ç¬¬ 5 æ­¥: å¯¼å‡ºç»“æœ")
            logger.info("=" * 70)
            
            # JSON æ ¼å¼
            if args.format in ['json', 'both']:
                results_json_path = output_path / "related_papers.json"
                with open(results_json_path, 'w', encoding='utf-8') as f:
                    json.dump(papers, f, ensure_ascii=False, indent=2)
                logger.info(f"âœ… JSON ç»“æœå·²ä¿å­˜: {results_json_path}")
            
            # CSV æ ¼å¼
            if args.format in ['csv', 'both']:
                import csv
                results_csv_path = output_path / "related_papers.csv"
                with open(results_csv_path, 'w', newline='', encoding='utf-8') as f:
                    if papers:
                        fieldnames = ['id', 'title', 'authors', 'published', 'categories', 'arxiv_url', 'source_query']
                        writer = csv.DictWriter(f, fieldnames=fieldnames)
                        writer.writeheader()
                        for paper in papers:
                            writer.writerow({
                                'id': paper['id'],
                                'title': paper['title'],
                                'authors': '; '.join(paper['authors']),
                                'published': paper['published'],
                                'categories': '; '.join(paper['categories']),
                                'arxiv_url': paper['arxiv_url'],
                                'source_query': paper.get('source_query', '')
                            })
                logger.info(f"âœ… CSV ç»“æœå·²ä¿å­˜: {results_csv_path}")
            
            # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
            logger.info("\n" + "=" * 70)
            logger.info("ğŸ“Š å¤„ç†æ‘˜è¦")
            logger.info("=" * 70)
            logger.info(f"è®ºæ–‡æ ‡é¢˜: {metadata.get('title', 'æœªçŸ¥')}")
            logger.info(f"ä½œè€…æ•°é‡: {len(metadata.get('authors', []))}")
            logger.info(f"ç« èŠ‚æ•°é‡: {len(metadata.get('sections', []))}")
            logger.info(f"ç”ŸæˆæŸ¥è¯¢: {len(queries)} æ¡")
            logger.info(f"æ‰¾åˆ°ç›¸å…³è®ºæ–‡: {len(papers)} ç¯‡")

            if papers:
                # æŒ‰å¹´ä»½ç»Ÿè®¡
                year_stats = {}
                for paper in papers:
                    year = paper['published'][:4]
                    year_stats[year] = year_stats.get(year, 0) + 1

                logger.info(f"\næŒ‰å¹´ä»½åˆ†å¸ƒ:")
                for year in sorted(year_stats.keys(), reverse=True):
                    logger.info(f"  {year}: {year_stats[year]} ç¯‡")
        
        except Exception as e:
            logger.exception(f"âŒ arXiv æœç´¢å¤±è´¥: {e}")
            sys.exit(1)
    else:
        logger.warning("\nâ­ï¸  è·³è¿‡ arXiv æœç´¢ï¼ˆ--skip-searchï¼‰")
    
    logger.info("\n" + "=" * 70)
    logger.info("âœ… æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
    logger.info("=" * 70)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger = get_logger("main")
        logger.warning("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        logger = get_logger("main")
        logger.exception(f"\n\nâŒ æœªé¢„æœŸçš„é”™è¯¯: {e}")
        sys.exit(1)
