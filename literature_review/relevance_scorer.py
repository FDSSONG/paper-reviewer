#!/usr/bin/env python3
"""
ç›¸å…³åº¦è¯„åˆ†æ¨¡å—
ä½¿ç”¨è½»é‡çº§ embedding æ¨¡å‹è®¡ç®—è®ºæ–‡ä¹‹é—´çš„ç›¸å…³åº¦
"""
import json
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple
from sentence_transformers import SentenceTransformer
from literature_review.logger import get_logger

logger = get_logger("relevance_scorer")


class RelevanceScorer:
    """ç›¸å…³åº¦è¯„åˆ†å™¨"""
    
    def __init__(self, model_name: str = "shibing624/text2vec-base-chinese"):
        """
        åˆå§‹åŒ–è¯„åˆ†å™¨
        
        Args:
            model_name: embedding æ¨¡å‹åç§°
        """
        logger.info(f"ğŸ“Š åŠ è½½ embedding æ¨¡å‹: {model_name}...")
        self.model = SentenceTransformer(model_name)
        logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def compute_embedding(self, text: str, normalize: bool = True) -> np.ndarray:
        """
        è®¡ç®—æ–‡æœ¬çš„ embedding
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            normalize: æ˜¯å¦å½’ä¸€åŒ–ï¼ˆå½’ä¸€åŒ–åç‚¹ç§¯=ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        
        Returns:
            embedding å‘é‡
        """
        return self.model.encode(text, normalize_embeddings=normalize, convert_to_tensor=False)
    
    def compute_similarity(self, emb1: np.ndarray, emb2: np.ndarray) -> float:
        """
        è®¡ç®—ä¸¤ä¸ª embedding çš„ä½™å¼¦ç›¸ä¼¼åº¦
        
        Args:
            emb1: ç¬¬ä¸€ä¸ª embeddingï¼ˆå½’ä¸€åŒ–åï¼‰
            emb2: ç¬¬äºŒä¸ª embeddingï¼ˆå½’ä¸€åŒ–åï¼‰
        
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        # å½’ä¸€åŒ–åï¼Œç‚¹ç§¯å°±æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦
        return float(np.dot(emb1, emb2))
    
    def prepare_paper_text(self, paper: Dict) -> str:
        """
        å‡†å¤‡è®ºæ–‡æ–‡æœ¬ç”¨äº embedding
        
        Args:
            paper: è®ºæ–‡å…ƒæ•°æ®å­—å…¸
        
        Returns:
            ç»„åˆçš„æ–‡æœ¬
        """
        # ç»„åˆæ ‡é¢˜å’Œæ‘˜è¦ï¼ˆæ‘˜è¦æƒé‡æ›´é«˜ï¼‰
        title = paper.get('title', '')
        abstract = paper.get('abstract', '')
        
        # æ ‡é¢˜é‡å¤2æ¬¡ä»¥å¢åŠ æƒé‡
        text = f"{title} {title} {abstract}"
        return text
    
    def score_papers(
        self,
        source_paper: Dict,
        candidate_papers: List[Dict],
        batch_size: int = 32
    ) -> List[Tuple[Dict, float]]:
        """
        ä¸ºå€™é€‰è®ºæ–‡è®¡ç®—ç›¸å…³åº¦åˆ†æ•°
        
        Args:
            source_paper: æºè®ºæ–‡å…ƒæ•°æ®
            candidate_papers: å€™é€‰è®ºæ–‡åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
        
        Returns:
            [(è®ºæ–‡, ç›¸å…³åº¦åˆ†æ•°), ...] æŒ‰åˆ†æ•°é™åºæ’åˆ—
        """
        logger.info(f"ğŸ” è®¡ç®—ç›¸å…³åº¦åˆ†æ•°...")
        logger.info(f"æºè®ºæ–‡: {source_paper.get('title', 'Unknown')[:60]}...")
        logger.info(f"å€™é€‰è®ºæ–‡æ•°: {len(candidate_papers)}")
        
        # è®¡ç®—æºè®ºæ–‡çš„ embedding
        source_text = self.prepare_paper_text(source_paper)
        source_emb = self.compute_embedding(source_text)
        
        # æ‰¹é‡è®¡ç®—å€™é€‰è®ºæ–‡çš„ embeddingï¼ˆå½’ä¸€åŒ–ï¼‰
        candidate_texts = [self.prepare_paper_text(p) for p in candidate_papers]
        
        logger.info(f"æ­£åœ¨è®¡ç®— {len(candidate_texts)} ç¯‡è®ºæ–‡çš„ embedding...")
        candidate_embs = self.model.encode(
            candidate_texts,
            batch_size=batch_size,
            normalize_embeddings=True,  # å½’ä¸€åŒ–
            show_progress_bar=True,
            convert_to_tensor=False
        )
        
        # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆå½’ä¸€åŒ–åï¼Œç‚¹ç§¯å°±æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        logger.info("æ­£åœ¨è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°...")
        
        # è½¬ä¸º numpy æ•°ç»„è¿›è¡Œæ‰¹é‡è®¡ç®—
        candidate_embs = np.array(candidate_embs)  # (n, 768)
        
        # æ‰¹é‡ç‚¹ç§¯: (n, 768) @ (768,) = (n,)
        similarity_scores = candidate_embs @ source_emb
        
        # ç»„è£…ç»“æœ
        scores = list(zip(candidate_papers, similarity_scores))
        
        # æŒ‰åˆ†æ•°é™åºæ’åº
        scores.sort(key=lambda x: x[1], reverse=True)
        
        logger.info(f"âœ… å®Œæˆï¼åˆ†æ•°èŒƒå›´: {scores[-1][1]:.3f} - {scores[0][1]:.3f}")
        
        return scores
    
    def filter_by_threshold(
        self,
        scored_papers: List[Tuple[Dict, float]],
        threshold: float = 0.5
    ) -> Tuple[List[Tuple[Dict, float]], List[Tuple[Dict, float]]]:
        """
        æ ¹æ®é˜ˆå€¼åˆ†ç¦»é«˜ç›¸å…³åº¦å’Œä½ç›¸å…³åº¦è®ºæ–‡
        
        Args:
            scored_papers: å·²è¯„åˆ†çš„è®ºæ–‡åˆ—è¡¨
            threshold: ç›¸å…³åº¦é˜ˆå€¼
        
        Returns:
            (é«˜ç›¸å…³åº¦è®ºæ–‡, ä½ç›¸å…³åº¦è®ºæ–‡)
        """
        high_relevance = [(p, s) for p, s in scored_papers if s >= threshold]
        low_relevance = [(p, s) for p, s in scored_papers if s < threshold]
        
        logger.info(f"ğŸ“Š ç›¸å…³åº¦åˆ†çº§ï¼ˆé˜ˆå€¼={threshold}ï¼‰: é«˜ç›¸å…³åº¦ {len(high_relevance)} ç¯‡, ä½ç›¸å…³åº¦ {len(low_relevance)} ç¯‡")
        
        return high_relevance, low_relevance
    
    def select_top_k(
        self,
        scored_papers: List[Tuple[Dict, float]],
        k: int = 15
    ) -> Tuple[List[Tuple[Dict, float]], List[Tuple[Dict, float]]]:
        """
        é€‰æ‹© top-k é«˜ç›¸å…³åº¦è®ºæ–‡
        
        Args:
            scored_papers: å·²è¯„åˆ†çš„è®ºæ–‡åˆ—è¡¨
            k: é€‰æ‹©çš„è®ºæ–‡æ•°é‡
        
        Returns:
            (top-k è®ºæ–‡, å…¶ä»–è®ºæ–‡)
        """
        top_k = scored_papers[:k]
        others = scored_papers[k:]
        
        logger.info(f"ğŸ“Š é€‰æ‹© Top-{k} è®ºæ–‡: é«˜ç›¸å…³åº¦ {len(top_k)} ç¯‡, ä½ç›¸å…³åº¦ {len(others)} ç¯‡")
        
        if top_k:
            logger.info(f"åˆ†æ•°èŒƒå›´: {top_k[-1][1]:.3f} - {top_k[0][1]:.3f}")
        
        return top_k, others
    
    def save_scores(
        self,
        scored_papers: List[Tuple[Dict, float]],
        output_path: Path
    ):
        """
        ä¿å­˜è¯„åˆ†ç»“æœ
        
        Args:
            scored_papers: å·²è¯„åˆ†çš„è®ºæ–‡åˆ—è¡¨
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        results = []
        for paper, score in scored_papers:
            results.append({
                'arxiv_id': paper.get('arxiv_id', paper.get('id', 'unknown')),
                'title': paper['title'],
                'score': float(score),
                'published': paper.get('published', ''),
                'authors': paper.get('authors', [])[:3]  # åªä¿å­˜å‰3ä½ä½œè€…
            })
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… è¯„åˆ†ç»“æœå·²ä¿å­˜: {output_path}")


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # ç¤ºä¾‹æ•°æ®
    source = {
        'title': 'Attention Is All You Need',
        'abstract': 'We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.'
    }
    
    candidates = [
        {
            'arxiv_id': '1706.03762',
            'title': 'BERT: Pre-training of Deep Bidirectional Transformers',
            'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.',
            'authors': ['Jacob Devlin'],
            'published': '2018-10-11'
        },
        {
            'arxiv_id': '2005.14165',
            'title': 'GPT-3: Language Models are Few-Shot Learners',
            'abstract': 'Recent work has demonstrated substantial gains on many NLP tasks using pre-training.',
            'authors': ['Tom Brown'],
            'published': '2020-05-28'
        },
        {
            'arxiv_id': '1512.03385',
            'title': 'Deep Residual Learning for Image Recognition',
            'abstract': 'Deep neural networks are difficult to train. We present a residual learning framework.',
            'authors': ['Kaiming He'],
            'published': '2015-12-10'
        }
    ]
    
    print("=" * 70)
    print("ç›¸å…³åº¦è¯„åˆ†æµ‹è¯•")
    print("=" * 70)
    
    scorer = RelevanceScorer()
    
    # è®¡ç®—åˆ†æ•°
    scored = scorer.score_papers(source, candidates)
    
    # æ˜¾ç¤ºç»“æœ
    print("\næ’åºç»“æœ:")
    for i, (paper, score) in enumerate(scored, 1):
        print(f"{i}. [{score:.3f}] {paper['title'][:60]}...")
    
    # é€‰æ‹© top-2
    top_k, others = scorer.select_top_k(scored, k=2)
    
    print("\nTop-2 é«˜ç›¸å…³åº¦è®ºæ–‡:")
    for paper, score in top_k:
        print(f"  - [{score:.3f}] {paper['title']}")
