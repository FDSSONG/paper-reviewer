#!/usr/bin/env python3
"""
ArXiv æœç´¢æ¨¡å— - ç±»å°è£…ç‰ˆæœ¬
ä½¿ç”¨ arXiv API æœç´¢ç›¸å…³è®ºæ–‡å¹¶æ‰¹é‡è·å–å…ƒæ•°æ®
"""
import time
import csv
import urllib.parse
import urllib.request
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
from literature_review.logger import get_logger

logger = get_logger("arxiv_searcher")


class ArxivSearcher:
    """ArXiv æœç´¢å™¨ç±»"""
    
    def __init__(self, delay: float = 5.0, max_retries: int = 3):
        """
        åˆå§‹åŒ–æœç´¢å™¨1
        
        Args:
            delay: æŸ¥è¯¢ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œé¿å…è§¦å‘é€Ÿç‡é™åˆ¶
            max_retries: è¯·æ±‚å¤±è´¥æ—¶çš„æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.delay = delay
        self.max_retries = max_retries
    
    def search(
        self,
        query: str,
        max_results: int = 20,
        since_year: int = 2020,
        sort_by: str = "relevance"
    ) -> List[Dict]:
        """
        åœ¨ arXiv ä¸Šæœç´¢è®ºæ–‡ï¼ˆå¸¦æŒ‡æ•°é€€é¿é‡è¯•ï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
            max_results: æœ€å¤§è¿”å›ç»“æœæ•°
            since_year: èµ·å§‹å¹´ä»½ï¼ˆé»˜è®¤2020ï¼‰
            sort_by: æ’åºæ–¹å¼ "relevance" æˆ– "lastUpdatedDate"
        
        Returns:
            [
                {
                    "arxiv_id": "2301.12345",
                    "title": "Paper Title",
                    "authors": ["Author 1", "Author 2"],
                    "abstract": "Abstract text...",
                    "published": "2023-01-15",
                    "updated": "2023-01-20",
                    "categories": ["cs.AI", "cs.LG"],
                    "pdf_url": "https://arxiv.org/pdf/2301.12345",
                    "arxiv_url": "https://arxiv.org/abs/2301.12345"
                },
                ...
            ]
        """
        # æ„å»ºæœç´¢ URL
        base_url = "http://export.arxiv.org/api/query?"
        
        # arXiv API å‚æ•°
        params = {
            'search_query': f'all:{query}',
            'start': 0,
            'max_results': max_results,
            'sortBy': sort_by,
            'sortOrder': 'descending'
        }
        
        url = base_url + urllib.parse.urlencode(params)
        
        last_error = None
        for attempt in range(self.max_retries + 1):
            try:
                # å‘é€è¯·æ±‚
                with urllib.request.urlopen(url, timeout=30) as response:
                    xml_data = response.read()
                
                # è§£æ XML
                root = ET.fromstring(xml_data)
                
                # å‘½åç©ºé—´
                ns = {
                    'atom': 'http://www.w3.org/2005/Atom',
                    'arxiv': 'http://arxiv.org/schemas/atom'
                }
                
                papers = []
                
                for entry in root.findall('atom:entry', ns):
                    # æå–åŸºæœ¬ä¿¡æ¯
                    title = entry.find('atom:title', ns).text.strip().replace('\n', ' ')
                    
                    # æå–ä½œè€…
                    authors = []
                    for author in entry.findall('atom:author', ns):
                        name = author.find('atom:name', ns).text
                        authors.append(name)
                    
                    # æ‘˜è¦
                    abstract = entry.find('atom:summary', ns).text.strip().replace('\n', ' ')
                    
                    # å‘å¸ƒå’Œæ›´æ–°æ—¥æœŸ
                    published = entry.find('atom:published', ns).text
                    updated = entry.find('atom:updated', ns).text
                    
                    # æå–å¹´ä»½è¿›è¡Œè¿‡æ»¤
                    pub_year = int(published[:4])
                    if pub_year < since_year:
                        continue
                    
                    # IDï¼ˆä»é“¾æ¥ä¸­æå–ï¼‰
                    arxiv_id = entry.find('atom:id', ns).text.split('/abs/')[-1]
                    
                    # åˆ†ç±»
                    categories = []
                    for category in entry.findall('atom:category', ns):
                        categories.append(category.get('term'))
                    
                    # PDFé“¾æ¥
                    pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
                    
                    paper = {
                        'arxiv_id': arxiv_id,
                        'title': title,
                        'authors': authors,
                        'abstract': abstract,
                        'published': published[:10],  # YYYY-MM-DD
                        'updated': updated[:10],
                        'categories': categories,
                        'pdf_url': pdf_url,
                        'arxiv_url': f"https://arxiv.org/abs/{arxiv_id}"
                    }
                    
                    papers.append(paper)
                
                return papers
            
            except Exception as e:
                last_error = e
                error_str = str(e)
                # åˆ¤æ–­æ˜¯å¦ä¸ºå¯é‡è¯•çš„é”™è¯¯ (429/503/IncompleteRead/è¿æ¥é—®é¢˜)
                retryable = any(keyword in error_str for keyword in [
                    '429', '503', 'IncompleteRead', 'Connection',
                    'Too Many Requests', 'Service Unavailable',
                    'RemoteDisconnected', 'ConnectionReset', 'SSL', 'EOF',
                    'timed out', 'timeout'
                ])

                if retryable and attempt < self.max_retries:
                    wait = self.delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿: 5s, 10s, 20s
                    logger.warning(f"arXiv è¯·æ±‚å¤±è´¥: {e}")
                    logger.info(f"ç¬¬ {attempt + 1} æ¬¡é‡è¯•ï¼Œç­‰å¾… {wait:.0f}s...")
                    time.sleep(wait)
                else:
                    logger.exception(f"arXiv æœç´¢å¤±è´¥: {e}")
                    return []

        logger.exception(f"arXiv æœç´¢å¤±è´¥ï¼ˆå·²é‡è¯• {self.max_retries} æ¬¡ï¼‰: {last_error}")
        return []
    
    def batch_search(
        self,
        queries: List[str],
        max_results_per_query: int = 10,
        since_year: int = 2020
    ) -> Dict[str, List[Dict]]:
        """
        æ‰¹é‡æœç´¢å¤šä¸ªæŸ¥è¯¢
        
        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
            max_results_per_query: æ¯ä¸ªæŸ¥è¯¢çš„æœ€å¤§ç»“æœæ•°
            since_year: èµ·å§‹å¹´ä»½
        
        Returns:
            {
                "query1": [paper1, paper2, ...],
                "query2": [paper1, paper2, ...],
                ...
            }
        """
        logger.info(f"ğŸ” å¼€å§‹æ‰¹é‡æœç´¢ {len(queries)} ä¸ªæŸ¥è¯¢...")
        
        results = {}
        
        for i, query in enumerate(queries, 1):
            # å¦‚æœqueryæ˜¯å­—å…¸ï¼Œæå–queryå­—ç¬¦ä¸²
            if isinstance(query, dict):
                query_str = query.get('query', str(query))
                perspective = query.get('perspective', 'unknown')
            else:
                query_str = str(query)
                perspective = 'unknown'
            
            logger.info(f"[{i}/{len(queries)}] æœç´¢: {query_str}")
            if perspective != 'unknown':
                logger.info(f"è§†è§’: {perspective}")
            
            papers = self.search(
                query=query_str,
                max_results=max_results_per_query,
                since_year=since_year
            )
            
            results[query_str] = papers
            logger.info(f"æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
            
            # å»¶è¿Ÿä»¥é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
            if i < len(queries):
                time.sleep(self.delay)
        
        return results
    
    def deduplicate(self, results: Dict[str, List[Dict]]) -> List[Dict]:
        """
        å»é‡è®ºæ–‡ç»“æœ
        
        Args:
            results: æ‰¹é‡æœç´¢çš„ç»“æœ
        
        Returns:
            å»é‡åçš„è®ºæ–‡åˆ—è¡¨
        """
        seen_ids = set()
        unique_papers = []
        
        for query, papers in results.items():
            for paper in papers:
                if paper['arxiv_id'] not in seen_ids:
                    seen_ids.add(paper['arxiv_id'])
                    # æ·»åŠ æ¥æºæŸ¥è¯¢ä¿¡æ¯
                    paper['source_query'] = query
                    unique_papers.append(paper)
        
        return unique_papers
    
    def search_and_deduplicate(
        self,
        queries: List[str],
        max_results_per_query: int = 10,
        since_year: int = 2020
    ) -> List[Dict]:
        """
        æœç´¢å¹¶å»é‡ï¼ˆä¸€ç«™å¼å‡½æ•°ï¼‰
        
        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
            max_results_per_query: æ¯ä¸ªæŸ¥è¯¢çš„æœ€å¤§ç»“æœæ•°
            since_year: èµ·å§‹å¹´ä»½
        
        Returns:
            å»é‡åçš„è®ºæ–‡åˆ—è¡¨
        """
        results = self.batch_search(queries, max_results_per_query, since_year)
        unique_papers = self.deduplicate(results)
        
        logger.info(f"âœ… æœç´¢å®Œæˆï¼æ€»æŸ¥è¯¢æ•°: {len(queries)}, å»é‡å‰: {sum(len(papers) for papers in results.values())} ç¯‡, å»é‡å: {len(unique_papers)} ç¯‡")
        
        return unique_papers
    
    def save_to_csv(self, papers: List[Dict], output_path: str):
        """
        ä¿å­˜è®ºæ–‡ç»“æœåˆ° CSV æ–‡ä»¶
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if not papers:
            logger.warning("æ²¡æœ‰è®ºæ–‡å¯ä¿å­˜")
            return
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', newline='', encoding='utf-8') as f:
            # CSV å­—æ®µ
            fields = [
                'arxiv_id', 'title', 'authors', 'published', 
                'categories', 'abstract', 'pdf_url', 'arxiv_url'
            ]
            
            writer = csv.DictWriter(f, fieldnames=fields)
            writer.writeheader()
            
            for paper in papers:
                # å¤„ç†åˆ—è¡¨å­—æ®µï¼ˆä½œè€…å’Œåˆ†ç±»ï¼‰
                row = {
                    'arxiv_id': paper['arxiv_id'],
                    'title': paper['title'],
                    'authors': '; '.join(paper['authors']),
                    'published': paper['published'],
                    'categories': '; '.join(paper['categories']),
                    'abstract': paper['abstract'],
                    'pdf_url': paper['pdf_url'],
                    'arxiv_url': paper['arxiv_url']
                }
                writer.writerow(row)
        
        logger.info(f"âœ… å·²ä¿å­˜ {len(papers)} ç¯‡è®ºæ–‡åˆ°: {output_path}")


# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸæœ‰çš„å‡½æ•°æ¥å£
def search_arxiv(query: str, max_results: int = 20, start_year: int = 2020, sort_by: str = "relevance") -> List[Dict]:
    """å‘åå…¼å®¹çš„å‡½æ•°æ¥å£"""
    searcher = ArxivSearcher()
    return searcher.search(query, max_results, start_year, sort_by)


def search_and_deduplicate(queries: List[Dict[str, str]], max_results_per_query: int = 10, start_year: int = 2020) -> List[Dict]:
    """å‘åå…¼å®¹çš„å‡½æ•°æ¥å£"""
    searcher = ArxivSearcher()
    return searcher.search_and_deduplicate(queries, max_results_per_query, start_year)


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    import json
    
    print("=" * 60)
    print("arXiv æœç´¢æµ‹è¯•")
    print("=" * 60)
    
    searcher = ArxivSearcher()
    
    # æµ‹è¯•å•ä¸ªæŸ¥è¯¢
    print("\næµ‹è¯•1: å•ä¸ªæŸ¥è¯¢")
    print("-" * 60)
    papers = searcher.search("transformer attention mechanism", max_results=5)
    print(f"æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
    if papers:
        print(f"\nç¬¬ä¸€ç¯‡è®ºæ–‡:")
        print(json.dumps(papers[0], ensure_ascii=False, indent=2))
    
    # æµ‹è¯•æ‰¹é‡æŸ¥è¯¢
    print("\n\næµ‹è¯•2: æ‰¹é‡æŸ¥è¯¢")
    print("-" * 60)
    test_queries = [
        "deep learning image classification",
        "convolutional neural networks"
    ]
    
    unique_papers = searcher.search_and_deduplicate(test_queries, max_results_per_query=3, since_year=2023)
    print(f"\nå»é‡åçš„è®ºæ–‡ID:")
    for paper in unique_papers:
        print(f"  - {paper['arxiv_id']}: {paper['title'][:60]}...")
