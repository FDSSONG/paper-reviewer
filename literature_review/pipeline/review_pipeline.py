"""
è‡ªåŠ¨åŒ–è¯„å®¡æµæ°´çº¿

å®Œæ•´æµç¨‹ï¼š
1. è¯»å–åŸå§‹è®ºæ–‡ Markdown
2. è¯»å–ç›¸å…³æ–‡çŒ®æ‘˜è¦
3. è°ƒç”¨ LLM ç”Ÿæˆç»“æ„åŒ–è¯„å®¡æŠ¥å‘Š
4. ä¿å­˜è¯„å®¡æŠ¥å‘Š
"""

import json
import sys
from pathlib import Path
from datetime import datetime
from literature_review.logger import get_logger

logger = get_logger("review_pipeline")

# å¯¼å…¥å·²æœ‰çš„æ¨¡å—
sys.path.append(str(Path(__file__).parent.parent))
from literature_review.review_generator import ReviewGenerator


def run_review_generation(
    paper_id: str,
    language: str = "chinese",
    input_dir: str = None
):
    """
    è¿è¡Œè¯„å®¡ç”Ÿæˆæµæ°´çº¿
    
    Args:
        paper_id: è®ºæ–‡ID
        language: è¯„å®¡è¯­è¨€ ('chinese' æˆ– 'english')
        input_dir: è¾“å…¥ç›®å½•ï¼ˆé»˜è®¤ä¸º pipeline/literature_search_resultsï¼‰
    """
    logger.info("â•" * 70)
    logger.info("ğŸ“ è‡ªåŠ¨åŒ–è¯„å®¡æŠ¥å‘Šç”Ÿæˆæµæ°´çº¿")
    logger.info("â•" * 70)
    logger.info(f"è®ºæ–‡ ID: {paper_id} | è¯„å®¡è¯­è¨€: {language}")
    
    # è®¾ç½®è·¯å¾„
    if input_dir is None:
        input_dir = Path(__file__).parent / "literature_search_results"
    else:
        input_dir = Path(input_dir)
    
    paper_dir = input_dir / paper_id
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not paper_dir.exists():
        logger.error(f"æ‰¾ä¸åˆ°è®ºæ–‡ç›®å½• {paper_dir}")
        logger.error("è¯·å…ˆè¿è¡Œå‰é¢çš„æµç¨‹ï¼ˆæ–‡çŒ®æ£€ç´¢å’Œç›¸å…³åº¦æ‰“åˆ†ï¼‰")
        return
    
    # ===== ç¬¬ 1 æ­¥: åŠ è½½åŸå§‹è®ºæ–‡ =====
    logger.info("â•" * 70)
    logger.info("ç¬¬ 1 æ­¥: åŠ è½½åŸå§‹è®ºæ–‡")
    logger.info("â•" * 70)
    
    # æŸ¥æ‰¾åŸå§‹è®ºæ–‡çš„ Markdown æ–‡ä»¶
    source_md_path = None
    
    # å°è¯•å¤šä¸ªå¯èƒ½çš„ä½ç½®
    possible_paths = [
        paper_dir / f"{paper_id}_source.md",  # literature_search_pipeline ä¿å­˜çš„ä½ç½®
        Path(__file__).parent / "outputs" / paper_id / "full.md",  # mineru_pipeline è¾“å‡ºä½ç½®
    ]
    
    for path in possible_paths:
        if path.exists():
            source_md_path = path
            break
    
    if not source_md_path:
        logger.error("æ‰¾ä¸åˆ°åŸå§‹è®ºæ–‡çš„ Markdown æ–‡ä»¶")
        logger.error("å°è¯•è¿‡çš„è·¯å¾„:")
        for p in possible_paths:
            logger.error(f"  - {p}")
        return
    
    logger.info(f"âœ… æ‰¾åˆ°åŸå§‹è®ºæ–‡: {source_md_path}")
    
    with open(source_md_path, 'r', encoding='utf-8') as f:
        source_md = f.read()
    
    logger.info(f"é•¿åº¦: {len(source_md)} å­—ç¬¦")
    
    # åŠ è½½å…ƒæ•°æ®
    metadata_path = paper_dir / "metadata.json"
    with open(metadata_path, 'r', encoding='utf-8') as f:
        source_metadata = json.load(f)
    
    logger.info(f"æ ‡é¢˜: {source_metadata['title']}")
    
    # ===== ç¬¬ 2 æ­¥: åŠ è½½ç›¸å…³æ–‡çŒ®æ‘˜è¦ =====
    logger.info("â•" * 70)
    logger.info("ç¬¬ 2 æ­¥: åŠ è½½ç›¸å…³æ–‡çŒ®æ‘˜è¦")
    logger.info("â•" * 70)
    
    # ä» ranked_papers.json è·å–é«˜ç›¸å…³åº¦è®ºæ–‡åˆ—è¡¨
    ranked_path = paper_dir / "ranked_papers.json"
    
    if not ranked_path.exists():
        logger.error("æ‰¾ä¸åˆ° ranked_papers.json")
        logger.error("è¯·å…ˆè¿è¡Œ ranking_and_summary_pipeline.py")
        return
    
    with open(ranked_path, 'r', encoding='utf-8') as f:
        ranked_data = json.load(f)
    
    high_rel_papers = ranked_data.get('high_relevance', [])
    logger.info(f"é«˜ç›¸å…³åº¦è®ºæ–‡æ•°: {len(high_rel_papers)}")
    
    # è¯»å–æ¯ç¯‡è®ºæ–‡çš„è¯¦ç»†æ‘˜è¦
    related_summaries = []
    high_rel_dir = paper_dir / "high_relevance"
    
    for item in high_rel_papers:
        arxiv_id = item['arxiv_id']
        summary_path = high_rel_dir / arxiv_id / "detailed_summary.md"
        
        if summary_path.exists():
            with open(summary_path, 'r', encoding='utf-8') as f:
                # è·³è¿‡å…ƒæ•°æ®å¤´éƒ¨ï¼Œåªè¯»å–æ‘˜è¦å†…å®¹
                content = f.read()
                # æå– "# å¯¹æ¯”æ‘˜è¦" åçš„å†…å®¹
                if "# å¯¹æ¯”æ‘˜è¦" in content:
                    summary = content.split("# å¯¹æ¯”æ‘˜è¦", 1)[1].strip()
                else:
                    summary = content
                
            related_summaries.append({
                'arxiv_id': arxiv_id,
                'title': item['title'],
                'score': item['score'],
                'summary': summary
            })
            logger.info(f"âœ… åŠ è½½æ‘˜è¦: {arxiv_id}")
        else:
            logger.warning(f"æœªæ‰¾åˆ°æ‘˜è¦: {arxiv_id}")
    
    logger.info(f"æˆåŠŸåŠ è½½ {len(related_summaries)} ç¯‡é«˜ç›¸å…³åº¦æ–‡çŒ®æ‘˜è¦")
    
    # åŠ è½½ä½ç›¸å…³åº¦è®ºæ–‡æ‘˜è¦
    low_rel_path = paper_dir / "low_relevance_summaries.json"
    low_rel_summaries = []
    if low_rel_path.exists():
        with open(low_rel_path, 'r', encoding='utf-8') as f:
            low_rel_summaries = json.load(f)
        logger.info(f"ä½ç›¸å…³åº¦è®ºæ–‡æ•°: {len(low_rel_summaries)}")
        for item in low_rel_summaries:
            related_summaries.append({
                'arxiv_id': item['arxiv_id'],
                'title': item['title'],
                'score': item['score'],
                'summary': item['summary']
            })
    else:
        logger.warning("æœªæ‰¾åˆ°ä½ç›¸å…³åº¦æ‘˜è¦æ–‡ä»¶ï¼Œä»…ä½¿ç”¨é«˜ç›¸å…³åº¦æ–‡çŒ®")
    
    logger.info(f"æ€»è®¡ {len(related_summaries)} ç¯‡æ–‡çŒ®å‚ä¸è¯„å®¡")
    
    # ===== ç¬¬ 3 æ­¥: ç”Ÿæˆè¯„å®¡æŠ¥å‘Š =====
    logger.info("â•" * 70)
    logger.info("ç¬¬ 3 æ­¥: ç”Ÿæˆè¯„å®¡æŠ¥å‘Š")
    logger.info("â•" * 70)
    
    generator = ReviewGenerator()
    
    review = generator.generate_review(
        source_md,
        source_metadata,
        related_summaries,
        language=language
    )
    

    
    # ===== ç¬¬ 4 æ­¥: ä¿å­˜è¯„å®¡æŠ¥å‘Š =====
    logger.info("â•" * 70)
    logger.info("ç¬¬ 4 æ­¥: ä¿å­˜è¯„å®¡æŠ¥å‘Š")
    logger.info("â•" * 70)
    
    # ä¿å­˜è¯„å®¡æŠ¥å‘Š
    review_filename = f"review_report_{language}.md"
    review_path = paper_dir / review_filename
    
    review_metadata = {
        'title': source_metadata['title'],
        'authors': source_metadata.get('authors', []),
        'num_related_papers': len(related_summaries),
        'language': language
    }
    
    generator.save_review(review, review_path, metadata=review_metadata)
    
    # ä¿å­˜å…ƒæ•°æ®
    review_meta_path = paper_dir / "review_metadata.json"
    with open(review_meta_path, 'w', encoding='utf-8') as f:
        json.dump({
            'paper_id': paper_id,
            'generated_at': datetime.now().isoformat(),
            'language': language,
            'num_related_papers': len(related_summaries),
            'review_file': review_filename
        }, f, ensure_ascii=False, indent=2)
    
    logger.info(f"âœ… å…ƒæ•°æ®å·²ä¿å­˜: {review_meta_path}")

    logger.info("â•" * 70)
    logger.info("âœ… å®Œæˆï¼è¯„å®¡æŠ¥å‘Šå·²ç”Ÿæˆ")
    logger.info("â•" * 70)
    logger.info(f"ğŸ“„ è¯„å®¡æŠ¥å‘Š: {review_path.resolve()}")
    logger.info(f"ğŸ“Š å…ƒæ•°æ®: {review_meta_path.resolve()}")
    logger.info("å»ºè®®ï¼šæŸ¥çœ‹è¯„å®¡æŠ¥å‘Šå¹¶æ ¹æ®éœ€è¦è°ƒæ•´")


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='è‡ªåŠ¨åŒ–è¯„å®¡æŠ¥å‘Šç”Ÿæˆæµæ°´çº¿'
    )
    
    parser.add_argument(
        'paper_id',
        help='è®ºæ–‡IDï¼ˆä¾‹å¦‚ï¼š2401.12345ï¼‰'
    )
    
    parser.add_argument(
        '-l', '--language',
        choices=['chinese', 'english'],
        default='chinese',
        help='è¯„å®¡è¯­è¨€ï¼ˆé»˜è®¤: chineseï¼‰'
    )
    
    parser.add_argument(
        '-i', '--input-dir',
        default=None,
        help='è¾“å…¥ç›®å½•ï¼ˆé»˜è®¤: pipeline/literature_search_resultsï¼‰'
    )
    
    args = parser.parse_args()
    
    run_review_generation(
        paper_id=args.paper_id,
        language=args.language,
        input_dir=args.input_dir
    )


if __name__ == '__main__':
    main()
