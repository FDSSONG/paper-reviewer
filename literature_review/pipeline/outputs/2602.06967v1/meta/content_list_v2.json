[
    [
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "Leveraging Adaptive Group Negotiation for Heterogeneous Multi-Robot Collaboration with Large Language Models "
                    }
                ],
                "level": 1
            },
            "bbox": [
                191,
                89,
                805,
                130
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Siqi Song1∗ Xuanbing Xie2∗ Zonglin "
                    },
                    {
                        "type": "equation_inline",
                        "content": "\\mathbf { L i ^ { 3 * } }"
                    },
                    {
                        "type": "text",
                        "content": "Yuqiang "
                    },
                    {
                        "type": "equation_inline",
                        "content": "\\mathbf { L i ^ { 3 } }"
                    },
                    {
                        "type": "text",
                        "content": "Shijie Wang3† Biqing "
                    },
                    {
                        "type": "equation_inline",
                        "content": "\\mathbf { Q } \\mathbf { i } ^ { 3 \\dagger }"
                    }
                ]
            },
            "bbox": [
                142,
                149,
                852,
                170
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "1 Tsinghua University "
                    }
                ]
            },
            "bbox": [
                169,
                184,
                352,
                202
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "2 Central South University "
                    }
                ]
            },
            "bbox": [
                373,
                184,
                589,
                202
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "3 Shanghai AI Laboratory "
                    }
                ]
            },
            "bbox": [
                611,
                185,
                823,
                202
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "∗ Equal Contribution. "
                    }
                ]
            },
            "bbox": [
                169,
                203,
                332,
                218
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Equal Advising. Correspondence: songsq21@mails.tsinghua.edu.cn "
                    }
                ]
            },
            "bbox": [
                334,
                203,
                821,
                219
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "Abstract "
                    }
                ],
                "level": 1
            },
            "bbox": [
                260,
                260,
                339,
                275
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Multi-robot collaboration tasks often require heterogeneous robots to work together over long horizons under spatial constraints and environmental uncertainties. Although Large Language Models (LLMs) excel at reasoning and planning, their potential for coordinated control has not been fully explored. Inspired by human teamwork, we present CLiMRS (Cooperative Large-Language-Model-Driven Heterogeneous Multi-Robot System) , an adaptive group negotiation framework among LLMs for multi-robot collaboration. This framework pairs each robot with an LLM agent and dynamically forms subgroups through a general proposal planner. Within each subgroup, a subgroup manager leads perception-driven multi-LLM discussions to get commands for actions. Feedback is provided by both robot execution outcomes and environment changes. This grouping–planning–execution–feedback loop enables efficient planning and robust execution. To evaluate these capabilities, we introduce CLiMBench, a heterogeneous multi-robot benchmark of challenging assembly tasks. Our experiments show that CLiMRS surpasses the best baseline, achieving over "
                    },
                    {
                        "type": "equation_inline",
                        "content": "40 \\%"
                    },
                    {
                        "type": "text",
                        "content": "higher efficiency on complex tasks without sacrificing success on simpler ones. Our results demonstrate that leveraging human-inspired group formation and negotiation principles markedly enhances the efficiency of heterogeneous multirobot collaboration. Our code is available here. "
                    }
                ]
            },
            "bbox": [
                142,
                285,
                460,
                740
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "1 Introduction "
                    }
                ],
                "level": 1
            },
            "bbox": [
                114,
                751,
                258,
                766
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Addressing real-world, everyday tasks often requires collaboration to efficiently handle longhorizon planning and complex perception. However, developing embodied agents for multi-robot tasks remains an open challenge. Inspired by human teamwork, incorporating human teaming principles into multi-robot collaboration, where groups of robotic agents coordinate planning and perception through shared observations and information, "
                    }
                ]
            },
            "bbox": [
                112,
                776,
                490,
                921
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "offers a promising yet challenging path to improving efficiency and robustness (Zhang et al., 2024b). "
                    }
                ]
            },
            "bbox": [
                507,
                261,
                884,
                293
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Meanwhile, large language models (LLMs) have demonstrated strong capabilities across a range of tasks, including question answering (Rein et al., 2024), code generation (Jain et al., 2024), and logical reasoning (Plaat et al., 2024). Recent work has integrated LLMs into robotic planning (Song et al., 2023; Zhang et al., 2024a; Mower et al., 2024; Salimpour et al., 2025; Liang et al., 2025), with several studies extending these approaches to multi-robot collaboration (Zhang et al., 2024b; Mandi et al., 2024; Liu et al., 2025). However, previous work has mainly focused on homogeneous agents (Liu et al., 2024a), limiting the diversity of capabilities that can be exhibited during collaboration. Moreover, existing work on heterogeneous teams often assumes idealized operating conditions (Liu et al., 2025), overlooking cumulative errors over long horizons, underestimating communication costs, and overestimating cooperative efficiency. As a result, despite the promise of LLMdriven multi-robot collaboration, significant gaps remain under heterogeneous agents, long-horizon objectives, and real-world constraints. "
                    }
                ]
            },
            "bbox": [
                507,
                294,
                884,
                662
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "To address these limitations, we propose CLiMRS (Cooperative Large-Language-Model-Driven Heterogeneous Multi-Robot System), an adaptive group negotiation framework among multiple LLMs that enables multi-robot collaboration. CLiMRS orchestrates heterogeneous robots via dynamic subgroup formation and cooperative planning, enabling robust long-horizon collaboration in uncertain environments. Within this framework, each robot is paired with an independent LLM agent that communicates with its peers to accomplish complex, long-horizon tasks. To enhance collaborative effectiveness, the system leverages the broad world knowledge of LLMs and explicitly models inter-agent dependencies through a grouping–planning–execution–feedback loop. "
                    }
                ]
            },
            "bbox": [
                507,
                664,
                885,
                921
            ]
        },
        {
            "type": "page_aside_text",
            "content": {
                "page_aside_text_content": [
                    {
                        "type": "text",
                        "content": "arXiv:2602.06967v1 [cs.RO] 29 Dec 2025 "
                    }
                ]
            },
            "bbox": [
                21,
                287,
                55,
                708
            ]
        },
        {
            "type": "page_number",
            "content": {
                "page_number_content": [
                    {
                        "type": "text",
                        "content": "1 "
                    }
                ]
            },
            "bbox": [
                492,
                942,
                504,
                954
            ]
        }
    ],
    [
        {
            "type": "image",
            "content": {
                "image_source": {
                    "path": "images/e882f2f458ac757d295a9f0f73639d7d573309e018e91942101baa12349c299d.jpg"
                },
                "image_caption": [
                    {
                        "type": "text",
                        "content": "Figure 1: Overview. We present CLiMRS, an adaptive group negotiation framework among multiple LLMs that enables multi-robot collaboration through a grouping–planning–execution–feedback loop, and CLiMBench, a heterogeneous multi-robot collaboration benchmark in simulation with challenging long-horizon assembly tasks. "
                    }
                ],
                "image_footnote": []
            },
            "bbox": [
                119,
                84,
                880,
                223
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "To systematically evaluate the applicability of CLiMRS in challenging scenarios where heterogeneous robots must cope with unpredictable execution errors, we introduce CLiMBench, a benchmark for heterogeneous multi-robot collaboration. CLiMBench is built around long-horizon assembly tasks that require robots to jointly perform object search, navigation, transportation, and assembly under partial observability. It features five robotic devices across three types of heterogeneous robots. Tasks of varying difficulty simulate material-handling and assembly processes with diverse skill usage, designed to test the planning and perception capabilities of LLM-based frameworks. "
                    }
                ]
            },
            "bbox": [
                112,
                303,
                489,
                527
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "We evaluated our framework in CLiMBench and another heterogeneous robot collaboration benchmark (Liu et al., 2025). Our experiments show that CLiMRS outperforms the best baseline, increasing success rates and improving efficiency on complex tasks while maintaining high success on simpler ones. These results demonstrate that incorporating human-inspired dynamic subgroup formation and negotiation principles substantially enhances the efficiency of heterogeneous multi-robot collaboration. To summarize, our main contributions are: "
                    }
                ]
            },
            "bbox": [
                112,
                527,
                489,
                705
            ]
        },
        {
            "type": "list",
            "content": {
                "list_type": "text_list",
                "list_items": [
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "• We present CLiMRS, a multi-LLM cooperation framework for heterogeneous multi-robot collaboration that can perform long-horizon planning and efficient perception in complex tasks. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "• We propose CLiMBench, a benchmark evaluating heterogeneous multi-robot collaboration with long-horizon assembly tasks, featuring varied skills and a realistic simulation environment. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "• We demonstrate through extensive experiments that CLiMRS achieves significant efficiency improvements via dynamic group formation and cooperative long-horizon planning. "
                            }
                        ]
                    }
                ]
            },
            "bbox": [
                119,
                715,
                489,
                921
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "2 Related Works "
                    }
                ],
                "level": 1
            },
            "bbox": [
                509,
                302,
                672,
                317
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "2.1 Robotic Skills Training Across Scenarios "
                    }
                ],
                "level": 1
            },
            "bbox": [
                507,
                331,
                873,
                347
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Single Agent Skill Training. Current approaches to train embodied skills for task execution generally follow two primary paradigms: rule-based and learning-driven methods. Traditional embodiment controllers optimize joint movements through the resolution of robotic kinematics, aiming to improve motion robustness and generate smoother, more precise trajectories (Kashyap and Parhi, 2021; Katayama et al., 2023). In recent years, advances in reinforcement learning and imitation learning have significantly improved robotic motion control across domains such as dexterous manipulation, bipedal locomotion, and quadrupedal navigation (Rajeswaran et al., 2017; Li et al., 2025; Bellegarda et al., 2024). These advances have progressively enabled embodied systems to coordinate actions in a cerebellum-like manner, supporting increasingly complex tasks in diverse environments. "
                    }
                ]
            },
            "bbox": [
                505,
                354,
                884,
                643
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Multi-agent Skill Training. Originally developed in game AI (Kurach et al., 2020; Perolat et al., 2022), multi-agent skill training has since been extended to more physically grounded domains such as robotics (Wang et al., 2024; Lai et al., 2025) and autonomous driving (Li et al., 2022). Despite these advances, multi-agent embodied learning remains limited by exponential state-space growth, and existing approaches such as mean-field methods (Yang et al., 2018) struggle to generalize across heterogeneous robots in collaboration. "
                    }
                ]
            },
            "bbox": [
                507,
                646,
                884,
                822
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "To further this goal, we design a set of diverse robotic skills in CLiMBench that model robots’ low-level execution outcomes, capturing both successful executions and failure cases, thereby providing a testbed for planning methods in heterogeneous multi-agent collaboration. "
                    }
                ]
            },
            "bbox": [
                507,
                825,
                884,
                921
            ]
        },
        {
            "type": "page_number",
            "content": {
                "page_number_content": [
                    {
                        "type": "text",
                        "content": "2 "
                    }
                ]
            },
            "bbox": [
                492,
                942,
                505,
                954
            ]
        }
    ],
    [
        {
            "type": "image",
            "content": {
                "image_source": {
                    "path": "images/891f15d54b46ec0a2c966c0fe9e3682f227c2bc8d4e28c8abd89957105b631f0.jpg"
                },
                "image_caption": [
                    {
                        "type": "text",
                        "content": "Figure 2: CLiMRS Framework. To employ our grouping–planning–execution–feedback cycle, CLiMRS comprises (a) a general proposal planner that forms dynamic agent subgroups, (b) multiple subgroup managers for local agent commands, (c) multiple agent executors for robot skills and agent feedback, (d) a simulation environment for environment interaction and feedback, and (e) a context memory module for all dialogue context and feedback. "
                    }
                ],
                "image_footnote": []
            },
            "bbox": [
                119,
                85,
                880,
                350
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "2.2 Task Planning with LLMs in Robotics "
                    }
                ],
                "level": 1
            },
            "bbox": [
                112,
                445,
                458,
                462
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "LLM Planner for Robotics. The rapid progress of LLMs has motivated their use as task planners for robots, as few-shot and zero-shot learning enable the decomposition of high-level goals into executable task sequences (Brown et al., 2020; Huang et al., 2022; Kojima et al., 2022). Their codegeneration abilities further allow LLMs to produce executable skill-level commands for robotic control (Liang et al., 2023a; Singh et al., 2023; Wang et al., 2023; Wu et al., 2023), while value-functionbased methods guide robust skill selection (Lin et al., 2023; Ahn et al., 2022). Recent advances in prompting strategies further improve long-horizon LLM-based planning (Zhang et al., 2024b; Liu et al., 2025). Some studies also explore reasoning in joint or 3D action spaces (Mandi et al., 2023). "
                    }
                ]
            },
            "bbox": [
                112,
                468,
                489,
                726
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Multi-LLM Task Planning. Prior work emphasizes multi-LLM cooperation through discussion, debate, and role assignment (Chen et al., 2023a; Liang et al., 2023b; Hong et al., 2024) to provide complementary perspectives and improve output reliability, often augmented with feedback and memory mechanisms to improve long-horizon planning (Mandi et al., 2023; Liu et al., 2025; Zhang et al., 2024b; Wang et al., 2023). "
                    }
                ]
            },
            "bbox": [
                112,
                727,
                489,
                872
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Decision-making Paradigms in Multi-Robot Collaboration. Two primary paradigms emerged for complex multi-robot tasks: centralized and de-"
                    }
                ]
            },
            "bbox": [
                112,
                873,
                489,
                921
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "centralized approaches. In decentralized schemes, multiple models or agents communicate, exchange intermediate plans, and iteratively refine their decisions through structured dialogue (Mandi et al., 2023; Zhang et al., 2024b; Liu et al., 2024b),while centralized methods typically rely on a single LLM to decompose global objectives and allocate tasks when planning (Kannan et al., 2023; Liu et al., 2025). A recent comparative study conducted in four diverse multi-agent scenarios (Chen et al., 2023b) further reports that centralized communication consistently achieves higher success rates and markedly greater token efficiency with proper feedback mechanisms, highlighting its strong potential for scalable real-world deployment. "
                    }
                ]
            },
            "bbox": [
                507,
                445,
                884,
                687
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Based on previous findings, CLiMRS structures multi-LLM cooperation through a grouping–planning–execution–feedback loop. By dynamically forming subgroups for subtasks, the framework enables parallel planning, agent execution, and robust collaboration. "
                    }
                ]
            },
            "bbox": [
                507,
                688,
                884,
                783
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "3 Method "
                    }
                ],
                "level": 1
            },
            "bbox": [
                509,
                797,
                611,
                813
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "In this section, we present CLiMRS, an adaptive group negotiation framework among multiple LLMs that enables multi-robot collaboration. In human team problem-solving, a common and effective strategy for tackling complex tasks is to decompose the problem and assign subtasks to subgroups "
                    }
                ]
            },
            "bbox": [
                507,
                825,
                884,
                921
            ]
        },
        {
            "type": "page_number",
            "content": {
                "page_number_content": [
                    {
                        "type": "text",
                        "content": "3 "
                    }
                ]
            },
            "bbox": [
                492,
                942,
                504,
                954
            ]
        }
    ],
    [
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "that work in parallel. Inspired by this, our approach forms dynamic agent subgroups that hold centralized discussions on robot perception in parallel, with each robot paired with an independent LLM agent to give feedback to discussions, thus forming a dynamic grouping–planning–execution–feedback cycle, which is shown in Fig. 1. "
                    }
                ]
            },
            "bbox": [
                112,
                84,
                487,
                197
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "As illustrated in Fig. 2, CLiMRS comprises five core modules: (a) a general proposal planner that forms dynamic agent groups, (b) multiple subgroup managers that generate local agent commands, (c) multiple agent executors that produce robot skills and return agent-level execution feedback, (d) a simulation environment for real-time interaction and provides environment feedback, and (e) a context memory module that records all inter-agent dialogues and feedback. "
                    }
                ]
            },
            "bbox": [
                115,
                198,
                489,
                357
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "3.1 Grouping with General Proposal Planner "
                    }
                ],
                "level": 1
            },
            "bbox": [
                112,
                369,
                485,
                386
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "The first stage of our cycle dynamically partitions agents into subgroups responsible for different aspects of the overall task, using a general proposal planner to orchestrate the grouping process. "
                    }
                ]
            },
            "bbox": [
                112,
                390,
                487,
                454
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "General Proposal Planner. As illustrated in Fig. 2(a), the general proposal planner generates a global task proposal that clusters agents into subtask-oriented teams. Given the overall task instruction, the prompted LLM incorporates robot capabilities, current observations, and dialogue history through a structured prompt. It outputs a plan with the following components: (1) Situation Analysis, assessing the environment and task progress; (2) Spatial Analysis, accounting for agent and object locations and spatial constraints; (3) Task Decomposition, breaking the objective into executable subtasks; (4) Grouping Strategy, clustering agents for parallel execution while minimizing interference; (5) Subgoal Assignment, specifying each group’s objective; (6) Coordination Strategy, outlining inter-group synchronization and execution order; and (7) Risk Assessment, identifying potential conflicts and mitigation plans. The resulting group-to-subtask assignments are then passed to the perception and execution modules. "
                    }
                ]
            },
            "bbox": [
                115,
                455,
                489,
                791
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "3.2 Planning with Local Subgroup Managers "
                    }
                ],
                "level": 1
            },
            "bbox": [
                112,
                804,
                485,
                820
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Given the agent groupings and their assigned subtasks, the second stage generates precise robotlevel commands based on robot capabilities and current observations. As the subtasks are independent, multiple subgroup managers operate in parallel, as illustrated in Fig. 2(b). "
                    }
                ]
            },
            "bbox": [
                112,
                825,
                489,
                921
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Subgroup Manager. Each subgroup manager conducts a centralized discussion within its subgroup to determine fine-grained commands for individual robots. Conditioned on subtask instructions, subgroup robot capabilities, subgroup observations, and dialogue history, the subgroup manager selects appropriate skills and parameters for each agent. "
                    }
                ]
            },
            "bbox": [
                507,
                84,
                884,
                198
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "3.3 Execution and Environment Interaction "
                    }
                ],
                "level": 1
            },
            "bbox": [
                507,
                208,
                870,
                223
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "With commands issued to robots, the final two stages of our cycle require them to evaluate these commands, determine appropriate actions, execute safely and provide feedback to refine future planning. The agent executor LLM verifies the feasibility of its command and issues the corresponding action only when the command is deemed executable. "
                    }
                ]
            },
            "bbox": [
                507,
                229,
                884,
                341
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Agent Execution. Shown in Fig. 2(c), the agent executors verify and execute commands from the subgroup manager while providing feedback. Each agent executor LLM considers its robot’s capabilities, current observations, and available actions. The executor first checks feasibility against the robot’s physical constraints and conditions. If feasible, the action is executed using the robot’s skills; otherwise, the robot remains idle for this cycle. "
                    }
                ]
            },
            "bbox": [
                507,
                342,
                884,
                486
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Simulation Environment. Shown in Fig. 2(d), the simulation environment serves as the execution backbone of our framework. It receives the robot skill execution signals issued by the agent executors and immediately carries out the corresponding low-level actions in real time. During execution, it monitors the evolving state of the environment and produces both updated robot observations and environment-level feedback. These outputs are fed back to the context memory shown in Fig. 2(e), allowing the overall method to track task progress, refine its environment understanding, and supply the information required for the next round of the grouping–planning–execution–feedback cycle. "
                    }
                ]
            },
            "bbox": [
                507,
                487,
                884,
                712
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "3.4 Context Memory "
                    }
                ],
                "level": 1
            },
            "bbox": [
                507,
                722,
                690,
                739
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "With the grouping–planning–execution–feedback cycle described above, our framework employs a context memory module that contains execution feedback, environment observations, and dialogue history, providing context to all LLM agents. "
                    }
                ]
            },
            "bbox": [
                507,
                744,
                882,
                824
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Feedback Formation. Execution feedback is collected from two sources: (1) environment feedback, including updated environmental observations after robot actions are executed in the simulator, and (2) agent feedback generated by the agent executors. Agent executors report execution "
                    }
                ]
            },
            "bbox": [
                507,
                825,
                884,
                921
            ]
        },
        {
            "type": "page_number",
            "content": {
                "page_number_content": [
                    {
                        "type": "text",
                        "content": "4 "
                    }
                ]
            },
            "bbox": [
                492,
                942,
                505,
                954
            ]
        }
    ],
    [
        {
            "type": "image",
            "content": {
                "image_source": {
                    "path": "images/b15dfb4368bfdc2aff25abb9a01d888c1f52c1c49aae67ac96b159649d7f3842.jpg"
                },
                "image_caption": [
                    {
                        "type": "text",
                        "content": "Figure 3: Our Benchmark. CLiMBench is a heterogeneous multi-robot collaboration benchmark, featuring multi-agent robots with diverse skills, enabling collaboration on complex assembly tasks of varying difficulty levels. "
                    }
                ],
                "image_footnote": []
            },
            "bbox": [
                137,
                80,
                858,
                254
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "outcomes and provide diagnostic explanations for infeasible commands, enabling subsequent planners to reason about failures and task progress. "
                    }
                ]
            },
            "bbox": [
                112,
                318,
                487,
                366
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Context Memory Organization. As illustrated in Fig. 2(d), the context memory module stores: (1) agent feedback, including execution outcomes and diagnostic explanations from the agent executors; (2) environment feedback, consisting of updated observations from the simulation environment after action execution; and (3) dialogue context, including current planning dialogue and dialogue history accumulated across previous cycles. "
                    }
                ]
            },
            "bbox": [
                112,
                367,
                487,
                511
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "The context memory provides tailored context inputs for different modules. For the general proposal planner, it retains the most recent five dialogue turns along with the latest environment observations, enabling agent feedback to inform new task proposals and subgroup formations. For the subgroup managers, it stores each subgroup’s latest observations and the last five dialogue turns, supplying rich, localized context to guide fine-grained planning and perception within each subgroup. "
                    }
                ]
            },
            "bbox": [
                112,
                512,
                489,
                674
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "4 Benchmark "
                    }
                ],
                "level": 1
            },
            "bbox": [
                112,
                686,
                250,
                700
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "In this section, we introduce CLiMBench, a benchmark for heterogeneous multi-robot collaboration. We consider an assembly-oriented setting in which multiple components must be collected and assembled into a wheeled robot within a large workspace. Unlike prior multi-agent collaboration benchmarks (Liu et al., 2025), which decouple robot skill execution from the planning loop and assume successful execution by default, CLiMBench executes every robot skill within a realistic physics simulator, IsaacGym (Makoviychuk et al., 2021). This design explicitly allows execution failures and enables systematic evaluation of planning modules "
                    }
                ]
            },
            "bbox": [
                112,
                712,
                489,
                921
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "under realistic execution uncertainty. The goal of CLiMBench is to evaluate long-horizon heterogeneous multi-robot collaboration in settings where planning and execution are tightly coupled. "
                    }
                ]
            },
            "bbox": [
                507,
                318,
                884,
                382
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "4.1 Task Setting and Scene Construction "
                    }
                ],
                "level": 1
            },
            "bbox": [
                507,
                397,
                843,
                413
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Task Setting. The goal of the task is to assemble the components into a wheeled robot. Within the workspace, the components are distributed across different rooms, some of which are occluded or blocked by obstacles that must be cleared before access is possible. As a result, robots must search for components, navigate complex environments, and perform coordinated manipulation under partial observability and execution uncertainty. This long-horizon task is considered successfully complete once all required components are correctly assembled into the target product within a step count limit to evaluate task execution efficiency. "
                    }
                ]
            },
            "bbox": [
                505,
                420,
                882,
                629
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Scene Construction. We instantiate this setting in the IsaacGym simulator by constructing the scene with multiple robotic agents and modular components, as is shown in Fig. 3. The environment includes three types of heterogeneous robots: one robotic arm, three autonomous ground vehicles (AGV), and one humanoid robot, each providing complementary capabilities for manipulation, transportation, and interaction with the environment to complete the assembly task. To decouple high-level planning from low-level control while keeping heterogeneous robot capabilities, robot actions are executed through predefined skills, allowing highlevel planners to reason over realistic execution outcomes while abstracting low-level control. "
                    }
                ]
            },
            "bbox": [
                507,
                631,
                882,
                871
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Scene Initialization and Randomization. We initialize the environment and introduce controlled variations in task parameters and object configu-"
                    }
                ]
            },
            "bbox": [
                507,
                873,
                882,
                921
            ]
        },
        {
            "type": "page_number",
            "content": {
                "page_number_content": [
                    {
                        "type": "text",
                        "content": "5 "
                    }
                ]
            },
            "bbox": [
                492,
                942,
                502,
                953
            ]
        }
    ],
    [
        {
            "type": "table",
            "content": {
                "image_source": {
                    "path": "images/19082aac1c21f7e3c2fb69f70029bb34149a9fd0619467fe0b772b1a188a8e0f.jpg"
                },
                "table_caption": [
                    {
                        "type": "text",
                        "content": "Table 1: Robot Skills in CLiMBench. We assign a distinct set of skills for each robot based on its specific capabilities in the assembly tasks in CLiMBench. "
                    }
                ],
                "table_footnote": [],
                "html": "<table><tr><td>Robot type</td><td>Skill list</td></tr><tr><td>Robotic Arm</td><td>[check] within reach[pick] on [wait] for next command</td></tr><tr><td>AGV</td><td>[move] to [push] to [wait] for next command</td></tr><tr><td>Humanoid</td><td>[walk] to [carry] with both hands [wait] for next command</td></tr></table>",
                "table_type": "simple_table",
                "table_nest_level": 1
            },
            "bbox": [
                119,
                80,
                485,
                258
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "rations to enhance generalization. At the start of each episode, robots execute their skills under randomized task conditions, leading to diverse skill sequences and varying levels of agent-wise interaction. This provides a robust testbed for evaluating the effectiveness of different frameworks in multiagent collaborative tasks. "
                    }
                ]
            },
            "bbox": [
                112,
                336,
                489,
                447
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Environment feedback. We design the environment feedback along two dimensions: (1) We update the state of all agents and the coordinates of objects within their perceptible range, and (2) We report conflicts that arise when multiple robots execute skills simultaneously. "
                    }
                ]
            },
            "bbox": [
                112,
                450,
                489,
                546
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "4.2 Robot Skill Design in CLiMBench "
                    }
                ],
                "level": 1
            },
            "bbox": [
                112,
                558,
                428,
                574
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "In CLiMBench, each robot receives both the global task objectives and its observations (e.g., a humanoid robot observes its joint states, torso status, and target positions). As is shown in Table 1, we design distinct skills for different types of robots. Further details are provided in the Appendix. "
                    }
                ]
            },
            "bbox": [
                112,
                581,
                489,
                678
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Robotic Arm Manipulation. We use a Franka Panda arm to instantiate robotic manipulation skills in CLiMBench. To balance speed and precision, we adopt a two-stage strategy consisting of a fast coarse motion followed by fine adjustment phase. The arm is controlled using an operational-space controller (OSC) with gravity compensation, producing compliant spring–damper behavior in task space (Narang et al., 2022). Smooth operationalspace waypoints are generated via interpolation to ensure reliable execution. "
                    }
                ]
            },
            "bbox": [
                112,
                678,
                489,
                854
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "AGV Transportation. We use a TRACER Mini robot to instantiate AGV transportation skills in CLiMBench. Navigation paths are planned using a Rapidly-exploring Random Tree (RRT) planner "
                    }
                ]
            },
            "bbox": [
                112,
                857,
                489,
                921
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "that accounts for environmental obstacles. The resulting paths are executed via differential-drive control, enabling smooth turns during navigation. For final delivery, the AGV follows straight-line motion to ensure reliable transportation and accurate placement at the assembly location. "
                    }
                ]
            },
            "bbox": [
                507,
                84,
                884,
                180
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Humanoid Carrying. We instantiate humanoid carrying as a goal-conditioned control skill trained with Adversarial Motion Priors (AMP) (Peng et al., 2021) following the single-object manipulation paradigm in COOHOI (Gao et al., 2024). Given a target object and a goal location, the policy produces whole-body locomotion and manipulation to (1) approach and grasp the object, (2) carry it while maintaining balance and collision-robust gait, and (3) place it at the goal. Training combines task rewards for reaching, holding, and placing, and AMP style rewards to encourage natural, stable motions and rapid recovery, yielding reliable execution under contact and minor disturbances. "
                    }
                ]
            },
            "bbox": [
                507,
                181,
                884,
                405
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "5 Experiments "
                    }
                ],
                "level": 1
            },
            "bbox": [
                509,
                417,
                655,
                434
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "In this section, we present a comprehensive evaluation of CLiMRS to address the following questions: "
                    }
                ]
            },
            "bbox": [
                507,
                443,
                882,
                475
            ]
        },
        {
            "type": "list",
            "content": {
                "list_type": "text_list",
                "list_items": [
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "(1) Is CLiMRS effective for simple daily-life multi-robot collaboration tasks? "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "(2) Can CLiMRS perform well in challenging multi-robot assembly tasks? "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "(3) Through ablation studies, how critical are the individual components of CLiMRS? "
                            }
                        ]
                    }
                ]
            },
            "bbox": [
                515,
                483,
                880,
                590
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "We evaluate CLiMRS in two distinct environments: CLiMBench and a simpler heterogeneous multi-robot collaboration benchmark from CO-HERENT (Liu et al., 2025). For LLM use, we use gpt-4-0125-preview to align with the setting in COHERENT. For quantitative analysis, we use task Success Rate (SR) and Average Step (AS) as evaluation metrics in this paper. "
                    }
                ]
            },
            "bbox": [
                507,
                600,
                882,
                728
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "5.1 Evaluating CLiMRS on Simple Daily-Life Multi-Robot Collaboration "
                    }
                ],
                "level": 1
            },
            "bbox": [
                507,
                739,
                880,
                770
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "To answer Question (1), we evaluate CLiMRS on the COHERENT benchmark, a simpler heterogeneous multi-robot benchmark that includes diverse tasks across five real-world scenes, but involves at most three heterogeneous robots and assumes perfect skill execution. We adopt its evaluation metrics and use the reported results as our baseline. "
                    }
                ]
            },
            "bbox": [
                507,
                776,
                882,
                888
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Results shown in Table 2 and 3 suggest that CLiMRS succeeds on nearly all COHERENT tasks "
                    }
                ]
            },
            "bbox": [
                507,
                889,
                880,
                920
            ]
        },
        {
            "type": "page_number",
            "content": {
                "page_number_content": [
                    {
                        "type": "text",
                        "content": "6 "
                    }
                ]
            },
            "bbox": [
                492,
                942,
                504,
                953
            ]
        }
    ],
    [
        {
            "type": "table",
            "content": {
                "image_source": {
                    "path": "images/b97a124326e75a951a0e54ff289a66a614112db9d184c0802425260238297008.jpg"
                },
                "table_caption": [
                    {
                        "type": "text",
                        "content": "Table 2: Comparison Across Task Types in the COHERENT Benchmark. CLiMRS outperforms all the baselines, achieving the largest gain on the most challenging trio-type tasks. "
                    }
                ],
                "table_footnote": [],
                "html": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">Mono-type Task</td><td colspan=\"2\">Dual-type Task</td><td colspan=\"2\">Trio-type Task</td><td colspan=\"2\">Average</td></tr><tr><td>SR</td><td>AS</td><td>SR</td><td>AS</td><td>SR</td><td>AS</td><td>SR</td><td>AS</td></tr><tr><td>DMRS-1D</td><td>0.700</td><td>10.6</td><td>0.467</td><td>18.0</td><td>0.667</td><td>20.7</td><td>0.600</td><td>17.2</td></tr><tr><td>DMRS-2D</td><td>0.500</td><td>11.5</td><td>0.267</td><td>19.9</td><td>0.400</td><td>24.5</td><td>0.375</td><td>19.6</td></tr><tr><td>CMRS</td><td>0.900</td><td>7.9</td><td>0.533</td><td>16.4</td><td>0.533</td><td>22.2</td><td>0.625</td><td>16.5</td></tr><tr><td>Primitive MCTS</td><td>0.000</td><td>14.0</td><td>0.000</td><td>21.5</td><td>0.000</td><td>26.9</td><td>0.000</td><td>21.7</td></tr><tr><td>LLM-MCTS</td><td>0.700</td><td>10.2</td><td>0.067</td><td>20.9</td><td>0.000</td><td>26.9</td><td>0.200</td><td>20.5</td></tr><tr><td>COHERENT</td><td>0.900</td><td>7.4</td><td>1.000</td><td>11.9</td><td>1.000</td><td>16.1</td><td>0.975</td><td>12.4</td></tr><tr><td>CLiMRS(Ours)</td><td>0.900</td><td>6.8</td><td>1.000</td><td>11.5</td><td>1.000</td><td>13.1</td><td>0.975</td><td>10.9</td></tr><tr><td>Ground Truth (GT)</td><td>-</td><td>6.5</td><td>-</td><td>10.3</td><td>-</td><td>12.9</td><td>-</td><td>10.3</td></tr></table>",
                "table_type": "complex_table",
                "table_nest_level": 1
            },
            "bbox": [
                233,
                80,
                763,
                231
            ]
        },
        {
            "type": "table",
            "content": {
                "image_source": {
                    "path": "images/d5cacc34bd38b18749d3291c707fe0fdfd2e9f7b9ea7196a72f863ee7ef23513.jpg"
                },
                "table_caption": [
                    {
                        "type": "text",
                        "content": "Table 3: Comparison Across Scenes in the COHERENT Benchmark. CLiMRS outperforms all the baselines in every scene, demonstrating its superior performance. "
                    }
                ],
                "table_footnote": [],
                "html": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">S1</td><td colspan=\"2\">S2</td><td colspan=\"2\">S3</td><td colspan=\"2\">S4</td><td colspan=\"2\">S5</td><td colspan=\"2\">Average</td></tr><tr><td>SR</td><td>AS</td><td>SR</td><td>AS</td><td>SR</td><td>AS</td><td>SR</td><td>AS</td><td>SR</td><td>AS</td><td>SR</td><td>AS</td></tr><tr><td>DMRS-1D</td><td>0.500</td><td>17.4</td><td>0.625</td><td>15.8</td><td>0.625</td><td>18.3</td><td>0.750</td><td>15.1</td><td>0.500</td><td>19.3</td><td>0.600</td><td>17.2</td></tr><tr><td>DMRS-2D</td><td>0.500</td><td>18.9</td><td>0.500</td><td>18.3</td><td>0.375</td><td>20.6</td><td>0.250</td><td>18.9</td><td>0.250</td><td>21.1</td><td>0.375</td><td>19.6</td></tr><tr><td>CMRS</td><td>0.875</td><td>13.1</td><td>0.625</td><td>16.6</td><td>0.625</td><td>18.5</td><td>0.375</td><td>18.1</td><td>0.625</td><td>15.9</td><td>0.625</td><td>16.5</td></tr><tr><td>Primitive MCTS</td><td>0.000</td><td>21.5</td><td>0.000</td><td>21.8</td><td>0.000</td><td>22.5</td><td>0.000</td><td>20.5</td><td>0.000</td><td>22.0</td><td>0.000</td><td>21.7</td></tr><tr><td>LLM-MCTS</td><td>0.250</td><td>20.0</td><td>0.250</td><td>20.4</td><td>0.250</td><td>21.3</td><td>0.125</td><td>19.9</td><td>0.125</td><td>20.9</td><td>0.200</td><td>20.5</td></tr><tr><td>COHERENT</td><td>1.000</td><td>13.1</td><td>1.000</td><td>11.4</td><td>1.000</td><td>11.9</td><td>1.000</td><td>11.4</td><td>0.875</td><td>14.0</td><td>0.975</td><td>12.4</td></tr><tr><td>CLiMRS(Ours)</td><td>1.000</td><td>10.8</td><td>1.000</td><td>10.4</td><td>1.000</td><td>11.8</td><td>1.000</td><td>10.4</td><td>0.875</td><td>11.4</td><td>0.975</td><td>10.9</td></tr><tr><td>Ground Truth (GT)</td><td>-</td><td>10.3</td><td>-</td><td>10.4</td><td>-</td><td>10.8</td><td>-</td><td>9.8</td><td>-</td><td>10.5</td><td>-</td><td>10.3</td></tr></table>",
                "table_type": "complex_table",
                "table_nest_level": 1
            },
            "bbox": [
                157,
                297,
                840,
                451
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "and achieves higher efficiency with fewer steps. This trend holds across every scene, demonstrating our CLiMRS’ superior performance. Notably, in the most challenging trio-type tasks, which require all three robots to collaborate, CLiMRS delivers the largest gain, reducing the Average Step count by "
                    },
                    {
                        "type": "equation_inline",
                        "content": "1 8 . 6 \\%"
                    },
                    {
                        "type": "text",
                        "content": ", indicating that our approach offers stronger improvements on more complex tasks. "
                    }
                ]
            },
            "bbox": [
                112,
                531,
                487,
                659
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "5.2 Evaluating CLiMRS on CLiMBench with Robot Assembly Tasks "
                    }
                ],
                "level": 1
            },
            "bbox": [
                112,
                670,
                485,
                701
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "To answer Question (2), we evaluate CLiMRS on CLiMBench. Our baselines include the following: "
                    }
                ]
            },
            "bbox": [
                112,
                707,
                487,
                739
            ]
        },
        {
            "type": "list",
            "content": {
                "list_type": "text_list",
                "list_items": [
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "• DMRS-1D: a variant of CoELA (Zhang et al., 2024b), this decentralized framework lets robots determine their next step through dialogue, with the final decision summarized by the last robot. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "• CMRS: a primitive centralized system (Huang et al., 2022) that uses a single decision-making LLM to output executable actions, where all information is stored in the prompt. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "• COHERENT: an approximately centralized baseline that combines an oracle planning LLM "
                            }
                        ]
                    }
                ]
            },
            "bbox": [
                119,
                749,
                487,
                921
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "with a feedback LLM, where dialogue history is used to adjust future perception and planning. "
                    }
                ]
            },
            "bbox": [
                527,
                531,
                880,
                564
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "For quantitative evaluation, we fix the scene parameters and select four representative scenarios to evaluate all methods. For each scenario, we manually derive minimal-step solutions as ground-truth references. We observe that scenarios with heavier object occlusion require more ground-truth steps, naturally forming two levels of difficulty. "
                    }
                ]
            },
            "bbox": [
                507,
                579,
                882,
                690
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Due to stochastic skill execution in CLiMBench, each task is executed five times, and we report the mean Success Rate (SR) and Average Steps (AS). To control evaluation cost and runtime, a task is considered successful only if it is completed within twice the corresponding ground-truth step count. "
                    }
                ]
            },
            "bbox": [
                507,
                693,
                882,
                789
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Results in Table 4 show that CLiMRS achieves a "
                    },
                    {
                        "type": "equation_inline",
                        "content": "100 \\%"
                    },
                    {
                        "type": "text",
                        "content": "success rate on CLiMBench, outperforming all baselines. In addition, CLiMRS reduces the Average Steps (AS) by "
                    },
                    {
                        "type": "equation_inline",
                        "content": "4 4 . 3 0 \\%"
                    },
                    {
                        "type": "text",
                        "content": "compared to the best baseline, indicating substantially higher planning efficiency in long-horizon collaboration. "
                    }
                ]
            },
            "bbox": [
                507,
                791,
                882,
                887
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Notably, most baselines terminate due to timeout and fail to complete tasks within the step "
                    }
                ]
            },
            "bbox": [
                507,
                889,
                882,
                921
            ]
        },
        {
            "type": "page_number",
            "content": {
                "page_number_content": [
                    {
                        "type": "text",
                        "content": "7 "
                    }
                ]
            },
            "bbox": [
                492,
                942,
                504,
                953
            ]
        }
    ],
    [
        {
            "type": "table",
            "content": {
                "image_source": {
                    "path": "images/ac7df3cd67bb7cb3f25c16e649275576055b66823d9171b9bb72fc8ace438cee.jpg"
                },
                "table_caption": [
                    {
                        "type": "text",
                        "content": "Table 4: Comparison Across Tasks in CLiMBench. CLiMRS outperforms all our baselines and reduces the Average Step (AS) by over "
                    },
                    {
                        "type": "equation_inline",
                        "content": "40 \\%"
                    },
                    {
                        "type": "text",
                        "content": ". "
                    }
                ],
                "table_footnote": [],
                "html": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">Task 1 (Easy)</td><td colspan=\"2\">Task 2 (Easy)</td><td colspan=\"2\">Task 3 (Hard)</td><td colspan=\"2\">Task 4 (Hard)</td><td colspan=\"2\">Average</td></tr><tr><td>SR</td><td>AS</td><td>SR</td><td>AS</td><td>SR</td><td>AS</td><td>SR</td><td>AS</td><td>SR</td><td>AS</td></tr><tr><td>DMRS-1D</td><td>0.000</td><td>15.0</td><td>0.000</td><td>15.0</td><td>0.000</td><td>19.0</td><td>0.000</td><td>19.0</td><td>0.000</td><td>17.0</td></tr><tr><td>CMRS</td><td>0.000</td><td>15.0</td><td>0.000</td><td>15.0</td><td>0.000</td><td>19.0</td><td>0.000</td><td>19.0</td><td>0.000</td><td>17.0</td></tr><tr><td>COHERENT</td><td>1.000</td><td>13.6</td><td>0.800</td><td>13.6</td><td>0.400</td><td>18.2</td><td>0.600</td><td>17.8</td><td>0.700</td><td>15.8</td></tr><tr><td>CLiMRS (Ours)</td><td>1.000</td><td>8.2</td><td>1.000</td><td>8.4</td><td>1.000</td><td>9.4</td><td>1.000</td><td>9.2</td><td>1.000</td><td>8.8</td></tr><tr><td>Ground Truth (GT)</td><td>-</td><td>7.0</td><td>-</td><td>7.0</td><td>-</td><td>9.0</td><td>-</td><td>9.0</td><td>-</td><td>8.0</td></tr></table>",
                "table_type": "complex_table",
                "table_nest_level": 1
            },
            "bbox": [
                176,
                80,
                820,
                202
            ]
        },
        {
            "type": "table",
            "content": {
                "image_source": {
                    "path": "images/75b44aa290014f3fa533b4fba35c8df2600bd2ff51771cf07b100493dc0fbbe7.jpg"
                },
                "table_caption": [
                    {
                        "type": "text",
                        "content": "Table 5: Ablation Studies. Removing dialogue history, feedback information, or the grouping stage significantly reduces both Success Rate (SR) and Average Step (AS). "
                    }
                ],
                "table_footnote": [],
                "html": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">Task 1 (Easy)</td><td colspan=\"2\">Task 2 (Easy)</td><td colspan=\"2\">Task 3 (Hard)</td><td colspan=\"2\">Task 4 (Hard)</td><td colspan=\"2\">Average</td></tr><tr><td>SR</td><td>AS</td><td>SR</td><td>AS</td><td>SR</td><td>AS</td><td>SR</td><td>AS</td><td>SR</td><td>AS</td></tr><tr><td>CLiMRS w/o history</td><td>0.000</td><td>15.0</td><td>0.000</td><td>15.0</td><td>0.000</td><td>19.0</td><td>0.000</td><td>19.0</td><td>0.000</td><td>17.0</td></tr><tr><td>CLiMRS w/o feedback</td><td>0.200</td><td>14.8</td><td>0.200</td><td>14.8</td><td>0.200</td><td>18.8</td><td>0.200</td><td>18.8</td><td>0.200</td><td>16.8</td></tr><tr><td>CLiMRS w/o grouping</td><td>0.600</td><td>14.0</td><td>0.800</td><td>13.2</td><td>0.600</td><td>17.2</td><td>0.600</td><td>17.4</td><td>0.650</td><td>15.5</td></tr><tr><td>CLiMRS (Ours)</td><td>1.000</td><td>8.2</td><td>1.000</td><td>8.4</td><td>1.000</td><td>9.4</td><td>1.000</td><td>9.2</td><td>1.000</td><td>8.8</td></tr><tr><td>Ground Truth (GT)</td><td>-</td><td>7.0</td><td>-</td><td>7.0</td><td>-</td><td>9.0</td><td>-</td><td>9.0</td><td>-</td><td>8.0</td></tr></table>",
                "table_type": "complex_table",
                "table_nest_level": 1
            },
            "bbox": [
                176,
                268,
                820,
                385
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "budget, and the performance degradation of baselines from the COHERENT benchmark (Table 2) to CLiMBench (Table 4) demonstrates that CLiMBench provides a significantly more challenging evaluation setting for robot collaboration. "
                    }
                ]
            },
            "bbox": [
                112,
                464,
                487,
                545
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Recover from Execution Failures. From our experiments, we observe three types of execution failures: (1) improper grouping, where no robot in the assigned group can complete the subtask; (2) incorrect agent selection, where a valid subtask is assigned to a robot without the required capabilities; and (3) state inconsistency, where missing information or unmet preconditions prevent proper execution. Nevertheless, the natural-language feedback provided by the executors captures these failure signals and, through the context memory, enables downstream perception and planning to be refined, ultimately leading to successful task completion. "
                    }
                ]
            },
            "bbox": [
                112,
                546,
                489,
                755
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "5.3 Ablation Studies on CLiMRS "
                    }
                ],
                "level": 1
            },
            "bbox": [
                112,
                769,
                391,
                784
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "To answer Question (3), we conduct an ablation study to assess the contribution of each component in CLiMRS by: (i) removing the dialogue history, (ii) removing the feedback information, and (iii) removing the grouping stage from the grouping–planning–execution–feedback cycle. We evaluate all variants on the same tasks and metrics as in Section 5.2, with results summarized in Table 5. "
                    }
                ]
            },
            "bbox": [
                112,
                791,
                489,
                921
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "The results show that both dialogue history and feedback information are critical to the framework: removing either component leads to failure in task completion, indicating that sustained context accumulation and execution-aware feedback are indispensable for long-horizon planning. The grouping stage also plays a central role in CLiMRS, significantly improving task success rates and reducing average steps by enabling effective coordination and parallelization across heterogeneous robots. "
                    }
                ]
            },
            "bbox": [
                507,
                464,
                884,
                625
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "6 Conclusion "
                    }
                ],
                "level": 1
            },
            "bbox": [
                507,
                638,
                640,
                653
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "In this paper, we present CLiMRS, a human-teaminspired adaptive group negotiation framework among multiple LLMs for heterogeneous multirobot collaboration, together with CLiMBench, a challenging benchmark for long-horizon assembly tasks under realistic execution uncertainty. Extensive experiments show that CLiMRS consistently outperforms all baselines in both success rate and efficiency, while robustly recovering from execution failures with feedback-driven negotiation. Ablation studies highlight the complementary roles of dialogue history, feedback, and dynamic subgroup formation. Overall, our results demonstrate that human-inspired group formation and negotiation principles offer an effective and scalable solution for heterogeneous multi-robot collaboration. "
                    }
                ]
            },
            "bbox": [
                507,
                663,
                884,
                921
            ]
        },
        {
            "type": "page_number",
            "content": {
                "page_number_content": [
                    {
                        "type": "text",
                        "content": "8 "
                    }
                ]
            },
            "bbox": [
                492,
                942,
                504,
                954
            ]
        }
    ],
    [
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "7 Limitations "
                    }
                ],
                "level": 1
            },
            "bbox": [
                114,
                84,
                250,
                98
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "In this paper, we primarily focus on improving the efficiency of multi-robot collaboration at the level of high-level behavioral planning. We assume that LLM inference latency is negligible compared to skill execution time, and therefore do not explicitly model network delays or computational costs in the current framework. Managing API costs under inference-efficiency constraints and exploring asynchronous inference–execution pipelines remain important directions for future work. In addition, while CLiMBench emphasizes long-horizon behavioral planning and coordination, it simplifies sensory inputs and perception modules in simulation. Bridging this gap toward more realistic perception and enabling sim-to-real transfer constitute promising avenues for future research. "
                    }
                ]
            },
            "bbox": [
                112,
                109,
                492,
                367
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "8 Ethical Considerations "
                    }
                ],
                "level": 1
            },
            "bbox": [
                112,
                376,
                346,
                392
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Our study investigates multi-robot collaboration using large language models (LLMs) for planning and negotiation in simulated environments. We discuss the primary ethical considerations relevant to this work below. "
                    }
                ]
            },
            "bbox": [
                112,
                401,
                489,
                481
            ]
        },
        {
            "type": "list",
            "content": {
                "list_type": "text_list",
                "list_items": [
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "• No Human or Sensitive Data. Our research is conducted entirely in simulation and does not involve human subjects, personally identifiable information, or sensitive real-world data. We do not collect or process user data, nor do we rely on proprietary datasets. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "• Safety and Responsible Deployment. While our methods and benchmark are evaluated exclusively in simulation, real-world deployment of autonomous multi-robot systems may pose physical safety risks. In particular, LLM-based planning may be susceptible to erroneous or inconsistent decisions under distribution shift. Any future deployment should therefore incorporate rigorous safety validation, fail-safe mechanisms, and human oversight, and comply with applicable safety standards and regulations. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "• Bias and Model Limitations. The LLMs used in our study are pretrained by third parties and may reflect societal biases present in their training data. Although our work does not directly deploy LLMs in human-facing decision-making, such biases could indirectly influence planning behavior. We acknowledge this limitation and recommend bias auditing and robustness testing before any real-world application. "
                            }
                        ]
                    }
                ]
            },
            "bbox": [
                119,
                493,
                489,
                921
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "References "
                    }
                ],
                "level": 1
            },
            "bbox": [
                510,
                84,
                608,
                98
            ]
        },
        {
            "type": "list",
            "content": {
                "list_type": "reference_list",
                "list_items": [
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, and 26 others. 2022. Do as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint arXiv:2204.01691. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Guillaume Bellegarda, Milad Shafiee, and Auke Ijspeert. 2024. Visual cpg-rl: Learning central pattern generators for visually-guided quadruped locomotion. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1420–1427. IEEE. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, and 12 others. 2020. Language models are few-shot learners. Preprint, arXiv:2005.14165. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Justin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal. 2023a. Reconcile: Round-table conference improves reasoning via consensus among diverse llms. arXiv preprint arXiv:2309.13007. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Yongchao Chen, Jacob Arkin, Yang Zhang, Nicholas Roy, and Chuchu Fan. 2023b. Scalable multi-robot collaboration with large language models: Centralized or decentralized systems? arXiv preprint arXiv:2309.15943. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Jiawei Gao, Ziqin Wang, Zeqi Xiao, Jingbo Wang, Tai Wang, Jinkun Cao, Xiaolin Hu, Si Liu, Jifeng Dai, and Jiangmiao Pang. 2024. Coohoi: Learning cooperative human-object interaction with manipulated object dynamics. In Advances in Neural Information Processing Systems. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. 2024. MetaGPT: Meta programming for a multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974. "
                            }
                        ]
                    }
                ]
            },
            "bbox": [
                510,
                105,
                885,
                921
            ]
        },
        {
            "type": "page_number",
            "content": {
                "page_number_content": [
                    {
                        "type": "text",
                        "content": "9 "
                    }
                ]
            },
            "bbox": [
                492,
                942,
                504,
                953
            ]
        }
    ],
    [
        {
            "type": "list",
            "content": {
                "list_type": "reference_list",
                "list_items": [
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Shyam Sundar Kannan, Vishnunandan LN Venkatesh, and Byung-Cheol Min. 2023. Smart-llm: Smart multi-agent robot task planning using large language models. arXiv preprint arXiv:2309.10062. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Abhishek Kumar Kashyap and Dayal R Parhi. 2021. Particle swarm optimization aided pid gait controller design for a humanoid robot. ISA transactions, 114:306–330. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Sotaro Katayama, Masaki Murooka, and Yuichi Tazaki. 2023. Model predictive control of legged and humanoid robots: models and algorithms. Advanced Robotics, 37(5):298–315. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS ’22, Red Hook, NY, USA. Curran Associates Inc. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Karol Kurach, Anton Raichuk, Piotr Stanczyk, Michał ´ Zaj ˛ac, Olivier Bachem, Lasse Espeholt, Carlos Riquelme, Damien Vincent, Marcin Michalski, Olivier Bousquet, and 1 others. 2020. Google research football: A novel reinforcement learning environment. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 4501–4510. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Matthew Lai, Keegan Go, Zhibin Li, Torsten Kröger, Stefan Schaal, Kelsey Allen, and Jonathan Scholz. 2025. Roboballet: Planning for multirobot reaching with graph neural networks and reinforcement learning. Science Robotics, 10(106):eads1204. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Yiming Li, Dekun Ma, Ziyan An, Zixun Wang, Yiqi Zhong, Siheng Chen, and Chen Feng. 2022. V2xsim: Multi-agent collaborative perception dataset and benchmark for autonomous driving. IEEE Robotics and Automation Letters, 7(4):10914–10921. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Zhongyu Li, Xue Bin Peng, Pieter Abbeel, Sergey Levine, Glen Berseth, and Koushil Sreenath. 2025. Reinforcement learning for versatile, dynamic, and robust bipedal locomotion control. The International Journal of Robotics Research, 44(5):840–888. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. 2023a. Code as policies: Language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 9493–9500. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023b. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Wenlong Liang, Rui Zhou, Yang Ma, Bing Zhang, Songlin Li, Yijia Liao, and Ping Kuang. 2025. Large model empowered embodied ai: A survey on decision-making and embodied learning. arXiv preprint arXiv:2508.10399. "
                            }
                        ]
                    }
                ]
            },
            "bbox": [
                115,
                85,
                485,
                919
            ]
        },
        {
            "type": "list",
            "content": {
                "list_type": "reference_list",
                "list_items": [
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg. 2023. Text2motion: from natural language instructions to feasible plans. Autonomous Robots. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Kehui Liu, Zixin Tang, Dong Wang, Zhigang Wang, Xuelong Li, and Bin Zhao. 2025. Coherent: Collaboration of heterogeneous multi-robot system with large language models. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pages 10208–10214. IEEE. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Xinzhu Liu, Peiyan Li, Wenju Yang, Di Guo, and Huaping Liu. 2024a. Leveraging large language model for heterogeneous ad hoc teamwork collaboration. arXiv preprint arXiv:2406.12224. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Xinzhu Liu, Peiyan Li, Wenju Yang, Di Guo, and Huaping Liu. 2024b. Leveraging large language model for heterogeneous ad hoc teamwork collaboration. Preprint, arXiv:2406.12224. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J Black. 2019. Amass: Archive of motion capture as surface shapes. In Proceedings of the IEEE/CVF international conference on computer vision, pages 5442–5451. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, and 1 others. 2021. Isaac gym: High performance gpu-based physics simulation for robot learning. arXiv preprint arXiv:2108.10470. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Zhao Mandi, Shreeya Jain, and Shuran Song. 2023. Roco: Dialectic multi-robot collaboration with large language models. Preprint, arXiv:2307.04738. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Zhao Mandi, Shreeya Jain, and Shuran Song. 2024. Roco: Dialectic multi-robot collaboration with large language models. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 286–299. IEEE. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Christopher E Mower, Yuhui Wan, Hongzhan Yu, Antoine Grosnit, Jonas Gonzalez-Billandon, Matthieu Zimmer, Jinlong Wang, Xinyu Zhang, Yao Zhao, Anbang Zhai, and 1 others. 2024. Ros-llm: A ros framework for embodied ai with task feedback and structured reasoning. arXiv preprint arXiv:2406.19741. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Yashraj Narang, Kier Storey, Iretiayo Akinola, Miles Macklin, Philipp Reist, Lukasz Wawrzyniak, Yunrong Guo, Adam Moravanszky, Gavriel State, Michelle Lu, Ankur Handa, and Dieter Fox. 2022. Factory: Fast contact for robotic assembly. Preprint, arXiv:2205.03532. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. 2021. Amp: adversarial motion priors for stylized physics-based character control. ACM Transactions on Graphics, 40(4):1–20. "
                            }
                        ]
                    }
                ]
            },
            "bbox": [
                510,
                85,
                880,
                919
            ]
        },
        {
            "type": "page_number",
            "content": {
                "page_number_content": [
                    {
                        "type": "text",
                        "content": "10 "
                    }
                ]
            },
            "bbox": [
                489,
                942,
                509,
                954
            ]
        }
    ],
    [
        {
            "type": "list",
            "content": {
                "list_type": "reference_list",
                "list_items": [
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Julien Perolat, Bart De Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer, Paul Muller, Jerome T Connor, Neil Burch, Thomas Anthony, and 1 others. 2022. Mastering the game of stratego with model-free multiagent reinforcement learning. Science, 378(6623):990–996. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas Bäck. 2024. Reasoning with large language models, a survey. CoRR. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. 2017. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. arXiv preprint arXiv:1709.10087. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. 2024. Gpqa: A graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Sahar Salimpour, Lei Fu, Farhad Keramat, Leonardo Militano, Giovanni Toffetti, Harry Edelman, and Jorge Peña Queralta. 2025. Towards embodied agentic ai: Review and classification of llm-and vlm-driven robot autonomy and interaction. arXiv preprint arXiv:2508.05294. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2023. Progprompt: Generating situated robot task plans using large language models. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 11523–11530. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and Yu Su. 2023. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2998–3009. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv: Arxiv-2305.16291. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Weizheng Wang, Le Mao, Ruiqi Wang, and Byung-Cheol Min. 2024. Multi-robot cooperative sociallyaware navigation using multi-agent reinforcement learning. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 12353– 12360. IEEE. "
                            }
                        ]
                    }
                ]
            },
            "bbox": [
                115,
                85,
                489,
                917
            ]
        },
        {
            "type": "list",
            "content": {
                "list_type": "reference_list",
                "list_items": [
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. 2023. Tidybot: Personalized robot assistance with large language models. Autonomous Robots. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. 2018. Mean field multi-agent reinforcement learning. In International conference on machine learning, pages 5571–5580. PMLR. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Hangtao Zhang, Chenyu Zhu, Xianlong Wang, Ziqi Zhou, Shengshan Hu, and Leo Yu Zhang. 2024a. Badrobot: Jailbreaking llm-based embodied ai in the physical world. arXiv preprint arXiv:2407.20242, 3. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B. Tenenbaum, Tianmin Shu, and Chuang Gan. 2024b. Building cooperative embodied agents modularly with large language models. Preprint, arXiv:2307.02485. "
                            }
                        ]
                    }
                ]
            },
            "bbox": [
                510,
                85,
                882,
                351
            ]
        },
        {
            "type": "page_number",
            "content": {
                "page_number_content": []
            },
            "bbox": [
                489,
                942,
                507,
                954
            ]
        }
    ],
    [
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "Appendix "
                    }
                ],
                "level": 1
            },
            "bbox": [
                114,
                80,
                220,
                101
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "A Additional Details on CLiMBench "
                    }
                ],
                "level": 1
            },
            "bbox": [
                114,
                112,
                448,
                127
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "A.1 Simulation Setup and Agent Parameters "
                    }
                ],
                "level": 1
            },
            "bbox": [
                112,
                137,
                480,
                154
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Physics Simulation. All tasks in CLiMBench are developed in the IsaacGym physics simulator (Makoviychuk et al., 2021). The simulator executes robot actions in a step-by-step manner, advancing the physical state of the environment after each issued skill. Gravity, collision detection, and joint limit constraints are enabled for all agents to ensure physically consistent execution. "
                    }
                ]
            },
            "bbox": [
                112,
                159,
                487,
                287
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Agent Roster and Physical Specifications. The assembly scene in CLiMBench includes three types of heterogeneous robotic agents: a single robotic manipulator, three autonomous ground vehicles (AGVs), and one humanoid. Table 6 summarizes the key specifications of each agent type, including degrees of freedom, payload capacity, maximum reach, and the number of instantiated agents. "
                    }
                ]
            },
            "bbox": [
                112,
                288,
                485,
                417
            ]
        },
        {
            "type": "table",
            "content": {
                "image_source": {
                    "path": "images/f32f4ae922de2c23433cdc5df69052797ad649e869fc8b94914287556ae553a3.jpg"
                },
                "table_caption": [
                    {
                        "type": "text",
                        "content": "Table 6: Agent Parameters in CLiMBench. "
                    }
                ],
                "table_footnote": [],
                "html": "<table><tr><td></td><td>DoF</td><td>Payload</td><td>Max Reach</td><td>Quantity</td></tr><tr><td>Franka</td><td>7</td><td>3kg</td><td>855mm</td><td>1</td></tr><tr><td>TRACER Mini</td><td>3</td><td>80kg</td><td>All</td><td>3</td></tr><tr><td>Humanoid</td><td>28</td><td>-</td><td>All</td><td>1</td></tr></table>",
                "table_type": "simple_table",
                "table_nest_level": 1
            },
            "bbox": [
                119,
                426,
                482,
                495
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "The robotic arm is instantiated as a Franka Panda manipulator with seven degrees of freedom and a maximum reach of "
                    },
                    {
                        "type": "equation_inline",
                        "content": "8 5 5 \\mathrm { m m }"
                    },
                    {
                        "type": "text",
                        "content": ", suitable for precise manipulation and assembly. The AGVs are instantiated as TRACER Mini mobile robots with differential-drive kinematics and high payload capacity, enabling reliable transportation of components within the workspace. The humanoid features a high-dimensional articulated body with 28 degrees of freedom, supporting whole-body locomotion and object carrying skills. "
                    }
                ]
            },
            "bbox": [
                112,
                533,
                489,
                712
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "A.2 Environment Setup and Randomization "
                    }
                ],
                "level": 1
            },
            "bbox": [
                112,
                722,
                477,
                738
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Environment Workspace Initialization. All tasks in CLiMBench are executed within a shared global coordinate frame. The workspace domain, component placement candidates, and agent initialization ranges are summarized in Table 7. All object poses and agent base positions are specified in this global frame, while the vertical axis is used only to model object height and collision geometry. "
                    }
                ]
            },
            "bbox": [
                112,
                744,
                487,
                872
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Obstacle Configuration. In addition to randomly initialized agents and components, the environment includes static obstacles that block navi-"
                    }
                ]
            },
            "bbox": [
                112,
                873,
                489,
                920
            ]
        },
        {
            "type": "table",
            "content": {
                "image_source": {
                    "path": "images/3f9f1cec13ede8e8b499005adfc2168e8916a2f61f6586a00e551d2fc7c8b248.jpg"
                },
                "table_caption": [
                    {
                        "type": "text",
                        "content": "Table 7: Random Initialization of the Environment Components and Agents in CLiMBench. "
                    }
                ],
                "table_footnote": [],
                "html": "<table><tr><td>Entity</td><td>Initialization Range</td></tr><tr><td>Environment Domain</td><td>(x,y) ∈ [−6,6] × [−10,10]</td></tr><tr><td>Components (Wheels &amp; Trunk)</td><td>(x,y) ∈ { (4,8), (−4,8), (−4,−8), (4,−8) }</td></tr><tr><td>AGVs</td><td>x ∈ [−3,3], y ∈ [−5,5]</td></tr><tr><td>Humanoid</td><td>Central region of the environment</td></tr></table>",
                "table_type": "simple_table",
                "table_nest_level": 1
            },
            "bbox": [
                512,
                80,
                880,
                142
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "gation and cause occlusions. Four rectangular obstacles of size "
                    },
                    {
                        "type": "equation_inline",
                        "content": "1 \\times 5 \\mathrm { m }"
                    },
                    {
                        "type": "text",
                        "content": "are placed near the corners of the workspace. Three smaller obstacles of size "
                    },
                    {
                        "type": "equation_inline",
                        "content": "1 \\times 1 \\times 0 . 5 \\mathrm { m }"
                    },
                    {
                        "type": "text",
                        "content": "are positioned along primary traversal paths of the mobile agents, introducing potential collisions and blocked access regions. "
                    }
                ]
            },
            "bbox": [
                507,
                205,
                882,
                300
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Randomization Policy. For each episode, component locations and agent initial positions are independently sampled from the ranges specified in Table 7. All methods are evaluated under identical environment configurations and random seeds to ensure fair comparison. No curriculum learning or adaptive environment scheduling is employed. "
                    }
                ]
            },
            "bbox": [
                507,
                302,
                882,
                414
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "A.3 Low-level Skill Implementation Details "
                    }
                ],
                "level": 1
            },
            "bbox": [
                507,
                425,
                865,
                439
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Franka Manipulation Implementation. Franka manipulation skills are executed using a numerical inverse kinematics (IK) solver combined with an operational-space controller (OSC). The IK solver computes target joint configurations for the first seven joints, with a maximum of 200 iterations and a residual tolerance of "
                    },
                    {
                        "type": "equation_inline",
                        "content": "1 0 ^ { - 4 }"
                    },
                    {
                        "type": "text",
                        "content": ". The Franka base is initialized at a fixed pose "
                    },
                    {
                        "type": "equation_inline",
                        "content": "( 0 , - 2 . 0 , 0 )"
                    },
                    {
                        "type": "text",
                        "content": "with orientation quaternion "
                    },
                    {
                        "type": "equation_inline",
                        "content": "( 0 , 0 , 0 , 1 )"
                    },
                    {
                        "type": "text",
                        "content": ". Task-space motion is controlled using an impedance-based OSC with: "
                    }
                ]
            },
            "bbox": [
                507,
                445,
                882,
                605
            ]
        },
        {
            "type": "equation_interline",
            "content": {
                "math_content": "\\mathbf {F} = k _ {p} \\mathbf {e} + k _ {v} \\dot {\\mathbf {e}}, \\tag {1}",
                "math_type": "latex",
                "image_source": {
                    "path": "images/ba04aedb2fb29fdf3f92f4aaa40e943d3ced540b99452d7ab279ee2a7f26ae02.jpg"
                }
            },
            "bbox": [
                628,
                619,
                882,
                636
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "where the proportional gain is set to "
                    },
                    {
                        "type": "equation_inline",
                        "content": "k _ { p } = 5"
                    },
                    {
                        "type": "text",
                        "content": "and the derivative gain is chosen as "
                    },
                    {
                        "type": "equation_inline",
                        "content": "k _ { v } = 2 \\sqrt { k _ { p } }"
                    },
                    {
                        "type": "text",
                        "content": "to achieve critical damping. The gripper closing threshold is set to "
                    },
                    {
                        "type": "equation_inline",
                        "content": "0 . 0 5 \\mathrm { m }"
                    },
                    {
                        "type": "text",
                        "content": ", accounting for the object grasping radius of "
                    },
                    {
                        "type": "equation_inline",
                        "content": "0 . 0 3 \\mathrm { m }"
                    },
                    {
                        "type": "text",
                        "content": "used in CLiMBench. "
                    }
                ]
            },
            "bbox": [
                507,
                646,
                882,
                726
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "During assembly, components are aligned and attached using a magnetic attraction model with an effective range of "
                    },
                    {
                        "type": "equation_inline",
                        "content": "0 . 0 3 \\mathrm { m }"
                    },
                    {
                        "type": "text",
                        "content": ". For each manipulation trajectory, operational-space waypoints are generated via interpolation to ensure smooth and continuous execution. "
                    }
                ]
            },
            "bbox": [
                507,
                728,
                882,
                822
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "AGV Navigation and Motion Control. AGV transportation skills are implemented using a Rapidly-exploring Random Tree (RRT) planner operating in the planar workspace. The planner considers all static and dynamic obstacles and robots in the environment as the collision space. The "
                    }
                ]
            },
            "bbox": [
                507,
                824,
                884,
                921
            ]
        },
        {
            "type": "page_number",
            "content": {
                "page_number_content": [
                    {
                        "type": "text",
                        "content": "12 "
                    }
                ]
            },
            "bbox": [
                489,
                941,
                510,
                954
            ]
        }
    ],
    [
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "RRT planner uses a collision-checking resolution of "
                    },
                    {
                        "type": "equation_inline",
                        "content": "0 . 1 \\mathrm { m }"
                    },
                    {
                        "type": "text",
                        "content": "along each edge and a rewire count of 32. Once a feasible path is found, it is executed using differential-drive control to enable smooth turning and stable navigation. For final positioning during delivery, the AGV follows a straight-line motion segment to improve placement reliability. "
                    }
                ]
            },
            "bbox": [
                112,
                84,
                487,
                197
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Humanoid Observation and Skill Interface. Humanoid skills are executed by a pretrained policy that operates on a structured observation interface. Let "
                    },
                    {
                        "type": "equation_inline",
                        "content": "N _ { j } = 1 5"
                    },
                    {
                        "type": "text",
                        "content": "denote the number of articulated bodies of the humanoid. For each body "
                    },
                    {
                        "type": "equation_inline",
                        "content": "j \\in \\{ 1 , \\ldots , N _ { j } \\}"
                    },
                    {
                        "type": "text",
                        "content": ", the observation is defined as "
                    }
                ]
            },
            "bbox": [
                112,
                197,
                489,
                294
            ]
        },
        {
            "type": "equation_interline",
            "content": {
                "math_content": "\\mathbf {o} _ {j} = \\left[ \\mathbf {q} _ {j}, \\mathbf {p} _ {j}, \\mathbf {v} _ {j}, \\mathbf {a} _ {j} \\right], \\tag {2}",
                "math_type": "latex",
                "image_source": {
                    "path": "images/ec9013eb92cf184a6f72b52dce5c4d4a22e230ac06dd472a42b86e06d63dae39.jpg"
                }
            },
            "bbox": [
                216,
                303,
                487,
                322
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "where "
                    },
                    {
                        "type": "equation_inline",
                        "content": "\\mathbf { q } _ { j } \\in \\mathbb { R } ^ { 6 }"
                    },
                    {
                        "type": "text",
                        "content": "represents the body relative rotational and translational configuration, "
                    },
                    {
                        "type": "equation_inline",
                        "content": "\\mathbf { p } _ { j } \\in \\mathbb { R } ^ { 3 }"
                    },
                    {
                        "type": "text",
                        "content": "the Cartesian position, "
                    },
                    {
                        "type": "equation_inline",
                        "content": "\\mathbf { v } _ { j } \\in \\mathbb { R } ^ { 3 }"
                    },
                    {
                        "type": "text",
                        "content": "the linear velocity, and "
                    },
                    {
                        "type": "equation_inline",
                        "content": "\\mathbf { a } _ { j } \\in \\mathbb { R } ^ { 3 }"
                    },
                    {
                        "type": "text",
                        "content": "the linear acceleration. The full humanoid observation vector is constructed by concatenating all body-level observations and a task observation: "
                    }
                ]
            },
            "bbox": [
                112,
                331,
                487,
                428
            ]
        },
        {
            "type": "equation_interline",
            "content": {
                "math_content": "\\mathbf {o} = \\left[ \\mathbf {o} _ {1}, \\mathbf {o} _ {2}, \\dots , \\mathbf {o} _ {N _ {j}}, \\mathbf {o} _ {\\text {t a s k}} \\right]. \\tag {3}",
                "math_type": "latex",
                "image_source": {
                    "path": "images/13507fcda49bc8d703ef5770369ce5a4fba84f6d2ebf551bd5b97cb30c29937a.jpg"
                }
            },
            "bbox": [
                189,
                438,
                487,
                456
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Observations of the root body and global reference frame are excluded, as they encode absolute pose information rather than local kinematic and dynamic states. This formulation provides a consistent interface for skill execution while remaining independent of the humanoid’s global position and orientation in the environment. "
                    }
                ]
            },
            "bbox": [
                112,
                466,
                487,
                576
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "A.4 Humanoid Carry Skill Training Details "
                    }
                ],
                "level": 1
            },
            "bbox": [
                112,
                589,
                472,
                604
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "AMP Objective and Reward Design. Humanoid carrying skills are trained using the Adversarial Motion Priors (AMP) framework (Peng et al., 2021), following the single-object manipulation paradigm introduced in COOHOI (Gao et al., 2024). AMP augments the task objective with a discriminatorbased style reward that encourages motions consistent with human motion data. At each time step "
                    },
                    {
                        "type": "equation_inline",
                        "content": "t"
                    },
                    {
                        "type": "text",
                        "content": ", the total reward is defined as "
                    }
                ]
            },
            "bbox": [
                112,
                609,
                487,
                752
            ]
        },
        {
            "type": "equation_interline",
            "content": {
                "math_content": "r _ {t} = w ^ {G} r ^ {G} \\left(\\mathbf {s} _ {t}, \\mathbf {g} _ {t}, \\mathbf {s} _ {t + 1}\\right) + w ^ {S} r ^ {S} \\left(\\mathbf {s} _ {t}, \\mathbf {s} _ {t + 1}\\right), \\tag {4}",
                "math_type": "latex",
                "image_source": {
                    "path": "images/310cc77964b47d64e3bb209f68ab7dca4a2ac14a2d12239d7d83c892d7300305.jpg"
                }
            },
            "bbox": [
                127,
                762,
                487,
                781
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "where "
                    },
                    {
                        "type": "equation_inline",
                        "content": "r ^ { G }"
                    },
                    {
                        "type": "text",
                        "content": "denotes the task-specific reward and "
                    },
                    {
                        "type": "equation_inline",
                        "content": "r ^ { S }"
                    },
                    {
                        "type": "text",
                        "content": "is the AMP style reward. The task reward consists of components for walking, holding, and placement: "
                    }
                ]
            },
            "bbox": [
                112,
                791,
                487,
                840
            ]
        },
        {
            "type": "equation_interline",
            "content": {
                "math_content": "r ^ {G} = 0. 2 r _ {\\text {w a l k}} ^ {G} + 0. 4 r _ {\\text {h e l d}} ^ {G} + 0. 4 r _ {\\text {t a r g e t}} ^ {G}, \\tag {5}",
                "math_type": "latex",
                "image_source": {
                    "path": "images/6dd9d9b24a4fb65e5328587b77287e20dc4b07fe11275475b0640087d8ac2eff.jpg"
                }
            },
            "bbox": [
                156,
                848,
                487,
                869
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "while the style reward is computed as "
                    }
                ]
            },
            "bbox": [
                112,
                878,
                394,
                892
            ]
        },
        {
            "type": "equation_interline",
            "content": {
                "math_content": "r ^ {S} \\left(\\mathbf {s} _ {t}, \\mathbf {s} _ {t + 1}\\right) = - \\log (1 - D \\left(\\mathbf {s} _ {t}, \\mathbf {s} _ {t + 1}\\right)), \\tag {6}",
                "math_type": "latex",
                "image_source": {
                    "path": "images/eef23e1907a5d5d663378b5429c277ff751a9958525f6e7202199a3041628924.jpg"
                }
            },
            "bbox": [
                139,
                903,
                487,
                922
            ]
        },
        {
            "type": "table",
            "content": {
                "image_source": {
                    "path": "images/59f490a4e9f0026439117a1a41794994333a13a1761d49095d719849cb824c4e.jpg"
                },
                "table_caption": [
                    {
                        "type": "text",
                        "content": "Table 8: Humanoid Carrying Skill Training. "
                    }
                ],
                "table_footnote": [],
                "html": "<table><tr><td>Parameters</td><td>Value</td></tr><tr><td>num envs</td><td>16384</td></tr><tr><td>episode length</td><td>600</td></tr><tr><td>discount factor</td><td>0.99</td></tr><tr><td>e Clip</td><td>0.2</td></tr><tr><td>horizon_length</td><td>32</td></tr><tr><td>minibatch_size</td><td>16384</td></tr><tr><td>amp_minibatch_size</td><td>4096</td></tr><tr><td>task Reward_weight</td><td>0.5</td></tr><tr><td>disc Reward_weight</td><td>0.5</td></tr></table>",
                "table_type": "simple_table",
                "table_nest_level": 1
            },
            "bbox": [
                588,
                80,
                803,
                229
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "with "
                    },
                    {
                        "type": "equation_inline",
                        "content": "D ( \\cdot )"
                    },
                    {
                        "type": "text",
                        "content": "denoting the discriminator trained on reference human motion data. "
                    }
                ]
            },
            "bbox": [
                507,
                282,
                880,
                313
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Discriminator Training. The AMP discriminator is trained to distinguish between policygenerated motion transitions and reference human motion data. We adopt the same discriminator architecture and training procedure as in (Peng et al., 2021), where the discriminator operates on pairs of consecutive states "
                    },
                    {
                        "type": "equation_inline",
                        "content": "\\left( \\mathbf { s } _ { t } , \\mathbf { s } _ { t + 1 } \\right)"
                    },
                    {
                        "type": "text",
                        "content": "and outputs the probability that the transition originates from real motion data. Reference motions are drawn from the AC-CAD subset of the AMASS dataset (Mahmood et al., 2019), which contains a diverse set of human motions including locomotion, lifting, carrying, and object placement. During training, the discriminator and the policy are updated alternately. The discriminator is trained using binary cross-entropy loss to separate reference transitions from policygenerated transitions, while the policy receives the discriminator-based style reward defined in Eq. 6. This adversarial training scheme encourages the humanoid policy to produce stable and human-like motions while optimizing task-specific objectives. "
                    }
                ]
            },
            "bbox": [
                507,
                318,
                882,
                656
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Domain Randomization. To improve robustness and generalization in assembly environments, we apply domain randomization during training. Randomized factors include object orientation, relative alignment between the humanoid and the object, and object proximity. These variations encourage the learned policy to tolerate spatial uncertainty and contact perturbations commonly encountered during execution in CLiMBench. "
                    }
                ]
            },
            "bbox": [
                507,
                659,
                882,
                802
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Training Hyperparameters. The humanoid policy is trained using Proximal Policy Optimization (PPO) (Schulman et al., 2017). Key training hyperparameters, including the number of parallel environments, episode length, reward weights, and optimization settings, are summarized in Table 8. "
                    }
                ]
            },
            "bbox": [
                507,
                806,
                880,
                902
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Training Infrastructure. Training is conducted "
                    }
                ]
            },
            "bbox": [
                527,
                904,
                880,
                920
            ]
        },
        {
            "type": "page_number",
            "content": {
                "page_number_content": [
                    {
                        "type": "text",
                        "content": "13 "
                    }
                ]
            },
            "bbox": [
                489,
                941,
                507,
                954
            ]
        }
    ],
    [
        {
            "type": "image",
            "content": {
                "image_source": {
                    "path": "images/abaee7e542397b6b8fd0a5f583b2aba33a841ef074f817626a4d574450607eb9.jpg"
                },
                "image_caption": [
                    {
                        "type": "text",
                        "content": "(a) "
                    }
                ],
                "image_footnote": []
            },
            "bbox": [
                181,
                93,
                391,
                204
            ]
        },
        {
            "type": "image",
            "content": {
                "image_source": {
                    "path": "images/41568b566994b0af4ec7b169b9f7d5ca815f4c4a5e5a10adea69af116393012e.jpg"
                },
                "image_caption": [
                    {
                        "type": "text",
                        "content": "(b) "
                    }
                ],
                "image_footnote": []
            },
            "bbox": [
                398,
                93,
                606,
                204
            ]
        },
        {
            "type": "image",
            "content": {
                "image_source": {
                    "path": "images/b5241c338f0dc945aa7059ee373b50c76c907ec34688927d2b6fba61baa9370c.jpg"
                },
                "image_caption": [
                    {
                        "type": "text",
                        "content": "Figure 5: Single Agent Prompt Templates. This template encodes the agent’s name, unique identifier, and action specification to ensure structured and unambiguous command interpretation for an agent executor. "
                    }
                ],
                "image_footnote": []
            },
            "bbox": [
                608,
                93,
                818,
                205
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "(c) "
                    }
                ]
            },
            "bbox": [
                715,
                208,
                729,
                219
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Figure 4: Ablation Study on AMP Domain Randomization. Results show that Domain Randomization helps the humanoid agent to obtain a stable and reusable carrying skill in CLiMBench. "
                    }
                ]
            },
            "bbox": [
                112,
                237,
                880,
                267
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "Single Agent Prompt Templates "
                    }
                ],
                "level": 1
            },
            "bbox": [
                139,
                292,
                337,
                306
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Think of yourself as one of the robots #ID(101, 606, 202, 203, or 204), and your available skills are: "
                    }
                ]
            },
            "bbox": [
                144,
                312,
                763,
                324
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Available Skills for Wheeled Robots: "
                    }
                ]
            },
            "bbox": [
                146,
                326,
                364,
                335
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "- [#SKILL#] <#ROBOT#> (#ID) move to component location using RRT path "
                    }
                ]
            },
            "bbox": [
                146,
                336,
                564,
                346
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Your role: You are a #character skill description# "
                    }
                ]
            },
            "bbox": [
                146,
                349,
                450,
                359
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "The list of available actions you can perform in your current environment is: #ACTIONLIST# "
                    }
                ]
            },
            "bbox": [
                146,
                362,
                690,
                372
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "The instructions for the next step of the task is：#INSTRUCTION# "
                    }
                ]
            },
            "bbox": [
                146,
                376,
                532,
                386
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Please choose the best available action to achieve the goal as soon as possible. "
                    }
                ]
            },
            "bbox": [
                146,
                388,
                628,
                399
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Note: If there is an action in the available actionlist that can satisfy the instruction, the first sentence in the output needs to be \"YES I CAN.\" "
                    }
                ]
            },
            "bbox": [
                146,
                403,
                842,
                423
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Note: if there is not an action in the available actionlist that can satisfy the instruction, then output the possible reasons, such as the agent does not have the ability to execute the current instruction and needs the assistance of other types of agents; or the current agent is not in the state of executing this instruction and an additional action needs to be performed as a prerequisite before the instruction can be executed; or the current state already meets the requirements of the task instructions and no other actions need to be performed. The first sentence in the output needs to be \"SORRY I CANNOT.\" "
                    }
                ]
            },
            "bbox": [
                144,
                425,
                835,
                482
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Note: Think of yourself as #ROBOT#, generating content in a first-person conversation. "
                    }
                ]
            },
            "bbox": [
                146,
                486,
                665,
                495
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Let's think step by step, but you should strictly obey the rules above about the first sentence in the output, do not include \"*\" in the first sentence to affect output parsing. "
                    }
                ]
            },
            "bbox": [
                146,
                499,
                853,
                519
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "in IsaacGym headless mode on a single NVIDIA A100 GPU to enable large-scale parallel simulation. After training, the learned policy is deployed with rendering enabled on a workstation equipped with an NVIDIA RTX 4060 Ti for evaluation. "
                    }
                ]
            },
            "bbox": [
                112,
                606,
                487,
                684
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Ablation on Domain Randomization. Fig. 4 presents an ablation on domain randomization in AMP training. Without randomization, it converges to a local optimum early and exhibits poor generalization, while domain-randomized training achieves consistently higher task rewards and more reliable execution. This result supports the use of domain randomization to obtain a stable and reusable carrying skill for the benchmark. "
                    }
                ]
            },
            "bbox": [
                112,
                688,
                487,
                833
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "A.5 LLM-to-Skill Execution Interface "
                    }
                ],
                "level": 1
            },
            "bbox": [
                112,
                848,
                428,
                863
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Command Schema and Prompt Templates. All robot actions in CLiMBench are issued through structured natural-language commands generated "
                    }
                ]
            },
            "bbox": [
                112,
                873,
                487,
                921
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "by LLM-based planners. Each command explicitly specifies the action type, the acting agent(s), the target object, and the target location. Figure 5 illustrates the single-agent prompt templates used to ensure parsable command generation. These templates encode the agent name, unique identifier, and action specification, providing a unified interface across heterogeneous robots. "
                    }
                ]
            },
            "bbox": [
                507,
                606,
                884,
                734
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Command Parsing via Regular Expressions. Given an LLM-generated command "
                    },
                    {
                        "type": "equation_inline",
                        "content": "C"
                    },
                    {
                        "type": "text",
                        "content": ", the execution interface parses it into a structured representation using a set of predefined regular expressions. Formally, each command is mapped as "
                    }
                ]
            },
            "bbox": [
                507,
                734,
                884,
                816
            ]
        },
        {
            "type": "equation_interline",
            "content": {
                "math_content": "C \\xrightarrow {\\text {p a r s e}} \\{G, A, O, L \\}, \\tag {7}",
                "math_type": "latex",
                "image_source": {
                    "path": "images/578a4fa70e8233505dae5194217131f72ac32c756fa3bedb474c94c5f1803a5d.jpg"
                }
            },
            "bbox": [
                608,
                825,
                882,
                845
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "where "
                    },
                    {
                        "type": "equation_inline",
                        "content": "G"
                    },
                    {
                        "type": "text",
                        "content": "denotes the group identifier, "
                    },
                    {
                        "type": "equation_inline",
                        "content": "A \\quad ="
                    },
                    {
                        "type": "equation_inline",
                        "content": "\\{ ( a _ { i } , \\mathrm { i d } _ { i } ) \\}"
                    },
                    {
                        "type": "text",
                        "content": "is the set of assigned agents with their identifiers, "
                    },
                    {
                        "type": "equation_inline",
                        "content": "O"
                    },
                    {
                        "type": "text",
                        "content": "specifies the target object and its identifier, and "
                    },
                    {
                        "type": "equation_inline",
                        "content": "L"
                    },
                    {
                        "type": "text",
                        "content": "denotes the target location. Only com-"
                    }
                ]
            },
            "bbox": [
                507,
                857,
                882,
                921
            ]
        },
        {
            "type": "page_number",
            "content": {
                "page_number_content": [
                    {
                        "type": "text",
                        "content": "14 "
                    }
                ]
            },
            "bbox": [
                489,
                941,
                509,
                954
            ]
        }
    ],
    [
        {
            "type": "image",
            "content": {
                "image_source": {
                    "path": "images/8ae95089de308a37a288b10b4ae8ff956856962680db59792f01ce6f0f5695ee.jpg"
                },
                "image_caption": [],
                "image_footnote": []
            },
            "bbox": [
                200,
                87,
                494,
                171
            ]
        },
        {
            "type": "image",
            "content": {
                "image_source": {
                    "path": "images/03934ced9a9a9e712b52d7eea141f29896e126f84100da805abe514d91f6bb92.jpg"
                },
                "image_caption": [],
                "image_footnote": []
            },
            "bbox": [
                502,
                87,
                796,
                172
            ]
        },
        {
            "type": "image",
            "content": {
                "image_source": {
                    "path": "images/23a007a2d74a40f9078730104be4a214b883e062d5c708fa7c4cd1a138ea68d3.jpg"
                },
                "image_caption": [],
                "image_footnote": []
            },
            "bbox": [
                200,
                177,
                494,
                292
            ]
        },
        {
            "type": "image",
            "content": {
                "image_source": {
                    "path": "images/7a10c5c6d070c0a9f4079575c7ff11db7d39c98fa4a75f74d2dac437d1765ef6.jpg"
                },
                "image_caption": [],
                "image_footnote": []
            },
            "bbox": [
                502,
                177,
                796,
                291
            ]
        },
        {
            "type": "image",
            "content": {
                "image_source": {
                    "path": "images/32fb72e61c3b1bd29862e10d9ef1578759a2aa23ea7dae8c8b2eaf42ed6da664.jpg"
                },
                "image_caption": [],
                "image_footnote": []
            },
            "bbox": [
                200,
                297,
                494,
                321
            ]
        },
        {
            "type": "image",
            "content": {
                "image_source": {
                    "path": "images/af0554e78799ef2ac129a83cebb34c0788f6c37ddb88d042dd754d9690511e92.jpg"
                },
                "image_caption": [
                    {
                        "type": "text",
                        "content": "Figure 6: Tasks of Different Difficulties. To evaluate the effectiveness of our method in facilitating multi-robot collaboration, we design four heterogeneous agent scenarios under two difficulty tiers, namely easy and hard. "
                    }
                ],
                "image_footnote": []
            },
            "bbox": [
                502,
                297,
                796,
                321
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "mands that can be successfully parsed into this structure are considered for further verification. "
                    }
                ]
            },
            "bbox": [
                112,
                391,
                485,
                420
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Command Verification. To ensure reliable and safe execution, each parsed command is validated through a four-stage verification pipeline. Let "
                    },
                    {
                        "type": "equation_inline",
                        "content": "f"
                    },
                    {
                        "type": "text",
                        "content": "denote an LLM function call corresponding to a structured command. The verification operator "
                    },
                    {
                        "type": "equation_inline",
                        "content": "V"
                    },
                    {
                        "type": "text",
                        "content": "maps "
                    },
                    {
                        "type": "equation_inline",
                        "content": "f"
                    },
                    {
                        "type": "text",
                        "content": "to a validation vector "
                    }
                ]
            },
            "bbox": [
                112,
                423,
                485,
                518
            ]
        },
        {
            "type": "equation_interline",
            "content": {
                "math_content": "f \\xrightarrow {V} \\left\\{v _ {1}, v _ {2}, v _ {3}, v _ {4} \\right\\}, \\tag {8}",
                "math_type": "latex",
                "image_source": {
                    "path": "images/47f8d45a27019635cd31bc8b2b6891855451e7bb64d880f5396f7f66aa8bdd29.jpg"
                }
            },
            "bbox": [
                213,
                527,
                485,
                549
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "where each component corresponds to a specific verification stage. The pipeline consists of the following four stages: "
                    }
                ]
            },
            "bbox": [
                112,
                558,
                487,
                607
            ]
        },
        {
            "type": "list",
            "content": {
                "list_type": "text_list",
                "list_items": [
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "• Capability assessment. The system first evaluates whether the assigned agent(s) are capable of executing the requested action based on their available skills. The assessment yields a binary decision or confidence score, and commands below a preset threshold "
                            },
                            {
                                "type": "equation_inline",
                                "content": "\\tau _ { c }"
                            },
                            {
                                "type": "text",
                                "content": "are rejected or deferred. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "• Action selection. For commands that pass the capability assessment, the planner selects a concrete action "
                            },
                            {
                                "type": "equation_inline",
                                "content": "a ^ { * }"
                            },
                            {
                                "type": "text",
                                "content": "from the available action set "
                            },
                            {
                                "type": "equation_inline",
                                "content": "\\mathcal { A }"
                            },
                            {
                                "type": "text",
                                "content": ", optionally providing a ranked subset or peraction confidence. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "• Action parsing. The selected action "
                            },
                            {
                                "type": "equation_inline",
                                "content": "a ^ { * }"
                            },
                            {
                                "type": "text",
                                "content": "is processed by a parsing operator "
                            },
                            {
                                "type": "equation_inline",
                                "content": "\\mathcal { P }"
                            },
                            {
                                "type": "text",
                                "content": "to produce a structured command "
                            },
                            {
                                "type": "equation_inline",
                                "content": "C ^ { \\mathrm { s t r u c t } } = \\mathcal { P } ( a ^ { * } )"
                            },
                            {
                                "type": "text",
                                "content": ", which includes fields such as action type, agent assignment, target object, and target location. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "• Execution verification (judge). A dedicated judge module "
                            },
                            {
                                "type": "equation_inline",
                                "content": "\\mathcal { I }"
                            },
                            {
                                "type": "text",
                                "content": "evaluates "
                            },
                            {
                                "type": "equation_inline",
                                "content": "C ^ { \\mathrm { s t r u c t } }"
                            },
                            {
                                "type": "text",
                                "content": "for format "
                            }
                        ]
                    }
                ]
            },
            "bbox": [
                119,
                617,
                489,
                921
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "correctness, semantic consistency, physical feasibility, and execution safety. "
                    }
                ]
            },
            "bbox": [
                527,
                391,
                882,
                422
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Only commands that satisfy all verification criteria are forwarded to the execution module. Overall, the verification operator can be expressed as "
                    }
                ]
            },
            "bbox": [
                507,
                437,
                882,
                485
            ]
        },
        {
            "type": "equation_interline",
            "content": {
                "math_content": "V = \\mathcal {J} \\circ \\mathcal {P} \\circ \\mathcal {S} _ {2} \\circ \\mathcal {S} _ {1}, \\tag {9}",
                "math_type": "latex",
                "image_source": {
                    "path": "images/834d82b11bbe8c7cd91236888264cd2848b37358e6578199db504b622b8bd147.jpg"
                }
            },
            "bbox": [
                610,
                500,
                880,
                516
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "where "
                    },
                    {
                        "type": "equation_inline",
                        "content": "S _ { 1 }"
                    },
                    {
                        "type": "text",
                        "content": "and "
                    },
                    {
                        "type": "equation_inline",
                        "content": "S _ { 2 }"
                    },
                    {
                        "type": "text",
                        "content": "denote the capability assessment and action selection stages, respectively. "
                    }
                ]
            },
            "bbox": [
                507,
                532,
                880,
                564
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Failure Handling and Feedback Logging. Commands that fail at any stage of the verification pipeline are not executed. Instead, the interface records structured failure feedback indicating the reason for rejection, such as insufficient agent capability, semantic mismatch, or physical infeasibility. This feedback is propagated to the future planning process and stored in the context memory module, enabling planners to adjust subsequent decisions. Deferred commands may be reconsidered in later planning iterations once agent states change or environment conditions are ready. "
                    }
                ]
            },
            "bbox": [
                507,
                565,
                882,
                757
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "A.6 Benchmark Evaluation Details "
                    }
                ],
                "level": 1
            },
            "bbox": [
                507,
                771,
                801,
                784
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Task Difficulty Settings. We define two levels of task difficulty, referred to as easy and hard, which differ in obstacle configuration and agent initialization. In the easy setting, AGV transportation paths are free of blocking obstacles, and initial agent placements are arranged to avoid potential collisions with the humanoid. In contrast, the hard setting introduces obstacles along AGV trajectories "
                    }
                ]
            },
            "bbox": [
                507,
                791,
                880,
                921
            ]
        },
        {
            "type": "page_number",
            "content": {
                "page_number_content": []
            },
            "bbox": [
                489,
                942,
                507,
                954
            ]
        }
    ],
    [
        {
            "type": "image",
            "content": {
                "image_source": {
                    "path": "images/9c92623a107cf8f6d58114bca544b437775376ef3abad8d6e76cf450cd5ec8ee.jpg"
                },
                "image_caption": [
                    {
                        "type": "text",
                        "content": "Figure 8: Parallel Execution. To enhance task execution efficiency, agents within each group are executed in parallel, as reflected in the execution log. "
                    }
                ],
                "image_footnote": []
            },
            "bbox": [
                178,
                85,
                823,
                259
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Figure 7: Simulation. It shows an example of the assembly process in the dynamic simulation. "
                    }
                ]
            },
            "bbox": [
                176,
                272,
                818,
                287
            ]
        },
        {
            "type": "code",
            "content": {
                "code_caption": [],
                "code_content": [
                    {
                        "type": "text",
                        "content": "Parallel Execution   \nGroup 0: <humanoid>(101) - Sub-goal: Clear obstacles to ensure paths are clear for component transportation   \nGroup 1: <mobile_car_1>(201) - Sub-goal: Transport the left wheel (405) to the assembly area   \nGroup 2: <mobile_car_2>(202) - Sub-goal: Transport the right wheel (406) to the assembly area   \nGroup 3: <mobile_car_3>(203) - Sub-goal: Transport the trunk (303) to the assembly area   \nGroup 4: <robot arm>(606) - Sub-goal: Assemble the components by attaching the wheels to the trunk once all components are delivered   \nProcessing group 0 message: Hello <humanoid>(101): Please use [carry] skill to clear obstacles.   \n#**********the first sentence is YES I CAN   \n#**********the first sentence is YES I CAN   \nNo more things to do!   \nProcessing group 1 message: Hello <mobile_car_1>(201): Please use [move] skill to navigate to <left wheel>(405) location, then use [push] skill to transport it to the franka assembly area.   \n#**********the first sentence is YES I CAN   \n#**********the first sentence is YES I CAN   \n[move] <mobile_car_1> (201) move to assigned component location using RRT path   \nProcessing group 4 message: \"Hello <franka>(606): Please use [check] skill to check <trunk>(303) for assembly readiness.\"   \n#**********the first sentence is YES I CAN   \n#**********the first sentence is YES I CAN   \n[check] <franka> (606) check <trunk> (303) "
                    }
                ],
                "code_language": "txt"
            },
            "bbox": [
                136,
                310,
                865,
                550
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "and initializes agents in closer proximity, increasing the likelihood of collisions and requiring more coordination during execution. Examples of the two settings are illustrated in Fig. 6. "
                    }
                ]
            },
            "bbox": [
                112,
                626,
                487,
                690
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Trials and Stochasticity. Due to stochasticity in low-level skill execution and environment interactions, each task configuration is evaluated over multiple independent trials. Unless otherwise specified, we run each task five times and report averaged metrics across trials. All methods are evaluated under identical environment configurations and random seeds to ensure fair comparison. An example of the assembly process can be seen in Fig. 7. "
                    }
                ]
            },
            "bbox": [
                112,
                693,
                487,
                837
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Step Budget and Timeout Criteria. To control evaluation cost and ensure comparability, each task is assigned a maximum step budget. The step budget is defined as twice the number of steps in a manually derived minimal-step solution for the "
                    }
                ]
            },
            "bbox": [
                112,
                841,
                489,
                921
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "corresponding task. A task is considered unsuccessful if the target assembly is not completed within this budget, even if individual skills execute successfully. This criterion penalizes inefficient longhorizon planning and excessive replanning. "
                    }
                ]
            },
            "bbox": [
                507,
                626,
                884,
                706
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Execution Logging. During evaluation, the benchmark records detailed execution logs, including agent group assignments, issued skill commands, verification outcomes, and environment feedback at each step. These logs enable post-hoc analysis of planning behavior, grouping dynamics, and failure recovery, and are used for qualitative visualization and debugging. An example of the parallel execution logs can be seen in Fig. 8. "
                    }
                ]
            },
            "bbox": [
                507,
                707,
                884,
                851
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "B Example Logs and Prompts "
                    }
                ],
                "level": 1
            },
            "bbox": [
                507,
                863,
                788,
                879
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "We show some examples of logs and prompts in all the figures below. "
                    }
                ]
            },
            "bbox": [
                507,
                888,
                880,
                919
            ]
        },
        {
            "type": "page_number",
            "content": {
                "page_number_content": [
                    {
                        "type": "text",
                        "content": "16 "
                    }
                ]
            },
            "bbox": [
                489,
                942,
                510,
                954
            ]
        }
    ],
    [
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "Scene Dynamics Log "
                    }
                ],
                "level": 1
            },
            "bbox": [
                139,
                175,
                275,
                189
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "You are a structured output formatter for multi-agent task coordination. Your job is to convert a detailed grouping strategy into a precise, parseable format. "
                    }
                ]
            },
            "bbox": [
                144,
                193,
                845,
                214
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "Step:0 "
                    }
                ],
                "level": 1
            },
            "bbox": [
                147,
                223,
                186,
                231
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Assemble robot components: attach left and right wheels to trunk "
                    }
                ]
            },
            "bbox": [
                146,
                231,
                536,
                241
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 0: <humanoid>(101) - Sub-goal: Clear obstacles to ensure paths are clear for component transportation "
                    }
                ]
            },
            "bbox": [
                146,
                241,
                794,
                250
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 1: <mobile_car_1>(201) - Sub-goal: Transport the left wheel (405) to the assembly area "
                    }
                ]
            },
            "bbox": [
                147,
                250,
                705,
                260
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 2: <mobile_car_2>(202) - Sub-goal: Transport the right wheel (406) to the assembly area "
                    }
                ]
            },
            "bbox": [
                147,
                260,
                710,
                269
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 3: <mobile_car_3>(203) - Sub-goal: Transport the trunk (303) to the assembly area "
                    }
                ]
            },
            "bbox": [
                147,
                269,
                675,
                278
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 4: <robot arm>(606) - Sub-goal: Assemble the components by attaching the wheels to the trunk once all "
                    }
                ]
            },
            "bbox": [
                147,
                278,
                794,
                287
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "components are delivered "
                    }
                ]
            },
            "bbox": [
                147,
                288,
                295,
                296
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "Step:1"
                    }
                ],
                "level": 1
            },
            "bbox": [
                147,
                307,
                186,
                313
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Assemble robot components: attach left and right wheels to trunk "
                    }
                ]
            },
            "bbox": [
                147,
                315,
                536,
                324
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 0: <humanoid>(101) - Sub-goal: Clear any obstacles that might prevent the mobile cars from reaching and transporting their components "
                    }
                ]
            },
            "bbox": [
                146,
                325,
                806,
                343
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 1: <mobile_car_1>(201) - Sub-goal: Transport the left wheel to the franka assembly area "
                    }
                ]
            },
            "bbox": [
                147,
                343,
                710,
                353
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 2: <mobile_car_2>(202) - Sub-goal: Transport the right wheel to the franka assembly area "
                    }
                ]
            },
            "bbox": [
                147,
                353,
                717,
                362
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 3: <mobile_car_3>(203) - Sub-goal: Transport the trunk to the franka assembly area "
                    }
                ]
            },
            "bbox": [
                147,
                362,
                680,
                370
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 4: <robot arm>(606) - Sub-goal: Assemble the components once delivered to the assembly area "
                    }
                ]
            },
            "bbox": [
                147,
                370,
                734,
                381
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "Step:2 "
                    }
                ],
                "level": 1
            },
            "bbox": [
                147,
                390,
                186,
                398
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Assemble robot components: attach left and right wheels to trunk "
                    }
                ]
            },
            "bbox": [
                147,
                400,
                536,
                407
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 0: <humanoid>(101) - Sub-goal: Ensure all paths are clear for mobile cars to transport components "
                    }
                ]
            },
            "bbox": [
                147,
                409,
                769,
                417
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 1: <mobile_car_1>(201) - Sub-goal: Transport left wheel to the assembly area "
                    }
                ]
            },
            "bbox": [
                147,
                417,
                643,
                426
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 2: <mobile_car_2>(202) - Sub-goal: Transport right wheel to the assembly area "
                    }
                ]
            },
            "bbox": [
                147,
                426,
                650,
                436
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 3: <mobile_car_3>(203) - Sub-goal: Transport trunk to the assembly area "
                    }
                ]
            },
            "bbox": [
                147,
                436,
                613,
                445
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 4: <robot arm>(606) - Sub-goal: Assemble left and right wheels onto trunk when components are ready "
                    }
                ]
            },
            "bbox": [
                147,
                445,
                783,
                455
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "Step:3 "
                    }
                ],
                "level": 1
            },
            "bbox": [
                147,
                464,
                186,
                472
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Assemble robot components: attach left and right wheels to trunk "
                    }
                ]
            },
            "bbox": [
                147,
                474,
                536,
                482
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 0: <mobile_car_1>(201), <mobile_car_2>(202), <mobile_car_3>(203) - Sub-goal: Reposition components if not in "
                    }
                ]
            },
            "bbox": [
                147,
                483,
                836,
                491
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "optimal position for assembly "
                    }
                ]
            },
            "bbox": [
                147,
                492,
                324,
                501
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 1: <robot arm>(606) - Sub-goal: Check readiness of left and right wheels (405 and 406) for assembly, then assemble onto trunk (303) "
                    }
                ]
            },
            "bbox": [
                147,
                502,
                818,
                520
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 2: <humanoid>(101) - Sub-goal: Clear any obstacles if necessary to ensure accessibility of assembly area "
                    }
                ]
            },
            "bbox": [
                147,
                520,
                813,
                529
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "Step:4"
                    }
                ],
                "level": 1
            },
            "bbox": [
                147,
                539,
                186,
                546
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Assemble robot components: attach left and right wheels to trunk "
                    }
                ]
            },
            "bbox": [
                147,
                548,
                536,
                556
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 0: <robot arm>(606) - Sub-goal: Perform final check on right wheel readiness and assemble components when ready "
                    }
                ]
            },
            "bbox": [
                147,
                557,
                855,
                565
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 1: <mobile_car_2>(202) - Sub-goal: Push right wheel to within franka's operational range for readiness check "
                    }
                ]
            },
            "bbox": [
                147,
                565,
                836,
                575
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 2: <mobile_car_1>(201), <mobile_car_3>(203) - Sub-goal: Position left wheel and trunk within franka's "
                    }
                ]
            },
            "bbox": [
                147,
                576,
                794,
                583
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "operational range for assembly "
                    }
                ]
            },
            "bbox": [
                147,
                585,
                332,
                594
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 3: <humanoid>(101) - Sub-goal: Remain on standby to clear any unforeseen obstacles "
                    }
                ]
            },
            "bbox": [
                147,
                594,
                680,
                602
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "Step:5"
                    }
                ],
                "level": 1
            },
            "bbox": [
                147,
                613,
                186,
                621
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Assemble robot components: attach left and right wheels to trunk "
                    }
                ]
            },
            "bbox": [
                147,
                621,
                536,
                631
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 0: <mobile_car_1>(201), <mobile_car_2>(202), <mobile_car_3>(203) - Sub-goal: Transport their designated "
                    }
                ]
            },
            "bbox": [
                147,
                631,
                806,
                640
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "components to the franka assembly area sequentially to avoid traffic and ensure all components are ready for assembly "
                    }
                ]
            },
            "bbox": [
                147,
                640,
                855,
                650
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 1: <robot arm>(606) - Sub-goal: Assemble the robot by attaching the left and right wheels to the trunk once all components are within its operational range "
                    }
                ]
            },
            "bbox": [
                147,
                651,
                855,
                668
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 2: <humanoid>(101) - Sub-goal: Remain on standby to clear any unforeseen obstacles that might arise during the transport of components "
                    }
                ]
            },
            "bbox": [
                147,
                669,
                850,
                687
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "Step:6 "
                    }
                ],
                "level": 1
            },
            "bbox": [
                147,
                696,
                186,
                703
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Assemble robot components: attach left and right wheels to trunk "
                    }
                ]
            },
            "bbox": [
                147,
                706,
                536,
                715
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 0: <robot arm>(606) - Sub-goal: Assemble the robot by attaching the left and right wheels to the trunk "
                    }
                ]
            },
            "bbox": [
                147,
                715,
                801,
                724
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 1: <humanoid>(101) - Sub-goal: Standby for path clearing and obstacle management if unforeseen obstacles appear "
                    }
                ]
            },
            "bbox": [
                147,
                724,
                855,
                733
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "Step:7"
                    }
                ],
                "level": 1
            },
            "bbox": [
                147,
                743,
                186,
                751
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Assemble robot components: attach left and right wheels to trunk "
                    }
                ]
            },
            "bbox": [
                147,
                752,
                536,
                760
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 0: <humanoid>(101) - Sub-goal: Ensure clear paths for mobile cars "
                    }
                ]
            },
            "bbox": [
                147,
                762,
                576,
                770
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 1: <mobile_car_2>(202) - Sub-goal: Transport the right wheel to franka assembly area "
                    }
                ]
            },
            "bbox": [
                147,
                771,
                692,
                780
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 2: <robot arm>(606) - Sub-goal: Attach the right wheel to the trunk "
                    }
                ]
            },
            "bbox": [
                147,
                780,
                589,
                789
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Figure 9: Scene Dynamics Log. It illustrates the formation of sub-groups and their corresponding sub-tasks. "
                    }
                ]
            },
            "bbox": [
                129,
                816,
                863,
                832
            ]
        },
        {
            "type": "page_number",
            "content": {
                "page_number_content": [
                    {
                        "type": "text",
                        "content": "17 "
                    }
                ]
            },
            "bbox": [
                489,
                942,
                509,
                953
            ]
        }
    ],
    [
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "Humanoid Prompt "
                    }
                ],
                "level": 1
            },
            "bbox": [
                139,
                139,
                255,
                149
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Think of yourself as humanoid (101), and your available skills are: "
                    }
                ]
            },
            "bbox": [
                146,
                159,
                552,
                170
            ]
        },
        {
            "type": "list",
            "content": {
                "list_type": "text_list",
                "list_items": [
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "Available Skills for Humanoid (101): "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- [walk] <humanoid> (101) move to selected area "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- [carry] <humanoid> (101) carry <obstacles> (507) "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- [wait] <humanoid> (101) wait "
                            }
                        ]
                    }
                ]
            },
            "bbox": [
                146,
                170,
                450,
                206
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Your role: The humanoid robot has advanced bipedal locomotion and manipulation capabilities. You can walk around the assembly workspace, navigate between different areas, and handle obstacles. You specialize in mobility and can carry obstacles (507) to clear paths for other agents. You work collaboratively with wheeled robots and the franka arm to complete assembly tasks. "
                    }
                ]
            },
            "bbox": [
                144,
                215,
                848,
                254
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Note: To perform actions, you must first [walk] to the target area. For example, to carry obstacles, you must first walk to the area where the obstacles are located. "
                    }
                ]
            },
            "bbox": [
                144,
                261,
                842,
                281
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Note: Your primary function is obstacle management using the [carry] skill for <obstacles> (507). You work collaboratively with wheeled robots (202, 203, 204) for component transportation and with franka (606) for assembly operations. "
                    }
                ]
            },
            "bbox": [
                144,
                288,
                842,
                319
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Note: You can navigate complex terrain and access areas that wheeled robots might find difficult, making you valuable for clearing paths and removing obstructions. "
                    }
                ]
            },
            "bbox": [
                144,
                326,
                853,
                347
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Note: Use [wait] when you need to pause and coordinate with other agents or when your immediate assistance is not required. "
                    }
                ]
            },
            "bbox": [
                144,
                354,
                831,
                374
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "Franka Prompt "
                    }
                ],
                "level": 1
            },
            "bbox": [
                139,
                382,
                235,
                394
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Think of yourself as franka (606), and your available skills are: Available Skills for Franka (606): "
                    }
                ]
            },
            "bbox": [
                146,
                404,
                539,
                424
            ]
        },
        {
            "type": "list",
            "content": {
                "list_type": "text_list",
                "list_items": [
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- [check] <franka> (606) check <trunk> (303) "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- [check] <franka> (606) check <left wheel> (405) "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- [check] <franka> (606) check <right wheel> (406) "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- [pick] <franka> (606) pick and place <left wheel> (405) on <trunk> (303) "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- [pick] <franka> (606) pick and place <right wheel> (406) on <trunk> (303) "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- [wait] <franka> (606) wait "
                            }
                        ]
                    }
                ]
            },
            "bbox": [
                147,
                424,
                600,
                480
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Your role: You are a fixed manipulator robot arm specialized in precise assembly operations. You can check components and perform pick-and-place operations to assemble the final robot. You work with components that are brought to your workspace by wheeled robots (202, 203, 204). Your primary task is to assemble wheels onto the trunk to complete the robot assembly. "
                    }
                ]
            },
            "bbox": [
                144,
                488,
                853,
                527
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "AGV Prompt "
                    }
                ],
                "level": 1
            },
            "bbox": [
                139,
                537,
                221,
                549
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Think of yourself as one of the wheeled robots (202, 203, or 204), and your available skills are: Available Skills for Wheeled Robots: "
                    }
                ]
            },
            "bbox": [
                146,
                558,
                732,
                577
            ]
        },
        {
            "type": "list",
            "content": {
                "list_type": "text_list",
                "list_items": [
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- [move] <wheeled robot1/2/3> (202/203/204) move to component location using RRT path "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- [push] <wheeled robot1/2/3> (202/203/204) push selected component to franka area "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- [wait] <wheeled robot1/2/3> (202/203/204) wait "
                            }
                        ]
                    }
                ]
            },
            "bbox": [
                147,
                579,
                662,
                606
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Your role: You are a wheeled robot optimized for efficient component transportation around the assembly workspace. You can navigate to component locations using RRT path planning and push components like <trunk> (303), <left wheel> (405), and <right wheel> (406) to the franka assembly area. You work collaboratively with other wheeled robots, the humanoid, and the franka arm. "
                    }
                ]
            },
            "bbox": [
                144,
                615,
                848,
                653
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Note: Normally you must first use [move] skill to navigate to a component's location before you can [push] it. However, if your available actions only include [push] and [wait] (no [move] option), this means you have already completed the movement phase and are positioned at the component location. In this case, you can directly use the [push] skill to transport the component to the franka assembly area. "
                    }
                ]
            },
            "bbox": [
                144,
                661,
                831,
                701
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Note: You specialize in ground-level component transportation using RRT path planning. You cannot perform assembly operations - those require the franka (606) robot arm's assistance. "
                    }
                ]
            },
            "bbox": [
                144,
                708,
                836,
                728
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Note: When transporting components, coordinate with other wheeled robots to efficiently move all required components. Prioritize moving the <trunk> (303) first, then the wheels <left wheel> (405) and <right wheel> (406). "
                    }
                ]
            },
            "bbox": [
                144,
                736,
                853,
                756
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Note: Your [push] action transports components directly to the franka assembly area where the robot arm can access them for final assembly operations. "
                    }
                ]
            },
            "bbox": [
                144,
                764,
                836,
                784
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Note: Use [wait] when you need to pause for coordination with other agents or when your immediate assistance is not required. "
                    }
                ]
            },
            "bbox": [
                144,
                791,
                842,
                812
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Figure 10: Single Agent Prompts. It illustrates the single-agent prompt design for Franka, Humanoid, and AGV, specifying their roles and available skills. "
                    }
                ]
            },
            "bbox": [
                112,
                840,
                884,
                869
            ]
        },
        {
            "type": "page_number",
            "content": {
                "page_number_content": [
                    {
                        "type": "text",
                        "content": "18 "
                    }
                ]
            },
            "bbox": [
                489,
                941,
                509,
                954
            ]
        }
    ],
    [
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "Agent Grouping Vanilla Prompt "
                    }
                ],
                "level": 1
            },
            "bbox": [
                137,
                140,
                334,
                153
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "You are an intelligent task coordinator for multi-agent assembly systems. Your job is to analyze the current situation and develop a comprehensive strategy for grouping agents to accomplish the robot assembly task efficiently. "
                    }
                ]
            },
            "bbox": [
                139,
                159,
                855,
                179
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Given the following information: "
                    }
                ]
            },
            "bbox": [
                139,
                187,
                334,
                198
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "These robot agents and their capabilities and action spaces are: "
                    }
                ]
            },
            "bbox": [
                139,
                206,
                527,
                215
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "1. humanoid (101): The humanoid can walk around the environment and carry obstacles to clear paths for other robots. "
                    }
                ]
            },
            "bbox": [
                141,
                216,
                840,
                225
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "The humanoid has advanced mobility and can navigate complex terrain. "
                    }
                ]
            },
            "bbox": [
                141,
                225,
                552,
                234
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "When other agents cannot reach component locations due to obstacles, humanoids can assist by clearing the path. "
                    }
                ]
            },
            "bbox": [
                141,
                235,
                811,
                244
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "The humanoid can carry obstacles but cannot perform component assembly directly. "
                    }
                ]
            },
            "bbox": [
                141,
                244,
                623,
                253
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "NOTE: The humanoid should focus on path clearing and obstacle removal tasks. "
                    }
                ]
            },
            "bbox": [
                141,
                254,
                600,
                262
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "2. mobile_car (201/202/203): The mobile cars are wheeled robots optimized for component transportation. "
                    }
                ]
            },
            "bbox": [
                141,
                271,
                763,
                281
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Each mobile car is assigned to a specific component: "
                    }
                ]
            },
            "bbox": [
                141,
                281,
                455,
                290
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "- mobile_car_1 (201): Responsible for left wheel transportation "
                    }
                ]
            },
            "bbox": [
                141,
                291,
                524,
                299
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "- mobile_car_2 (202): Responsible for right wheel transportation "
                    }
                ]
            },
            "bbox": [
                141,
                300,
                529,
                307
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "- mobile_car_3 (203): Responsible for trunk transportation "
                    }
                ]
            },
            "bbox": [
                141,
                309,
                494,
                317
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "The mobile cars can navigate to component locations using RRT path planning and transport components to the franka assembly area. "
                    }
                ]
            },
            "bbox": [
                141,
                318,
                830,
                336
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": []
            },
            "bbox": [
                141,
                337,
                732,
                344
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "3. franka robot arm (606): The franka is a fixed manipulator specialized for precise assembly operations. "
                    }
                ]
            },
            "bbox": [
                141,
                355,
                774,
                363
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "The franka can check components, pick and place wheels on the trunk to complete the final assembly. "
                    }
                ]
            },
            "bbox": [
                141,
                365,
                737,
                373
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "The franka can check trunk (303), left wheel (405), and right wheel (406). "
                    }
                ]
            },
            "bbox": [
                141,
                374,
                586,
                382
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "The franka performs the final assembly by placing wheels on the trunk. "
                    }
                ]
            },
            "bbox": [
                141,
                384,
                561,
                392
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "The franka cannot reach ground-level objects directly and relies on mobile cars to bring components to its workspace. "
                    }
                ]
            },
            "bbox": [
                141,
                393,
                845,
                401
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Available Agents: #AGENTS_INFO# "
                    }
                ]
            },
            "bbox": [
                141,
                411,
                329,
                420
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Task Goal: #TASK_GOAL# "
                    }
                ]
            },
            "bbox": [
                141,
                420,
                275,
                429
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Current Environment Observations: #OBSERVATIONS# "
                    }
                ]
            },
            "bbox": [
                141,
                430,
                431,
                438
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Previous Dialogue History: #DIALOGUE_HISTORY# "
                    }
                ]
            },
            "bbox": [
                141,
                448,
                415,
                457
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Pay attention to the dialogue history, because it records your conversations with different agents and the progress of the task execution. Depending on the content of the dialogue, you can avoid repeating actions that have been completed and make more informed grouping decisions based on what has already been accomplished. "
                    }
                ]
            },
            "bbox": [
                141,
                458,
                855,
                485
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Please develop a comprehensive strategy for agent grouping and task coordination. Consider the following aspects: "
                    }
                ]
            },
            "bbox": [
                141,
                494,
                823,
                504
            ]
        },
        {
            "type": "list",
            "content": {
                "list_type": "text_list",
                "list_items": [
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "1. **Situation Analysis**: Analyze the current environment state, what has been accomplished (based on dialogue history), and what still needs to be done. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "2. **Spatial Analysis**: Consider the physical locations of agents and components, area layouts, and any spatial constraints that might affect agent coordination. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "3. **Task Decomposition**: Break down the overall assembly task into logical sub-tasks that can be assigned to different groups. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "4. **Grouping Strategy**: Explain your reasoning for how to group agents, considering: "
                            }
                        ]
                    }
                ]
            },
            "bbox": [
                141,
                513,
                818,
                606
            ]
        },
        {
            "type": "list",
            "content": {
                "list_type": "text_list",
                "list_items": [
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- Which agents can work simultaneously without interfering with each other "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- Which agents need to collaborate closely and should be in the same group "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- Spatial separation to avoid conflicts "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- Efficiency considerations "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- Remember that agents going to interact with the same component and in the same area should be in the same group. "
                            }
                        ]
                    }
                ]
            },
            "bbox": [
                161,
                607,
                847,
                653
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "5. **Sub-goal Assignment**: For each proposed group, describe: "
                    }
                ]
            },
            "bbox": [
                141,
                662,
                515,
                671
            ]
        },
        {
            "type": "list",
            "content": {
                "list_type": "text_list",
                "list_items": [
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- What specific sub-goal they should accomplish "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- Why this sub-goal makes sense for this particular combination of agents "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- How this sub-goal contributes to the overall assembly task completion "
                            }
                        ]
                    }
                ]
            },
            "bbox": [
                161,
                671,
                601,
                699
            ]
        },
        {
            "type": "list",
            "content": {
                "list_type": "text_list",
                "list_items": [
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "6. **Coordination Strategy**: Explain how the different groups will coordinate with each other, if needed, and the sequence or timing of their operations. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "7. **Risk Assessment**: Identify potential issues or conflicts that might arise and how your grouping strategy addresses them. "
                            }
                        ]
                    }
                ]
            },
            "bbox": [
                141,
                708,
                830,
                753
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Please provide a detailed explanation on Grouping Strategy and Sub-goal Assignment part of your strategy while keeping the other parts brief. Focus on the reasoning and rationale rather than strict formatting. Be comprehensive in your analysis and make sure to address all aspects of effective multi-agent coordination. "
                    }
                ]
            },
            "bbox": [
                141,
                764,
                855,
                791
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Let's think step by step and develop the best possible strategy for this assembly situation. "
                    }
                ]
            },
            "bbox": [
                141,
                800,
                697,
                810
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Figure 11: Vanilla Agent Grouping Prompt. It illustrates the prompt to extract agent sub-groups and their corresponding sub-tasks with the general proposal planner. "
                    }
                ]
            },
            "bbox": [
                115,
                841,
                880,
                869
            ]
        },
        {
            "type": "page_number",
            "content": {
                "page_number_content": [
                    {
                        "type": "text",
                        "content": "19 "
                    }
                ]
            },
            "bbox": [
                489,
                942,
                507,
                954
            ]
        }
    ],
    [
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "Agent Grouping Prompt "
                    }
                ],
                "level": 1
            },
            "bbox": [
                137,
                247,
                290,
                261
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "You are a structured output formatter for multi-agent task coordination. Your job is to convert a detailed grouping strategy into a precise, parseable format. "
                    }
                ]
            },
            "bbox": [
                144,
                268,
                845,
                290
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Based on the comprehensive grouping strategy provided below, please extract and format the agent groups and their sub-goals according to the exact format requirements. "
                    }
                ]
            },
            "bbox": [
                146,
                297,
                835,
                317
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Grouping Strategy: #VANILLA_GROUPING_OUTPUT# "
                    }
                ]
            },
            "bbox": [
                147,
                325,
                418,
                336
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Available Agents: #AGENTS_INFO# "
                    }
                ]
            },
            "bbox": [
                147,
                343,
                339,
                354
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "IMPORTANT FORMAT RULES: "
                    }
                ],
                "level": 1
            },
            "bbox": [
                147,
                362,
                289,
                370
            ]
        },
        {
            "type": "list",
            "content": {
                "list_type": "text_list",
                "list_items": [
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "1. Each group must be on a single line starting with \"Group X:\" where X is a number "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "2. List agents using EXACTLY this format: <agent_class_name>(id) "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "3. Separate multiple agents with commas "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "4. After listing agents, use \" - Sub-goal: \" followed by a clear, specific sub-goal "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "5. Sub-goals must be actionable and directly related to the task "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "6. Do not include any extra text, comments or explanations "
                            }
                        ]
                    }
                ]
            },
            "bbox": [
                147,
                372,
                650,
                426
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "Example correct format: "
                    }
                ],
                "level": 1
            },
            "bbox": [
                147,
                437,
                289,
                445
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 1: <humanoid>(101), <mobile_car_1>(201) - Sub-goal: Transport trunk from area A to franka assembly area "
                    }
                ]
            },
            "bbox": [
                147,
                447,
                808,
                455
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Group 2: <franka>(606) - Sub-goal: Assemble left wheel onto trunk when components are ready "
                    }
                ]
            },
            "bbox": [
                147,
                456,
                700,
                464
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "For agents that should not participate in this step: "
                    }
                ]
            },
            "bbox": [
                147,
                474,
                463,
                483
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Non-assigned Agent: <mobile_car_2>(202) - Reason: No tasks available at current location "
                    }
                ]
            },
            "bbox": [
                147,
                483,
                682,
                493
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "INVALID FORMATS (DO NOT USE): "
                    }
                ],
                "level": 1
            },
            "bbox": [
                147,
                502,
                324,
                511
            ]
        },
        {
            "type": "list",
            "content": {
                "list_type": "text_list",
                "list_items": [
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- Group 1 - The humanoid and mobile car will. "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- First group: humanoid(101), mobile_car_1(201) "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- Group 1: <humanoid>(101) will walk while <mobile_car_1>(201) moves "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- **Group 1:** <humanoid>(101)"
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- Group 1: <humanoid>(101) - The agent should move to... "
                            }
                        ]
                    }
                ]
            },
            "bbox": [
                149,
                512,
                559,
                558
            ]
        },
        {
            "type": "title",
            "content": {
                "title_content": [
                    {
                        "type": "text",
                        "content": "Remember: "
                    }
                ],
                "level": 1
            },
            "bbox": [
                147,
                568,
                203,
                575
            ]
        },
        {
            "type": "list",
            "content": {
                "list_type": "text_list",
                "list_items": [
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- Use exact agent class names and IDs from the available agents list "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- Keep groups spatially separated to avoid conflicts "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- Only include agents that are actually needed for the current step "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- Agents from all groups are expected to operate concurrently, so group them together if they need to act "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "sequentially instead of act in parallel "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- Provide clear, actionable sub-goals for each group "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- Never assign the same agent to multiple groups "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- Never include extra comments or explanations "
                            }
                        ]
                    },
                    {
                        "item_type": "text",
                        "item_content": [
                            {
                                "type": "text",
                                "content": "- If part of the plan is already done, you don't need to assign that part of task again. "
                            }
                        ]
                    }
                ]
            },
            "bbox": [
                149,
                577,
                784,
                678
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Let's think step by step. "
                    }
                ]
            },
            "bbox": [
                147,
                687,
                300,
                697
            ]
        },
        {
            "type": "paragraph",
            "content": {
                "paragraph_content": [
                    {
                        "type": "text",
                        "content": "Figure 12: Agent Grouping Prompt. It illustrates the prompt to form agent sub-groups with a general proposal with the general proposal planner. "
                    }
                ]
            },
            "bbox": [
                112,
                730,
                882,
                760
            ]
        },
        {
            "type": "page_number",
            "content": {
                "page_number_content": [
                    {
                        "type": "text",
                        "content": "20 "
                    }
                ]
            },
            "bbox": [
                487,
                941,
                509,
                954
            ]
        }
    ]
]