"""
æ–‡çŒ®æ£€ç´¢æµæ°´çº¿ - è‡ªåŠ¨ç”Ÿæˆæœç´¢æŸ¥è¯¢å¹¶è·å–ç›¸å…³è®ºæ–‡

å·¥ä½œæµç¨‹ï¼š
1. è¯»å– mineru_pipeline ç”Ÿæˆçš„ full.md æ–‡ä»¶
2. æå–è®ºæ–‡å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ã€æ‘˜è¦ã€å…³é”®å†…å®¹ï¼‰
3. ä½¿ç”¨ DeepSeek AI ç”Ÿæˆ 5-10 æ¡è‹±æ–‡æ£€ç´¢å¥
4. åœ¨ arXiv æœç´¢ 2020 å¹´åçš„ç›¸å…³è®ºæ–‡
5. æ‰¹é‡ä¿å­˜å…ƒæ•°æ®åˆ°æœ¬åœ°
"""

import json
import sys
import time
from pathlib import Path
from datetime import datetime
from literature_review.logger import get_logger

logger = get_logger("literature_search_pipeline")

# å¯¼å…¥å·²æœ‰çš„æ¨¡å—
sys.path.append(str(Path(__file__).parent.parent))
from literature_review.metadata_extractor import MetadataExtractor
from literature_review.query_generator import QueryGenerator
from literature_review.arxiv_searcher import ArxivSearcher


def load_markdown_from_mineru(paper_id: str, base_dir: Path = None) -> str:
    """
    ä» mineru_pipeline è¾“å‡ºç›®å½•åŠ è½½ full.md
    
    Args:
        paper_id: è®ºæ–‡IDï¼ˆä¾‹å¦‚ '2401.12345'ï¼‰
        base_dir: mineru_pipeline è¾“å‡ºæ ¹ç›®å½•ï¼Œé»˜è®¤ä¸º 'pipeline/outputs'
    
    Returns:
        Markdown æ–‡æœ¬å†…å®¹
    """
    if base_dir is None:
        base_dir = Path(__file__).parent / "outputs"
    
    md_path = base_dir / paper_id / "full.md"
    
    if not md_path.exists():
        raise FileNotFoundError(
            f"æœªæ‰¾åˆ° Markdown æ–‡ä»¶: {md_path}\n"
            f"è¯·å…ˆè¿è¡Œ pipeline/mineru_pipeline.py ç”Ÿæˆ MD æ–‡ä»¶"
        )
    
    with open(md_path, 'r', encoding='utf-8') as f:
        return f.read()


def run_literature_search(
    paper_id: str,
    num_queries: int = 7,
    since_year: int = 2020,
    max_results_per_query: int = 20,
    output_dir: str = None
):
    """
    è¿è¡Œå®Œæ•´çš„æ–‡çŒ®æ£€ç´¢æµæ°´çº¿
    
    Args:
        paper_id: è®ºæ–‡ID
        num_queries: ç”Ÿæˆçš„æŸ¥è¯¢æ•°é‡
        since_year: èµ·å§‹å¹´ä»½
        max_results_per_query: æ¯ä¸ªæŸ¥è¯¢çš„æœ€å¤§ç»“æœæ•°
        output_dir: è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä¸º pipeline/literature_search_resultsï¼‰
    """
    # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºç›®å½•ï¼Œä½¿ç”¨ pipeline ä¸‹çš„é»˜è®¤ç›®å½•
    if output_dir is None:
        output_dir = str(Path(__file__).parent / "literature_search_results")
    logger.info("â•" * 70)
    logger.info("ğŸ“š æ–‡çŒ®æ£€ç´¢è‡ªåŠ¨åŒ–æµæ°´çº¿")
    logger.info("â•" * 70)
    logger.info(f"è®ºæ–‡ ID: {paper_id} | æŸ¥è¯¢æ•°é‡: {num_queries} | èµ·å§‹å¹´ä»½: {since_year} | è¾“å‡ºç›®å½•: {output_dir}")
    
    output_path = Path(output_dir) / paper_id
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ===== ç¬¬ 1 æ­¥: åŠ è½½ Markdown =====
    logger.info("â•" * 70)
    logger.info("ç¬¬ 1 æ­¥: åŠ è½½ Markdown æ–‡ä»¶")
    logger.info("â•" * 70)
    
    try:
        markdown_text = load_markdown_from_mineru(paper_id)
        logger.info(f"âœ… æˆåŠŸåŠ è½½ Markdown ({len(markdown_text)} å­—ç¬¦)")
        
        # ä¿å­˜å‰¯æœ¬
        md_copy_path = output_path / f"{paper_id}_source.md"
        try:
            md_copy_path.write_text(markdown_text, encoding='utf-8')
            logger.info(f"   å·²ä¿å­˜å‰¯æœ¬: {md_copy_path}")
        except Exception as e:
            logger.error(f"é”™è¯¯: æ— æ³•ä¿å­˜ Markdown å‰¯æœ¬åˆ° {md_copy_path}: {e}")
            return # If we can't save the source, something is wrong, exit.
    except FileNotFoundError as e:
        logger.error(f"âŒ é”™è¯¯: {e}")
        return
    

    
    # ===== ç¬¬ 2 æ­¥: æå–å…ƒæ•°æ® =====
    logger.info("â•" * 70)
    logger.info("ç¬¬ 2 æ­¥: æå–è®ºæ–‡å…ƒæ•°æ®")
    logger.info("â•" * 70)
    
    extractor = MetadataExtractor()
    
    # ä¼˜å…ˆä» MinerU çš„ç»“æ„åŒ– JSON æå–æ ‡é¢˜å’Œä½œè€…ï¼ˆæ›´å‡†ç¡®ï¼‰
    content_list_path = Path(__file__).parent / "outputs" / paper_id / "meta" / "content_list_v2.json"
    content_list_meta = extractor.extract_from_content_list(str(content_list_path))
    
    # ä» Markdown æå–å®Œæ•´å…ƒæ•°æ®ï¼ˆæ‘˜è¦ã€ç« èŠ‚ç­‰ï¼‰
    metadata = extractor.extract_metadata(markdown_text)
    
    # ç”¨ content_list çš„ç»“æœè¦†ç›– regex æå–çš„ title/authorsï¼ˆå¦‚æœæœ‰ï¼‰
    if content_list_meta.get('title'):
        metadata['title'] = content_list_meta['title']
        logger.info("æ ‡é¢˜æ¥æº: content_list_v2.json (ç»“æ„åŒ–)")
    if content_list_meta.get('authors'):
        metadata['authors'] = content_list_meta['authors']
        logger.info("ä½œè€…æ¥æº: content_list_v2.json (ç»“æ„åŒ–)")
    
    logger.info(f"æ ‡é¢˜: {metadata['title']}")
    authors_str = ', '.join(metadata['authors'][:3]) if metadata['authors'] else 'æœªæ‰¾åˆ°'
    logger.info(f"ä½œè€…: {authors_str}...")
    logger.info(f"ç« èŠ‚æ•°: {len(metadata['sections'])}")
    abstract_len = len(metadata.get('abstract', ''))
    logger.info(f"æ‘˜è¦é•¿åº¦: {abstract_len} å­—ç¬¦")
    
    # ä¿å­˜å…ƒæ•°æ®
    metadata_path = output_path / "metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    logger.info(f"   å·²ä¿å­˜: {metadata_path}")
    

    
    # ===== ç¬¬ 3 æ­¥: ç”Ÿæˆæœç´¢æŸ¥è¯¢ =====
    logger.info("â•" * 70)
    logger.info("ç¬¬ 3 æ­¥: ä½¿ç”¨ DeepSeek AI ç”Ÿæˆæœç´¢æŸ¥è¯¢")
    logger.info("â•" * 70)
    
    generator = QueryGenerator()
    queries = generator.generate_queries(
        markdown_text=markdown_text,
        metadata=metadata,
        num_queries=num_queries
    )
    
    logger.info(f"âœ… ç”Ÿæˆäº† {len(queries)} æ¡æŸ¥è¯¢:")
    for i, query in enumerate(queries, 1):
        logger.info(f"  {i}. {query}")
    
    # ä¿å­˜æŸ¥è¯¢
    queries_path = output_path / "search_queries.json"
    with open(queries_path, 'w', encoding='utf-8') as f:
        json.dump(queries, f, ensure_ascii=False, indent=2)
    logger.info(f"å·²ä¿å­˜: {queries_path}")
    

    
    # ===== ç¬¬ 4 æ­¥: æœç´¢ arXiv =====
    logger.info("â•" * 70)
    logger.info("ç¬¬ 4 æ­¥: æœç´¢ arXiv (2020å¹´å)")
    logger.info("â•" * 70)
    
    searcher = ArxivSearcher()
    
    all_results = []
    seen_ids = set()
    
    for i, query in enumerate(queries, 1):
        logger.info(f"æœç´¢ {i}/{len(queries)}: {query[:60]}...")
        
        results = searcher.search(
            query=query,
            max_results=max_results_per_query,
            since_year=since_year
        )
        
        # å»é‡
        new_results = []
        for result in results:
            if result['arxiv_id'] not in seen_ids:
                seen_ids.add(result['arxiv_id'])
                new_results.append(result)
        
        logger.info(f"æ‰¾åˆ° {len(results)} ç¯‡è®ºæ–‡ï¼Œå»é‡å {len(new_results)} ç¯‡")
        all_results.extend(new_results)
        
        # æŸ¥è¯¢é—´å»¶è¿Ÿï¼Œé¿å…è§¦å‘ arXiv é€Ÿç‡é™åˆ¶
        if i < len(queries):
            time.sleep(searcher.delay)
    
    logger.info(f"âœ… æ€»è®¡æ‰¾åˆ° {len(all_results)} ç¯‡ç›¸å…³è®ºæ–‡ï¼ˆå»é‡åï¼‰")
    
    # ===== ç¬¬ 5 æ­¥: ä¿å­˜ç»“æœ =====
    logger.info("â•" * 70)
    logger.info("ç¬¬ 5 æ­¥: ä¿å­˜ç»“æœ")
    logger.info("â•" * 70)
    
    # ä¿å­˜ JSON
    results_json_path = output_path / "arxiv_results.json"
    with open(results_json_path, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    logger.info(f"âœ… JSON æ ¼å¼: {results_json_path}")
    
    # ä¿å­˜ CSV
    results_csv_path = output_path / "arxiv_results.csv"
    searcher.save_to_csv(all_results, results_csv_path)
    logger.info(f"âœ… CSV æ ¼å¼: {results_csv_path}")
    
    # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
    summary_path = output_path / "search_summary.txt"
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("=" * 70 + "\n")
        f.write("æ–‡çŒ®æ£€ç´¢æ‘˜è¦æŠ¥å‘Š\n")
        f.write("=" * 70 + "\n\n")
        f.write(f"è®ºæ–‡ ID: {paper_id}\n")
        f.write(f"æ£€ç´¢æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æŸ¥è¯¢æ•°é‡: {len(queries)}\n")
        f.write(f"èµ·å§‹å¹´ä»½: {since_year}\n")
        f.write(f"æ‰¾åˆ°è®ºæ–‡: {len(all_results)} ç¯‡\n\n")
        
        f.write("=" * 70 + "\n")
        f.write("æœç´¢æŸ¥è¯¢åˆ—è¡¨\n")
        f.write("=" * 70 + "\n\n")
        for i, query in enumerate(queries, 1):
            f.write(f"{i}. {query}\n")
        
        f.write("\n" + "=" * 70 + "\n")
        f.write("å‰ 10 ç¯‡ç›¸å…³è®ºæ–‡\n")
        f.write("=" * 70 + "\n\n")
        for i, paper in enumerate(all_results[:10], 1):
            f.write(f"{i}. {paper['title']}\n")
            f.write(f"   ä½œè€…: {', '.join(paper['authors'][:3])}\n")
            f.write(f"   å¹´ä»½: {paper['published'][:4]}\n")
            f.write(f"   é“¾æ¥: {paper['pdf_url']}\n\n")
    
    logger.info(f"âœ… æ‘˜è¦æŠ¥å‘Š: {summary_path}")
    
    logger.info("â•" * 70)
    logger.info(f"âœ… å®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_path.resolve()}")
    logger.info("â•" * 70)


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='æ–‡çŒ®æ£€ç´¢è‡ªåŠ¨åŒ–æµæ°´çº¿ - ä» mineru_pipeline è¾“å‡ºå¼€å§‹'
    )
    
    parser.add_argument(
        'paper_id',
        help='è®ºæ–‡IDï¼ˆä¾‹å¦‚ï¼š2401.12345ï¼‰'
    )
    
    parser.add_argument(
        '-n', '--num-queries',
        type=int,
        default=7,
        help='ç”Ÿæˆçš„æŸ¥è¯¢æ•°é‡ï¼ˆé»˜è®¤: 7ï¼‰'
    )
    
    parser.add_argument(
        '-y', '--since-year',
        type=int,
        default=2020,
        help='èµ·å§‹å¹´ä»½ï¼ˆé»˜è®¤: 2020ï¼‰'
    )
    
    parser.add_argument(
        '-r', '--max-results',
        type=int,
        default=20,
        help='æ¯ä¸ªæŸ¥è¯¢çš„æœ€å¤§ç»“æœæ•°ï¼ˆé»˜è®¤: 20ï¼‰'
    )
    
    parser.add_argument(
        '-o', '--output-dir',
        default=None,
        help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: pipeline/literature_search_resultsï¼‰'
    )
    
    args = parser.parse_args()
    
    run_literature_search(
        paper_id=args.paper_id,
        num_queries=args.num_queries,
        since_year=args.since_year,
        max_results_per_query=args.max_results,
        output_dir=args.output_dir
    )


if __name__ == '__main__':
    main()
