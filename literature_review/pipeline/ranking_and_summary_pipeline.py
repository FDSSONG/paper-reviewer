"""
ç›¸å…³åº¦æ‰“åˆ†å’Œæ‘˜è¦ç”Ÿæˆæµæ°´çº¿

å®Œæ•´æµç¨‹ï¼š
1. è¯»å–æ–‡çŒ®æ£€ç´¢ç»“æœ
2. è®¡ç®—ç›¸å…³åº¦åˆ†æ•°
3. ç­›é€‰ top-k é«˜ç›¸å…³åº¦è®ºæ–‡
4. å¯¹é«˜ç›¸å…³åº¦è®ºæ–‡ï¼šä¸‹è½½ PDF â†’ è½¬ MD â†’ ç”Ÿæˆè¯¦ç»†æ‘˜è¦
5. å¯¹ä½ç›¸å…³åº¦è®ºæ–‡ï¼šä½¿ç”¨åŸæ‘˜è¦
6. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
"""

import json
import sys
from pathlib import Path
from datetime import datetime
from literature_review.logger import get_logger

logger = get_logger("ranking_and_summary_pipeline")

# å¯¼å…¥å·²æœ‰çš„æ¨¡å—
sys.path.append(str(Path(__file__).parent.parent))
from literature_review.relevance_scorer import RelevanceScorer
from literature_review.pdf_processor import PDFProcessor
from literature_review.summary_generator import SummaryGenerator


def run_ranking_and_summary(
    paper_id: str,
    top_k: int = 15,
    language: str = "chinese",
    input_dir: str = None
):
    """
    è¿è¡Œç›¸å…³åº¦æ‰“åˆ†å’Œæ‘˜è¦ç”Ÿæˆæµæ°´çº¿
    
    Args:
        paper_id: è®ºæ–‡ID
        top_k: é«˜ç›¸å…³åº¦è®ºæ–‡æ•°é‡
        language: æ‘˜è¦è¯­è¨€ ('chinese' æˆ– 'english')
        input_dir: è¾“å…¥ç›®å½•ï¼ˆé»˜è®¤ä¸º pipeline/literature_search_resultsï¼‰
    """
    logger.info("â•" * 70)
    logger.info("ğŸ“Š ç›¸å…³åº¦æ‰“åˆ†å’Œæ‘˜è¦ç”Ÿæˆæµæ°´çº¿")
    logger.info("â•" * 70)
    logger.info(f"è®ºæ–‡ ID: {paper_id} | Top-K: {top_k} | æ‘˜è¦è¯­è¨€: {language}")
    
    # è®¾ç½®è·¯å¾„
    if input_dir is None:
        input_dir = Path(__file__).parent / "literature_search_results"
    else:
        input_dir = Path(input_dir)
    
    paper_dir = input_dir / paper_id
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not paper_dir.exists():
        logger.error(f"æ‰¾ä¸åˆ°è®ºæ–‡ç›®å½• {paper_dir}")
        logger.error("è¯·å…ˆè¿è¡Œ literature_search_pipeline.py")
        return
    
    # ===== ç¬¬ 1 æ­¥: åŠ è½½æ•°æ® =====
    logger.info("â•" * 70)
    logger.info("ç¬¬ 1 æ­¥: åŠ è½½æ•°æ®")
    logger.info("â•" * 70)
    
    # åŠ è½½æºè®ºæ–‡å…ƒæ•°æ®
    metadata_path = paper_dir / "metadata.json"
    with open(metadata_path, 'r', encoding='utf-8') as f:
        source_metadata = json.load(f)
    
    logger.info(f"æºè®ºæ–‡: {source_metadata['title']}")
    
    # åŠ è½½å€™é€‰è®ºæ–‡åˆ—è¡¨
    results_path = paper_dir / "arxiv_results.json"
    with open(results_path, 'r', encoding='utf-8') as f:
        candidate_papers = json.load(f)
    
    logger.info(f"å€™é€‰è®ºæ–‡æ•°: {len(candidate_papers)}")
    
    # ===== ç¬¬ 2 æ­¥: è®¡ç®—ç›¸å…³åº¦ =====
    logger.info("â•" * 70)
    logger.info("ç¬¬ 2 æ­¥: è®¡ç®—ç›¸å…³åº¦åˆ†æ•°")
    logger.info("â•" * 70)
    
    scorer = RelevanceScorer()
    scored_papers = scorer.score_papers(source_metadata, candidate_papers)
    
    # ä¿å­˜è¯„åˆ†ç»“æœ
    scores_path = paper_dir / "relevance_scores.json"
    scorer.save_scores(scored_papers, scores_path)

    
    # ===== ç¬¬ 3 æ­¥: ç­›é€‰è®ºæ–‡ =====
    logger.info("â•" * 70)
    logger.info("ç¬¬ 3 æ­¥: ç­›é€‰é«˜ç›¸å…³åº¦è®ºæ–‡")
    logger.info("â•" * 70)
    
    high_rel, low_rel = scorer.select_top_k(scored_papers, k=top_k)
    
    # ä¿å­˜æ’åºåçš„è®ºæ–‡åˆ—è¡¨
    ranked_papers = {
        'high_relevance': [
            {
                'arxiv_id': p.get('arxiv_id', p.get('id')),
                'title': p['title'],
                'score': float(score),
                'authors': p.get('authors', [])[:3],
                'published': p.get('published', '')
            }
            for p, score in high_rel
        ],
        'low_relevance': [
            {
                'arxiv_id': p.get('arxiv_id', p.get('id')),
                'title': p['title'],
                'score': float(score)
            }
            for p, score in low_rel
        ]
    }
    
    ranked_path = paper_dir / "ranked_papers.json"
    with open(ranked_path, 'w', encoding='utf-8') as f:
        json.dump(ranked_papers, f, ensure_ascii=False, indent=2)
    
    logger.info(f"âœ… æ’åºç»“æœå·²ä¿å­˜: {ranked_path}")
    
    # ===== ç¬¬ 4 æ­¥: å¤„ç†é«˜ç›¸å…³åº¦è®ºæ–‡ =====
    logger.info("â•" * 70)
    logger.info("ç¬¬ 4 æ­¥: å¤„ç†é«˜ç›¸å…³åº¦è®ºæ–‡")
    logger.info("â•" * 70)
    
    high_rel_dir = paper_dir / "high_relevance"
    high_rel_dir.mkdir(exist_ok=True)
    
    processor = PDFProcessor()
    generator = SummaryGenerator()
    
    high_rel_summaries = []
    
    for i, (paper, score) in enumerate(high_rel, 1):
        arxiv_id = paper.get('arxiv_id', paper.get('id'))
        logger.info(f"[{i}/{len(high_rel)}] å¤„ç†: {arxiv_id}")
        logger.info(f"  æ ‡é¢˜: {paper['title'][:60]}...")
        logger.info(f"  ç›¸å…³åº¦: {score:.3f}")
        
        paper_output_dir = high_rel_dir / arxiv_id
        
        # 1) ä¸‹è½½å¹¶è½¬æ¢ PDF
        pdf_path = processor.download_pdf(arxiv_id, paper_output_dir)
        
        if pdf_path:
            md_path = processor.convert_pdf_to_markdown(pdf_path, paper_output_dir)
        else:
            md_path = None
        
        # 2) ç”Ÿæˆæ‘˜è¦
        if md_path and md_path.exists():
            # è¯»å– Markdown
            with open(md_path, 'r', encoding='utf-8') as f:
                candidate_md = f.read()
            
            # ç”Ÿæˆè¯¦ç»†æ‘˜è¦
            summary = generator.generate_detailed_summary(
                source_metadata,
                paper,
                candidate_md,
                language=language
            )
        else:
            logger.warning("PDF è½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨åŸæ‘˜è¦")
            summary = generator.use_original_abstract(paper)
        
        # 3) ä¿å­˜æ‘˜è¦
        summary_path = paper_output_dir / "detailed_summary.md"
        generator.save_summary(summary, summary_path, metadata=paper)
        
        high_rel_summaries.append({
            'arxiv_id': arxiv_id,
            'title': paper['title'],
            'score': score,
            'summary': summary,
            'has_full_md': md_path is not None
        })
    

    
    # ===== ç¬¬ 5 æ­¥: å¤„ç†ä½ç›¸å…³åº¦è®ºæ–‡ =====
    logger.info("â•" * 70)
    logger.info("ç¬¬ 5 æ­¥: å¤„ç†ä½ç›¸å…³åº¦è®ºæ–‡ï¼ˆä½¿ç”¨åŸæ‘˜è¦ï¼‰")
    logger.info("â•" * 70)
    
    low_rel_summaries = []
    for paper, score in low_rel:
        arxiv_id = paper.get('arxiv_id', paper.get('id'))
        summary = generator.use_original_abstract(paper)
        
        low_rel_summaries.append({
            'arxiv_id': arxiv_id,
            'title': paper['title'],
            'score': score,
            'summary': summary
        })
    
    logger.info(f"âœ… å·²å¤„ç† {len(low_rel_summaries)} ç¯‡ä½ç›¸å…³åº¦è®ºæ–‡")
    
    # ä¿å­˜ä½ç›¸å…³åº¦æ‘˜è¦åˆ° JSONï¼Œä¾›è¯„å®¡æŠ¥å‘Šä½¿ç”¨
    low_rel_json_path = paper_dir / "low_relevance_summaries.json"
    with open(low_rel_json_path, 'w', encoding='utf-8') as f:
        json.dump(low_rel_summaries, f, ensure_ascii=False, indent=2, default=str)
    logger.info(f"âœ… ä½ç›¸å…³åº¦æ‘˜è¦å·²ä¿å­˜: {low_rel_json_path}")
    
    # ===== ç¬¬ 6 æ­¥: ç”Ÿæˆç»¼åˆæŠ¥å‘Š =====
    logger.info("â•" * 70)
    logger.info("ç¬¬ 6 æ­¥: ç”Ÿæˆç»¼åˆæŠ¥å‘Š")
    logger.info("â•" * 70)
    
    report_path = paper_dir / "summary_report.md"
    generate_summary_report(
        source_metadata,
        high_rel_summaries,
        low_rel_summaries,
        report_path
    )
    
    logger.info(f"âœ… ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    logger.info("â•" * 70)
    logger.info(f"âœ… å®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {paper_dir.resolve()}")
    logger.info("â•" * 70)


def generate_summary_report(
    source_metadata: dict,
    high_rel_summaries: list,
    low_rel_summaries: list,
    output_path: Path
):
    """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
    
    content = f"""# æ–‡çŒ®ç»¼è¿°ç»¼åˆæŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æºè®ºæ–‡ä¿¡æ¯

- **æ ‡é¢˜**: {source_metadata['title']}
- **ä½œè€…**: {', '.join(source_metadata.get('authors', [])[:5])}
- **æ‘˜è¦**: {source_metadata.get('abstract', 'æ— æ‘˜è¦')[:200]}...

---

## é«˜ç›¸å…³åº¦æ–‡çŒ® ({len(high_rel_summaries)} ç¯‡)

"""
    
    for i, item in enumerate(high_rel_summaries, 1):
        content += f"""### {i}. {item['title']}

**ç›¸å…³åº¦**: {item['score']:.3f} | **arXiv ID**: {item['arxiv_id']} | **å®Œæ•´MD**: {'âœ…' if item['has_full_md'] else 'âŒ'}

{item['summary']}

---

"""
    
    content += f"""## ä½ç›¸å…³åº¦æ–‡çŒ® ({len(low_rel_summaries)} ç¯‡)

<details>
<summary>ç‚¹å‡»å±•å¼€æŸ¥çœ‹</summary>

"""
    
    for i, item in enumerate(low_rel_summaries, 1):
        content += f"""### {i}. {item['title']}

**ç›¸å…³åº¦**: {item['score']:.3f} | **arXiv ID**: {item['arxiv_id']}

{item['summary'][:300]}...

---

"""
    
    content += """
</details>
"""
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(content)


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='ç›¸å…³åº¦æ‰“åˆ†å’Œæ‘˜è¦ç”Ÿæˆæµæ°´çº¿'
    )
    
    parser.add_argument(
        'paper_id',
        help='è®ºæ–‡IDï¼ˆä¾‹å¦‚ï¼š2401.12345ï¼‰'
    )
    
    parser.add_argument(
        '-k', '--top-k',
        type=int,
        default=15,
        help='é«˜ç›¸å…³åº¦è®ºæ–‡æ•°é‡ï¼ˆé»˜è®¤: 15ï¼‰'
    )
    
    parser.add_argument(
        '-l', '--language',
        choices=['chinese', 'english'],
        default='chinese',
        help='æ‘˜è¦è¯­è¨€ï¼ˆé»˜è®¤: chineseï¼‰'
    )
    
    parser.add_argument(
        '-i', '--input-dir',
        default=None,
        help='è¾“å…¥ç›®å½•ï¼ˆé»˜è®¤: pipeline/literature_search_resultsï¼‰'
    )
    
    args = parser.parse_args()
    
    run_ranking_and_summary(
        paper_id=args.paper_id,
        top_k=args.top_k,
        language=args.language,
        input_dir=args.input_dir
    )


if __name__ == '__main__':
    main()
