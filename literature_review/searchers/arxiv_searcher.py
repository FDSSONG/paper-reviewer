#!/usr/bin/env python3
"""
ArXiv æœç´¢æ¨¡å— - é‡æ„ç‰ˆæœ¬
ç»§æ‰¿è‡ª BaseSearcherï¼Œä½¿ç”¨ arXiv API æœç´¢ç›¸å…³è®ºæ–‡
"""
import time
import urllib.parse
import urllib.request
import xml.etree.ElementTree as ET
from typing import List, Dict, Any

from literature_review.searchers.base_searcher import BaseSearcher


class ArxivSearcher(BaseSearcher):
    """ArXiv æœç´¢å™¨ç±»ï¼ˆç»§æ‰¿è‡ª BaseSearcherï¼‰"""

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ– arXiv æœç´¢å™¨

        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å« max_retries, delay, timeout ç­‰å‚æ•°
        """
        super().__init__(config)
        self.logger.info("ArXiv æœç´¢å™¨åˆå§‹åŒ–å®Œæˆ")

    def search(
        self,
        query: str,
        max_results: int = 20,
        since_year: int = 2020
    ) -> List[Dict[str, Any]]:
        """
        åœ¨ arXiv ä¸Šæœç´¢è®ºæ–‡ï¼ˆå¸¦æŒ‡æ•°é€€é¿é‡è¯•ï¼‰

        Args:
            query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
            max_results: æœ€å¤§è¿”å›ç»“æœæ•°
            since_year: èµ·å§‹å¹´ä»½ï¼ˆé»˜è®¤2020ï¼‰

        Returns:
            æ ‡å‡†åŒ–çš„è®ºæ–‡åˆ—è¡¨
        """
        # æ„å»ºæœç´¢ URL
        base_url = "http://export.arxiv.org/api/query?"

        # arXiv API å‚æ•°
        params = {
            'search_query': f'all:{query}',
            'start': 0,
            'max_results': max_results,
            'sortBy': 'relevance',
            'sortOrder': 'descending'
        }

        url = base_url + urllib.parse.urlencode(params)

        last_error = None
        for attempt in range(self.max_retries + 1):
            try:
                # å‘é€è¯·æ±‚
                with urllib.request.urlopen(url, timeout=self.timeout) as response:
                    xml_data = response.read()

                # è§£æ XML
                root = ET.fromstring(xml_data)

                # å‘½åç©ºé—´
                ns = {
                    'atom': 'http://www.w3.org/2005/Atom',
                    'arxiv': 'http://arxiv.org/schemas/atom'
                }

                papers = []

                for entry in root.findall('atom:entry', ns):
                    # æå–åŸºæœ¬ä¿¡æ¯
                    title = entry.find('atom:title', ns).text.strip().replace('\n', ' ')

                    # æå–ä½œè€…
                    authors = []
                    for author in entry.findall('atom:author', ns):
                        name = author.find('atom:name', ns).text
                        authors.append(name)

                    # æ‘˜è¦
                    abstract = entry.find('atom:summary', ns).text.strip().replace('\n', ' ')

                    # å‘å¸ƒå’Œæ›´æ–°æ—¥æœŸ
                    published = entry.find('atom:published', ns).text
                    updated = entry.find('atom:updated', ns).text

                    # æå–å¹´ä»½è¿›è¡Œè¿‡æ»¤
                    pub_year = int(published[:4])
                    if pub_year < since_year:
                        continue

                    # IDï¼ˆä»é“¾æ¥ä¸­æå–ï¼‰
                    arxiv_id = entry.find('atom:id', ns).text.split('/abs/')[-1]

                    # åˆ†ç±»
                    categories = []
                    for category in entry.findall('atom:category', ns):
                        categories.append(category.get('term'))

                    # æ„å»ºè®ºæ–‡æ•°æ®ï¼ˆarXiv åŸå§‹æ ¼å¼ï¼‰
                    paper = {
                        'arxiv_id': arxiv_id,
                        'title': title,
                        'authors': authors,
                        'abstract': abstract,
                        'published': published[:10],  # YYYY-MM-DD
                        'updated': updated[:10],
                        'categories': categories,
                        'pdf_url': f"https://arxiv.org/pdf/{arxiv_id}.pdf",
                        'arxiv_url': f"https://arxiv.org/abs/{arxiv_id}"
                    }

                    # æ ‡å‡†åŒ–æ ¼å¼
                    normalized_paper = self.normalize_paper(paper)
                    papers.append(normalized_paper)

                return papers

            except Exception as e:
                last_error = e
                error_str = str(e)
                # åˆ¤æ–­æ˜¯å¦ä¸ºå¯é‡è¯•çš„é”™è¯¯
                retryable = any(keyword in error_str for keyword in [
                    '429', '503', 'IncompleteRead', 'Connection',
                    'Too Many Requests', 'Service Unavailable',
                    'RemoteDisconnected', 'ConnectionReset', 'SSL', 'EOF',
                    'timed out', 'timeout'
                ])

                if retryable and attempt < self.max_retries:
                    wait = self.delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                    self.logger.warning(f"arXiv è¯·æ±‚å¤±è´¥: {e}")
                    self.logger.info(f"ç¬¬ {attempt + 1} æ¬¡é‡è¯•ï¼Œç­‰å¾… {wait:.0f}s...")
                    time.sleep(wait)
                else:
                    self.logger.exception(f"arXiv æœç´¢å¤±è´¥: {e}")
                    return []

        self.logger.exception(f"arXiv æœç´¢å¤±è´¥ï¼ˆå·²é‡è¯• {self.max_retries} æ¬¡ï¼‰: {last_error}")
        return []

    def batch_search(
        self,
        queries: List[str],
        max_results_per_query: int = 10,
        since_year: int = 2020
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        æ‰¹é‡æœç´¢å¤šä¸ªæŸ¥è¯¢

        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
            max_results_per_query: æ¯ä¸ªæŸ¥è¯¢çš„æœ€å¤§ç»“æœæ•°
            since_year: èµ·å§‹å¹´ä»½

        Returns:
            å­—å…¸ï¼Œé”®ä¸ºæŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œå€¼ä¸ºè®ºæ–‡åˆ—è¡¨
        """
        self.logger.info(f"ğŸ” å¼€å§‹æ‰¹é‡æœç´¢ {len(queries)} ä¸ªæŸ¥è¯¢...")

        results = {}

        for i, query in enumerate(queries, 1):
            # å¦‚æœqueryæ˜¯å­—å…¸ï¼Œæå–queryå­—ç¬¦ä¸²
            if isinstance(query, dict):
                query_str = query.get('query', str(query))
                perspective = query.get('perspective', 'unknown')
            else:
                query_str = str(query)
                perspective = 'unknown'

            self.logger.info(f"[{i}/{len(queries)}] æœç´¢: {query_str}")
            if perspective != 'unknown':
                self.logger.info(f"è§†è§’: {perspective}")

            papers = self.search(
                query=query_str,
                max_results=max_results_per_query,
                since_year=since_year
            )

            results[query_str] = papers
            self.logger.info(f"æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")

            # å»¶è¿Ÿä»¥é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
            if i < len(queries):
                time.sleep(self.delay)

        return results

    def normalize_paper(self, paper: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ‡å‡†åŒ–è®ºæ–‡æ ¼å¼ï¼ˆarXiv â†’ é€šç”¨æ ¼å¼ï¼‰

        Args:
            paper: arXiv åŸå§‹æ ¼å¼çš„è®ºæ–‡æ•°æ®

        Returns:
            æ ‡å‡†åŒ–åçš„è®ºæ–‡æ•°æ®
        """
        # ä¿ç•™ arXiv åŸå§‹å­—æ®µï¼ŒåŒæ—¶æ·»åŠ æ ‡å‡†å­—æ®µ
        normalized = paper.copy()
        normalized.update({
            'id': paper['arxiv_id'],
            'url': paper['arxiv_url'],
            'source': 'arxiv'
        })
        return normalized

    def deduplicate(self, results: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
        """
        å»é‡è®ºæ–‡ç»“æœ

        Args:
            results: æ‰¹é‡æœç´¢çš„ç»“æœ

        Returns:
            å»é‡åçš„è®ºæ–‡åˆ—è¡¨
        """
        seen_ids = set()
        unique_papers = []

        for query, papers in results.items():
            for paper in papers:
                paper_id = paper.get('id') or paper.get('arxiv_id')
                if paper_id not in seen_ids:
                    seen_ids.add(paper_id)
                    # æ·»åŠ æ¥æºæŸ¥è¯¢ä¿¡æ¯
                    paper['source_query'] = query
                    unique_papers.append(paper)

        return unique_papers

    def search_and_deduplicate(
        self,
        queries: List[str],
        max_results_per_query: int = 10,
        since_year: int = 2020
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢å¹¶å»é‡ï¼ˆä¸€ç«™å¼å‡½æ•°ï¼‰

        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
            max_results_per_query: æ¯ä¸ªæŸ¥è¯¢çš„æœ€å¤§ç»“æœæ•°
            since_year: èµ·å§‹å¹´ä»½

        Returns:
            å»é‡åçš„è®ºæ–‡åˆ—è¡¨
        """
        results = self.batch_search(queries, max_results_per_query, since_year)
        unique_papers = self.deduplicate(results)

        self.logger.info(
            f"âœ… æœç´¢å®Œæˆï¼æ€»æŸ¥è¯¢æ•°: {len(queries)}, "
            f"å»é‡å‰: {sum(len(papers) for papers in results.values())} ç¯‡, "
            f"å»é‡å: {len(unique_papers)} ç¯‡"
        )

        return unique_papers
