#!/usr/bin/env python3
"""
Semantic Scholar æœç´¢æ¨¡å—
ç»§æ‰¿è‡ª BaseSearcherï¼Œä½¿ç”¨ Semantic Scholar API æœç´¢ç›¸å…³è®ºæ–‡
"""
import time
import urllib.parse
import urllib.request
import json
from typing import List, Dict, Any

from literature_review.searchers.base_searcher import BaseSearcher


class SemanticScholarSearcher(BaseSearcher):
    """Semantic Scholar æœç´¢å™¨ç±»ï¼ˆç»§æ‰¿è‡ª BaseSearcherï¼‰"""

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ– Semantic Scholar æœç´¢å™¨

        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å« max_retries, delay, timeout ç­‰å‚æ•°
        """
        super().__init__(config)
        self.base_url = "https://api.semanticscholar.org/graph/v1/paper/search"
        self.api_key = config.get('api_key', None)  # API key æ˜¯å¯é€‰çš„
        self.logger.info("Semantic Scholar æœç´¢å™¨åˆå§‹åŒ–å®Œæˆ")

    def search(
        self,
        query: str,
        max_results: int = 20,
        since_year: int = 2020
    ) -> List[Dict[str, Any]]:
        """
        åœ¨ Semantic Scholar ä¸Šæœç´¢è®ºæ–‡ï¼ˆå¸¦æŒ‡æ•°é€€é¿é‡è¯•ï¼‰

        Args:
            query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
            max_results: æœ€å¤§è¿”å›ç»“æœæ•°
            since_year: èµ·å§‹å¹´ä»½ï¼ˆé»˜è®¤2020ï¼‰

        Returns:
            æ ‡å‡†åŒ–çš„è®ºæ–‡åˆ—è¡¨
        """
        # æ„å»ºæŸ¥è¯¢å‚æ•°
        params = {
            'query': query,
            'limit': max_results,
            'fields': 'paperId,title,authors,year,abstract,url,venue,publicationDate,citationCount',
            'year': f'{since_year}-'  # ä» since_year åˆ°ç°åœ¨
        }

        url = f"{self.base_url}?{urllib.parse.urlencode(params)}"

        # æ„å»ºè¯·æ±‚å¤´
        headers = {}
        if self.api_key:
            headers['x-api-key'] = self.api_key

        last_error = None
        for attempt in range(self.max_retries + 1):
            try:
                # å‘é€è¯·æ±‚
                request = urllib.request.Request(url, headers=headers)
                with urllib.request.urlopen(request, timeout=self.timeout) as response:
                    data = json.loads(response.read().decode('utf-8'))

                papers = []
                for item in data.get('data', []):
                    # æå–è®ºæ–‡ä¿¡æ¯
                    paper = self._parse_paper(item)
                    if paper:
                        # æ ‡å‡†åŒ–æ ¼å¼
                        normalized_paper = self.normalize_paper(paper)
                        papers.append(normalized_paper)

                return papers

            except Exception as e:
                last_error = e
                error_str = str(e)
                # åˆ¤æ–­æ˜¯å¦ä¸ºå¯é‡è¯•çš„é”™è¯¯
                retryable = any(keyword in error_str for keyword in [
                    '429', '503', 'IncompleteRead', 'Connection',
                    'Too Many Requests', 'Service Unavailable',
                    'RemoteDisconnected', 'ConnectionReset', 'SSL', 'EOF',
                    'timed out', 'timeout'
                ])

                if retryable and attempt < self.max_retries:
                    wait = self.delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                    self.logger.warning(f"Semantic Scholar è¯·æ±‚å¤±è´¥: {e}")
                    self.logger.info(f"ç¬¬ {attempt + 1} æ¬¡é‡è¯•ï¼Œç­‰å¾… {wait:.0f}s...")
                    time.sleep(wait)
                else:
                    self.logger.exception(f"Semantic Scholar æœç´¢å¤±è´¥: {e}")
                    return []

        self.logger.exception(f"Semantic Scholar æœç´¢å¤±è´¥ï¼ˆå·²é‡è¯• {self.max_retries} æ¬¡ï¼‰: {last_error}")
        return []

    def _parse_paper(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§£æ Semantic Scholar API è¿”å›çš„è®ºæ–‡æ•°æ®

        Args:
            item: API è¿”å›çš„å•ç¯‡è®ºæ–‡æ•°æ®

        Returns:
            è§£æåçš„è®ºæ–‡æ•°æ®
        """
        try:
            # æå–ä½œè€…
            authors = []
            for author in item.get('authors', []):
                if 'name' in author:
                    authors.append(author['name'])

            # æ„å»ºè®ºæ–‡æ•°æ®
            paper = {
                's2_id': item.get('paperId', ''),
                'title': item.get('title', '').strip(),
                'authors': authors,
                'abstract': item.get('abstract', '').strip() if item.get('abstract') else '',
                'year': item.get('year'),
                'published': item.get('publicationDate', ''),
                'venue': item.get('venue', ''),
                'citation_count': item.get('citationCount', 0),
                's2_url': item.get('url', '')
            }

            return paper
        except Exception as e:
            self.logger.warning(f"è§£æè®ºæ–‡æ•°æ®å¤±è´¥: {e}")
            return None

    def normalize_paper(self, paper: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ‡å‡†åŒ–è®ºæ–‡æ ¼å¼ï¼ˆSemantic Scholar â†’ é€šç”¨æ ¼å¼ï¼‰

        Args:
            paper: Semantic Scholar åŸå§‹æ ¼å¼çš„è®ºæ–‡æ•°æ®

        Returns:
            æ ‡å‡†åŒ–åçš„è®ºæ–‡æ•°æ®
        """
        # ä¿ç•™ Semantic Scholar åŸå§‹å­—æ®µï¼ŒåŒæ—¶æ·»åŠ æ ‡å‡†å­—æ®µ
        normalized = paper.copy()
        normalized.update({
            'id': paper['s2_id'],
            'url': paper['s2_url'],
            'source': 'semantic_scholar'
        })
        return normalized

    def batch_search(
        self,
        queries: List[str],
        max_results_per_query: int = 10,
        since_year: int = 2020
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        æ‰¹é‡æœç´¢å¤šä¸ªæŸ¥è¯¢

        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
            max_results_per_query: æ¯ä¸ªæŸ¥è¯¢çš„æœ€å¤§ç»“æœæ•°
            since_year: èµ·å§‹å¹´ä»½

        Returns:
            å­—å…¸ï¼Œé”®ä¸ºæŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œå€¼ä¸ºè®ºæ–‡åˆ—è¡¨
        """
        self.logger.info(f"ğŸ” å¼€å§‹æ‰¹é‡æœç´¢ {len(queries)} ä¸ªæŸ¥è¯¢...")

        results = {}

        for i, query in enumerate(queries, 1):
            # å¦‚æœqueryæ˜¯å­—å…¸ï¼Œæå–queryå­—ç¬¦ä¸²
            if isinstance(query, dict):
                query_str = query.get('query', str(query))
                perspective = query.get('perspective', 'unknown')
            else:
                query_str = str(query)
                perspective = 'unknown'

            self.logger.info(f"[{i}/{len(queries)}] æœç´¢: {query_str}")
            if perspective != 'unknown':
                self.logger.info(f"è§†è§’: {perspective}")

            papers = self.search(
                query=query_str,
                max_results=max_results_per_query,
                since_year=since_year
            )

            results[query_str] = papers
            self.logger.info(f"æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")

            # å»¶è¿Ÿä»¥é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
            if i < len(queries):
                time.sleep(self.delay)

        return results

