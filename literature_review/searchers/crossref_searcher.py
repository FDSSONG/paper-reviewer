#!/usr/bin/env python3
"""
CrossRef æœç´¢æ¨¡å—
ç»§æ‰¿è‡ª BaseSearcherï¼Œä½¿ç”¨ CrossRef API æœç´¢ç›¸å…³è®ºæ–‡
"""
import time
import urllib.parse
import urllib.request
import json
from typing import List, Dict, Any

from literature_review.searchers.base_searcher import BaseSearcher


class CrossRefSearcher(BaseSearcher):
    """CrossRef æœç´¢å™¨ç±»ï¼ˆç»§æ‰¿è‡ª BaseSearcherï¼‰"""

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ– CrossRef æœç´¢å™¨

        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å« max_retries, delay, timeout ç­‰å‚æ•°
        """
        super().__init__(config)
        self.base_url = "https://api.crossref.org/works"
        self.email = config.get('email', None)  # ç”¨äº User-Agent
        self.logger.info("CrossRef æœç´¢å™¨åˆå§‹åŒ–å®Œæˆ")

    def search(
        self,
        query: str,
        max_results: int = 20,
        since_year: int = 2020
    ) -> List[Dict[str, Any]]:
        """
        åœ¨ CrossRef ä¸Šæœç´¢è®ºæ–‡ï¼ˆå¸¦æŒ‡æ•°é€€é¿é‡è¯•ï¼‰

        Args:
            query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
            max_results: æœ€å¤§è¿”å›ç»“æœæ•°
            since_year: èµ·å§‹å¹´ä»½ï¼ˆé»˜è®¤2020ï¼‰

        Returns:
            æ ‡å‡†åŒ–çš„è®ºæ–‡åˆ—è¡¨
        """
        # æ„å»ºæŸ¥è¯¢å‚æ•°
        params = {
            'query': query,
            'rows': max_results,
            'filter': f'from-pub-date:{since_year}',
            'sort': 'relevance',
            'order': 'desc'
        }

        url = f"{self.base_url}?{urllib.parse.urlencode(params)}"

        # æ„å»ºè¯·æ±‚å¤´ï¼ˆCrossRef æ¨èåœ¨ User-Agent ä¸­åŒ…å«é‚®ç®±ï¼‰
        headers = {
            'User-Agent': f'LiteratureReview/1.0 ({self.email or "no-email"})'
        }

        last_error = None
        for attempt in range(self.max_retries + 1):
            try:
                # å‘é€è¯·æ±‚
                request = urllib.request.Request(url, headers=headers)
                with urllib.request.urlopen(request, timeout=self.timeout) as response:
                    data = json.loads(response.read().decode('utf-8'))

                papers = []
                for item in data.get('message', {}).get('items', []):
                    # æå–è®ºæ–‡ä¿¡æ¯
                    paper = self._parse_paper(item)
                    if paper:
                        # æ ‡å‡†åŒ–æ ¼å¼
                        normalized_paper = self.normalize_paper(paper)
                        papers.append(normalized_paper)

                return papers

            except Exception as e:
                last_error = e
                error_str = str(e)
                # åˆ¤æ–­æ˜¯å¦ä¸ºå¯é‡è¯•çš„é”™è¯¯
                retryable = any(keyword in error_str for keyword in [
                    '429', '503', 'IncompleteRead', 'Connection',
                    'Too Many Requests', 'Service Unavailable',
                    'RemoteDisconnected', 'ConnectionReset', 'SSL', 'EOF',
                    'timed out', 'timeout'
                ])

                if retryable and attempt < self.max_retries:
                    wait = self.delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                    self.logger.warning(f"CrossRef è¯·æ±‚å¤±è´¥: {e}")
                    self.logger.info(f"ç¬¬ {attempt + 1} æ¬¡é‡è¯•ï¼Œç­‰å¾… {wait:.0f}s...")
                    time.sleep(wait)
                else:
                    self.logger.exception(f"CrossRef æœç´¢å¤±è´¥: {e}")
                    return []

        self.logger.exception(f"CrossRef æœç´¢å¤±è´¥ï¼ˆå·²é‡è¯• {self.max_retries} æ¬¡ï¼‰: {last_error}")
        return []

    def _parse_paper(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§£æ CrossRef API è¿”å›çš„è®ºæ–‡æ•°æ®

        Args:
            item: API è¿”å›çš„å•ç¯‡è®ºæ–‡æ•°æ®

        Returns:
            è§£æåçš„è®ºæ–‡æ•°æ®
        """
        try:
            # æå–ä½œè€…
            authors = []
            for author in item.get('author', []):
                given = author.get('given', '')
                family = author.get('family', '')
                if given and family:
                    authors.append(f"{given} {family}")
                elif family:
                    authors.append(family)

            # æå–æ ‡é¢˜
            title_list = item.get('title', [])
            title = title_list[0] if title_list else ''

            # æå–å‘è¡¨æ—¥æœŸ
            published_date = ''
            if 'published' in item:
                date_parts = item['published'].get('date-parts', [[]])[0]
                if len(date_parts) >= 1:
                    year = date_parts[0]
                    month = date_parts[1] if len(date_parts) >= 2 else 1
                    day = date_parts[2] if len(date_parts) >= 3 else 1
                    published_date = f"{year:04d}-{month:02d}-{day:02d}"

            # æ„å»ºè®ºæ–‡æ•°æ®
            paper = {
                'doi': item.get('DOI', ''),
                'title': title.strip(),
                'authors': authors,
                'abstract': item.get('abstract', '').strip() if item.get('abstract') else '',
                'published': published_date,
                'venue': item.get('container-title', [''])[0] if item.get('container-title') else '',
                'type': item.get('type', ''),
                'doi_url': f"https://doi.org/{item.get('DOI', '')}" if item.get('DOI') else ''
            }

            return paper
        except Exception as e:
            self.logger.warning(f"è§£æè®ºæ–‡æ•°æ®å¤±è´¥: {e}")
            return None

    def normalize_paper(self, paper: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ‡å‡†åŒ–è®ºæ–‡æ ¼å¼ï¼ˆCrossRef â†’ é€šç”¨æ ¼å¼ï¼‰

        Args:
            paper: CrossRef åŸå§‹æ ¼å¼çš„è®ºæ–‡æ•°æ®

        Returns:
            æ ‡å‡†åŒ–åçš„è®ºæ–‡æ•°æ®
        """
        # ä¿ç•™ CrossRef åŸå§‹å­—æ®µï¼ŒåŒæ—¶æ·»åŠ æ ‡å‡†å­—æ®µ
        normalized = paper.copy()
        normalized.update({
            'id': paper['doi'],
            'url': paper['doi_url'],
            'source': 'crossref'
        })
        return normalized

    def batch_search(
        self,
        queries: List[str],
        max_results_per_query: int = 10,
        since_year: int = 2020
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        æ‰¹é‡æœç´¢å¤šä¸ªæŸ¥è¯¢

        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
            max_results_per_query: æ¯ä¸ªæŸ¥è¯¢çš„æœ€å¤§ç»“æœæ•°
            since_year: èµ·å§‹å¹´ä»½

        Returns:
            å­—å…¸ï¼Œé”®ä¸ºæŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œå€¼ä¸ºè®ºæ–‡åˆ—è¡¨
        """
        self.logger.info(f"ğŸ” å¼€å§‹æ‰¹é‡æœç´¢ {len(queries)} ä¸ªæŸ¥è¯¢...")

        results = {}

        for i, query in enumerate(queries, 1):
            # å¦‚æœqueryæ˜¯å­—å…¸ï¼Œæå–queryå­—ç¬¦ä¸²
            if isinstance(query, dict):
                query_str = query.get('query', str(query))
                perspective = query.get('perspective', 'unknown')
            else:
                query_str = str(query)
                perspective = 'unknown'

            self.logger.info(f"[{i}/{len(queries)}] æœç´¢: {query_str}")
            if perspective != 'unknown':
                self.logger.info(f"è§†è§’: {perspective}")

            papers = self.search(
                query=query_str,
                max_results=max_results_per_query,
                since_year=since_year
            )

            results[query_str] = papers
            self.logger.info(f"æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")

            # å»¶è¿Ÿä»¥é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
            if i < len(queries):
                time.sleep(self.delay)

        return results


