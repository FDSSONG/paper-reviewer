#!/usr/bin/env python3
"""
PDF å¤„ç†æ¨¡å—
æ‰¹é‡ä¸‹è½½å’Œè½¬æ¢é«˜ç›¸å…³åº¦è®ºæ–‡çš„ PDF
"""
import os
import time
import requests
import shutil
from pathlib import Path
from typing import List, Dict, Optional
from literature_review.logger import get_logger

logger = get_logger("pdf_processor")


class PDFProcessor:
    """PDF å¤„ç†å™¨"""
    
    def __init__(self, mineru_token: Optional[str] = None):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            mineru_token: MinerU API token
        """
        self.mineru_token = mineru_token or os.getenv('MINERU_TOKEN')
        if not self.mineru_token:
            logger.warning("æœªè®¾ç½® MINERU_TOKENï¼ŒPDF è½¬æ¢åŠŸèƒ½å°†ä¸å¯ç”¨")
    
    def download_pdf(
        self,
        arxiv_id: str,
        output_dir: Path,
        timeout: int = 60,
        max_retries: int = 3
    ) -> Optional[Path]:
        """
        ä» arXiv ä¸‹è½½ PDFï¼ˆå¸¦æŒ‡æ•°é€€é¿é‡è¯•ï¼‰
        
        Args:
            arxiv_id: arXiv ID (ä¾‹å¦‚ '2301.12345')
            output_dir: è¾“å‡ºç›®å½•
            timeout: ä¸‹è½½è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
        Returns:
            ä¸‹è½½çš„ PDF æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å› None
        """
        # æ„å»º PDF URL
        pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
        
        output_dir.mkdir(parents=True, exist_ok=True)
        pdf_path = output_dir / f"{arxiv_id}.pdf"
        
        # å¦‚æœå·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½
        if pdf_path.exists() and pdf_path.stat().st_size > 1000:
            logger.info(f"PDF å·²å­˜åœ¨: {arxiv_id}")
            return pdf_path
        
        last_error = None
        for attempt in range(max_retries + 1):
            try:
                logger.info(f"ğŸ“¥ ä¸‹è½½ PDF: {arxiv_id}")
                response = requests.get(pdf_url, timeout=timeout, stream=True)
                response.raise_for_status()
                
                with open(pdf_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                
                # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§ï¼ˆPDF è‡³å°‘åº”è¯¥æœ‰å‡  KBï¼‰
                if pdf_path.stat().st_size < 1000:
                    raise IOError(f"ä¸‹è½½çš„æ–‡ä»¶è¿‡å° ({pdf_path.stat().st_size} bytes)ï¼Œå¯èƒ½ä¸å®Œæ•´")
                
                logger.info(f"âœ… ä¸‹è½½å®Œæˆ: {pdf_path}")
                return pdf_path
            
            except Exception as e:
                last_error = e
                # æ¸…ç†ä¸å®Œæ•´æ–‡ä»¶
                if pdf_path.exists():
                    pdf_path.unlink()
                
                error_str = str(e)
                retryable = any(keyword in error_str for keyword in [
                    'IncompleteRead', 'Connection', 'RemoteDisconnected',
                    'ConnectionReset', 'Timeout', 'timeout',
                    'Connection broken', 'Connection aborted'
                ])
                
                if retryable and attempt < max_retries:
                    wait = 3 * (2 ** attempt)  # æŒ‡æ•°é€€é¿: 3s, 6s, 12s
                    logger.warning(f"ä¸‹è½½å¤±è´¥: {e}")
                    logger.info(f"ç¬¬ {attempt + 1} æ¬¡é‡è¯•ï¼Œç­‰å¾… {wait}s...")
                    time.sleep(wait)
                else:
                    logger.error(f"ä¸‹è½½å¤±è´¥: {e}")
                    return None
        
        logger.error(f"ä¸‹è½½å¤±è´¥ï¼ˆå·²é‡è¯• {max_retries} æ¬¡ï¼‰: {last_error}")
        return None
    
    def convert_pdf_to_markdown(
        self,
        pdf_path: Path,
        output_dir: Path,
        model_version: str = "vlm"
    ) -> Optional[Path]:
        """
        ä½¿ç”¨ MinerU API å°† PDF è½¬æ¢ä¸º Markdown
        
        Args:
            pdf_path: PDF æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            model_version: æ¨¡å‹ç‰ˆæœ¬
        
        Returns:
            ç”Ÿæˆçš„ Markdown æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å› None
        """
        if not self.mineru_token:
            logger.warning("æœªè®¾ç½® MINERU_TOKENï¼Œè·³è¿‡è½¬æ¢")
            return None
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        md_path = output_dir / "full.md"
        if md_path.exists():
            logger.info(f"Markdown å·²å­˜åœ¨: {md_path}")
            return md_path
        
        try:
            logger.info(f"ğŸ”„ è½¬æ¢ PDF ä¸º Markdown: {pdf_path.name}")
            
            # 1) ç”³è¯·ä¸Šä¼  URL
            batch_id, upload_url = self._apply_upload_url(pdf_path, model_version)
            
            # 2) ä¸Šä¼ æ–‡ä»¶
            self._upload_file(pdf_path, upload_url)
            logger.info(f"âœ… ä¸Šä¼ å®Œæˆ, batch_id = {batch_id}")
            
            # 3) è½®è¯¢ç»“æœ
            result = self._poll_result(batch_id, timeout=300, interval=5)
            
            # 4) ä¸‹è½½å¹¶ä¿å­˜
            zip_url = result['data']['extract_result'][0]['full_zip_url']
            self._download_and_extract(zip_url, output_dir)
            
            logger.info(f"âœ… è½¬æ¢å®Œæˆ: {md_path}")
            return md_path
        
        except Exception as e:
            logger.error(f"è½¬æ¢å¤±è´¥: {e}")
            return None
    
    def _apply_upload_url(self, pdf_path: Path, model_version: str):
        """ç”³è¯·ä¸Šä¼  URL"""
        url = "https://mineru.net/api/v4/file-urls/batch"
        headers = {
            "Authorization": f"Bearer {self.mineru_token}",
            "Content-Type": "application/json"
        }
        payload = {
            "files": [{"name": pdf_path.name, "data_id": pdf_path.stem}],
            "model_version": model_version
        }
        
        r = requests.post(url, headers=headers, json=payload, timeout=30)
        r.raise_for_status()
        j = r.json()
        
        if j.get("code") != 0:
            raise RuntimeError(f"ç”³è¯·ä¸Šä¼ URLå¤±è´¥: {j}")
        
        return j["data"]["batch_id"], j["data"]["file_urls"][0]
    
    def _upload_file(self, pdf_path: Path, upload_url: str):
        """ä¸Šä¼ æ–‡ä»¶"""
        with open(pdf_path, "rb") as f:
            r = requests.put(upload_url, data=f, timeout=300)
        r.raise_for_status()
    
    def _poll_result(self, batch_id: str, timeout: int, interval: int):
        """è½®è¯¢ç»“æœ"""
        url = f"https://mineru.net/api/v4/extract-results/batch/{batch_id}"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.mineru_token}"
        }
        
        start = time.time()
        while True:
            r = requests.get(url, headers=headers, timeout=30)
            r.raise_for_status()
            j = r.json()
            
            if j.get("code") != 0:
                raise RuntimeError(f"æŸ¥è¯¢ç»“æœå¤±è´¥: {j}")
            
            items = j.get("data", {}).get("extract_result", [])
            if items:
                state = items[0].get("state", "").lower()
                if state == "done" and items[0].get("full_zip_url"):
                    return j
            
            if time.time() - start > timeout:
                raise TimeoutError(f"ç­‰å¾…è¶…æ—¶ï¼ˆ>{timeout}sï¼‰")
            
            time.sleep(interval)
    
    def _download_and_extract(self, zip_url: str, output_dir: Path):
        """ä¸‹è½½å¹¶è§£å‹ç»“æœ"""
        import zipfile
        
        output_dir.mkdir(parents=True, exist_ok=True)
        zip_path = output_dir / "result.zip"
        tmp_dir = output_dir / "_tmp"
        
        # ä¸‹è½½
        with requests.get(zip_url, stream=True, timeout=300) as r:
            r.raise_for_status()
            with open(zip_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=1024*1024):
                    f.write(chunk)
        
        # è§£å‹
        if tmp_dir.exists():
            shutil.rmtree(tmp_dir)
        
        with zipfile.ZipFile(zip_path, "r") as z:
            z.extractall(tmp_dir)
        
        # æ‰¾åˆ° full.md
        full_mds = list(tmp_dir.rglob("full.md"))
        if not full_mds:
            raise FileNotFoundError("æœªæ‰¾åˆ° full.md")
        
        full_md_src = full_mds[0]
        base_dir = full_md_src.parent
        
        # æ‹·è´æ–‡ä»¶
        shutil.copy2(full_md_src, output_dir / "full.md")
        
        images_src = base_dir / "images"
        images_dst = output_dir / "images"
        if images_src.exists():
            if images_dst.exists():
                shutil.rmtree(images_dst)
            shutil.copytree(images_src, images_dst)
        
        # æ¸…ç†
        shutil.rmtree(tmp_dir)
        zip_path.unlink()
    
    def batch_process(
        self,
        papers: List[Dict],
        output_base_dir: Path,
        download_only: bool = False
    ) -> List[Dict]:
        """
        æ‰¹é‡å¤„ç†è®ºæ–‡
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            output_base_dir: è¾“å‡ºæ ¹ç›®å½•
            download_only: æ˜¯å¦åªä¸‹è½½ä¸è½¬æ¢
        
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        results = []
        
        for i, paper in enumerate(papers, 1):
            arxiv_id = paper.get('arxiv_id', paper.get('id', ''))
            title = paper.get('title', '')[:60]
            
            logger.info(f"[{i}/{len(papers)}] å¤„ç†: {arxiv_id} - {title}...")
            
            paper_dir = output_base_dir / arxiv_id
            
            # ä¸‹è½½ PDF
            pdf_path = self.download_pdf(arxiv_id, paper_dir)
            
            if not pdf_path:
                results.append({
                    'arxiv_id': arxiv_id,
                    'status': 'download_failed',
                    'pdf_path': None,
                    'md_path': None
                })
                continue
            
            # è½¬æ¢ä¸º Markdown
            if download_only:
                md_path = None
            else:
                md_path = self.convert_pdf_to_markdown(pdf_path, paper_dir)
            
            results.append({
                'arxiv_id': arxiv_id,
                'status': 'success' if md_path or download_only else 'conversion_failed',
                'pdf_path': str(pdf_path),
                'md_path': str(md_path) if md_path else None
            })
            
            # é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
            if i < len(papers):
                time.sleep(2)
        
        return results


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    processor = PDFProcessor()
    
    # æµ‹è¯•ä¸‹è½½
    test_dir = Path("test_pdfs")
    pdf_path = processor.download_pdf("2301.12345", test_dir)
    
    if pdf_path:
        logger.info(f"âœ… æµ‹è¯•æˆåŠŸ: {pdf_path}")
